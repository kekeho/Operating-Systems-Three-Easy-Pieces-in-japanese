\part{Virtualization}
# 1. 本の対話
教授：この本にようこそ！ この本は、Operating Systems in Three Easy Piecesと呼ばれていて、ここではオペレーティングシステムについて知っておくべきことを教えています。 私は "教授"と呼ばれています。 あなたは誰ですか？  

学生：こんにちは教授！私はあなたが思ったとおり、「学生」と呼ばれています。さっそく学んでいきたいです！  

教授：いいですね。質問はありますか？  

学生：はい！なぜ "Three Easy Pieces"と呼ばれているんですか？  

教授：それは簡単なことです。ええと、ご存知の通り、リチャードファインマンの物理学に関する講義があって...  

学生：ああ！ 「Surely You’re Joking, Mr. Feynman」を書いた人のことですか？素晴らしい本ですよね！その本と何か関係が？  

教授：ええと…その本ではないですね。彼が書いたその本は素晴らしかったですよ。よかったらあなたも読んでみてください。彼が書いた本は物理学に関する基本である「Six Easy Pieces」という本でまとめられています。それを私たちはOSの話題について3つの簡単なステップで行うつもりです。OSは物理学の半分ほど難しさなので三つのステップにしたというのがあります。  

学生：私は物理学が好きなので、それはおそらく良い本なのでしょうね。3つの簡単なステップというのは具体的には何ですか？  

教授：それは、私たちが学ぶべき3つの重要なものである**仮想化、並行性、永続性**です。これらのアイデアを学ぶにあたり、OSの仕組み、つまりCPU上でどのプログラムを次に実行するか、どのように仮想メモリシステムのメモリ過負荷を処理するか、仮想マシンモニタの動作方法、ディスクに関する情報を管理する方法、また少しではありますが部品が故障したときに動作する分散システムを構築する方法についても説明します。具体的に言ったらそういったようなものですね。  

学生：私はあなたが何について話しているのか分かりません。  

教授：大丈夫ですよ！それはこれから学んでいけばいいんですよ。  

学生：別の質問があります：これらのことを学ぶのに最も良い方法は何ですか？  

教授：よい質問ですね！ まあ、それぞれの人が自分でこれらのことを理解する必要がありますが、ここであなたが何をすべきかということについては：教授が教材を紹介するのを聞くためにクラスに行く。 そして、毎週末に、これらのメモを読んで、あなたの頭の中にある知識が少しでも入っていくようにしましょう。 もちろん、しばらくしてから（ヒント：試験前に！）、メモをもう一度読んで知識を固めましょう。 もちろん、あなたの教授はいくつかの宿題やプロジェクトを割り当てていくでしょう。 特に、実際の問題を解決するために、プロジェクトを参加しているとき、これらのノート内のアイデアを基に行動していくことが最良の方法です。 孔子が言ったように...  

学生：ああ、私は知っています！ 「聞いて忘れる。見ると覚える。しては理解する」  

教授：（驚いたことに）私が何を言おうとしていたのか知っていたのですか！？  

学生：定型文のようなものでしたので…。また、私は孔子の大ファンであり、実際にこの引用文のより良い出典である荀子のさらに大ファンです。

教授：（驚いた）これからお互いにうまくやっていけそうですね！  

学生：教授、もう一度質問させてください。これらの対話の目的は何ですか？私の考えではメインとしてやるのはこの本だけだと思いますが？なぜ、これらの資料でだけで説明をしないのですか？  

教授：良い質問だね！説明しますと、物語の外に身を引っ張って少し考えてみるのが時々実世界では役に立ちます。これらの対話はそのために必要なのです。二人で、これらのかなり複雑な知識をすべて理解するために協力していくのです。あなたにその役割を任せてもいいですか？  

学生：だから一緒に考える必要があるということですね？もちろんです。私は他に何をすればよいでしょうか？私はこの本から出たことないので人生というものがないですけど…

教授：私もそうですね、悲しいことに。まぁ、とりあえずやっていきましょう！

\newpage

# 2.オペレーティングシステムの概要

学部のオペレーティングシステムコースを受講している場合は、コンピュータプログラムが実行されたときに何が行われているかについての知識が必要です。 そうでなければこの本(と対応するコース)を読むことは困難です。(Patt / Patel [PP03]と特にBryant / O'Hallaron [BOH10]の両方がかなり素晴らしい本です。)そのため、わからない場合はこの文書を読んだり、最寄りの書店に行ったりしてください。  

さっそく質問です。プログラムを実行するとどうなりますか？  
実行中のプログラムは非常に簡単なことをしています。命令を実行します。それは毎秒何百万という(そして最近では数十億もの)何百もの時間にプロセッサはメモリから命令をフェッチし、それをデコードし(すなわち、これがどの命令であるかを調べる)、それを実行する(すなわち、2つの数値を加算する、メモリにアクセスする、条件をチェックする、関数にジャンプする、など)。この命令で処理が完了すると、プロセッサは次の命令に進みます。以下同様に、プログラムが最後に完了するまで続きます[1]。  
　このように、我々はちょうどコンピュータのフォンノイマンモデルの基礎を述べた[2]。シンプルに聞こえるでしょう？しかし、このクラスでは、プログラムの実行中に、システムを使いやすくすることを第一の目標として進めていきます。

[1] もちろん、現代のプロセッサは、プログラムをより速く実行させるために多くの不思議なことを行います。複数の命令を一度に実行するだけでなく、命令の発行や完了の順番を入れ替えてしまうのです！しかし、ここでそのことを心配するのはやめましょう。我々は今、シンプルなモデルだけに興味があります。外から見る限り、ほとんどのプログラムの命令は一度に1つずつ、規則正しく順番通りに実行されます。
[2] フォンノイマンは、コンピューティングシステムの初期のパイオニアの一人でした。彼はまた、ゲーム理論と原爆に関する先駆的な仕事をし、NBAで6年間プレーしました。OK、そのうちの1つは真実ではありません。  

>> 問題：リソースを仮想化する方法  
>> この本で答える中心的な質問の1つは、オペレーティングシステムがリソースをどのように仮想化するのかという単純なものです。これが問題の要点です。OSはなぜ仮想化をするのでしょうか？答えはシステムを使いやすくするためです。したがって、これらを理解するために、どのようにメカニズムに焦点を当てていますか？OSはどのように効率的に機能しますか？どのハードウェアサポートが必要ですか？  

　実際には、プログラムを実行しやすく(一度に多くのことを実行できるようにしても)、プログラムがメモリを共有できるようにし、プログラムとデバイスとのやりとりや他の作業を可能にする責任があります。ソフトウェアの本体は、システムが使いやすく簡単かつ正確に動作することを確認する責任を負っているため、オペレーティングシステム(OS)[3]と呼ばれています。  
OSが主に行う主な方法は、仮想化と呼ばれる一般的な手法です。つまり、OSは物理リソース(プロセッサ、メモリ、ディスクなど)を取り出し、より一般的で強力で使いやすい仮想形式に変換します。したがって、オペレーティングシステムを仮想マシンと呼ぶことがあります。  
　もちろん、ユーザーがOSに何をすべきかを伝え、仮想マシンの機能(プログラムの実行、メモリの割り当て、ファイルへのアクセスなど)を利用できるようにするため、OSはいくつかのインタフェースも提供します(API)。また呼び出すことができます。実際、典型的なOSは、アプリケーションで利用できる数百のシステムコールをエクスポートします。OSはプログラムを実行したり、メモリやデバイスにアクセスしたり、その他の関連するアクションを呼び出すため、OSはアプリケーションに標準ライブラリを提供することもあります。  
　最後に、仮想化は多くのプログラムを実行し(CPUを共有する)、多くのプログラムが同時に自分の命令とデータ(つまり共有メモリ)にアクセスし、デバイスにアクセスするための多くのプログラム(したがってディスクなどを共有する)では、リソースマネージャーと呼ばれることもあります。CPU、メモリ、およびディスクはそれぞれ、システムのリソースです。したがって、これらのリソースを管理するオペレーティングシステムの役割は、効率的に、公正に、または他の多くの可能な目標を念頭に置きながら行うことです。OSの役割を少しずつ理解するために、いくつかの例を見てみましょう。

[3] OSの初期の別の名前は、スーパーバイザーまたはマスターコントロールプログラムでした。どうやら、後者はちょっと過激に聞こえました(詳細はTronの映画を見てください)。だから、ありがたいことに、"オペレーティングシステム"が代わりました。

![](../02/img/fig2_1.png)

## 2.1 CPUの仮想化
図2.1に最初のプログラムを示します。大したことはやっていません。実際には、`Spin()`は、一度実行されると時間と戻り値を繰り返しチェックする関数です。次に、ユーザーがコマンドラインで渡した文字列を出力し、永遠に繰り返します。  
　このファイルをcpu.cとして保存し、シングルプロセッサ(CPUと呼ぶこともあります)を持つシステム上でコンパイルして実行することにします。  
![](../02/img/fig2_1_1.png)  
　さほど面白い実行結果でもありません。システムはプログラムの実行を開始し、1秒が経過するまで時間を繰り返し確認します。1秒が経過すると、コードはユーザーが渡した入力文字列(この例では文字"A")を出力し、処理を続けます。プログラムは永遠に実行されることに注意してください。"Control-c"(UNIXベースのシステムではフォアグラウンドで実行中のプログラムを終了させる)を押すだけでプログラムを停止できます。  
　さて、同じことをやろうが、今度はこの同じプログラムの多くの異なるインスタンスを実行しましょう。図2.2に、このやや複雑な例の結果を示します。  
![](../02/img/fig2_2.png)  
　さて、今度は少し面白いことになっています。プロセッサは一つだけなのに、どういうわけか4つのプログラムすべてが同時に動いているように見えます！この魔法はどうやって起こるのでしょうか？[4]  
　オペレーティングシステムは、ハードウェアの助けを借りて、この錯覚、すなわち、システムに非常に多数の仮想CPUがあるという錯覚を担当していることが分かります。単一の（または少数の）CPUを無限個のCPUに変換することで多くのプログラムを見かけ上一度に実行できるようにすることがCPUの仮想化であり、本書の第一の主題の焦点です。
　もちろん、プログラムを実行して停止させたり、実行するプログラムをOSに指示するには、OSに希望を伝えるために使用できるインタフェース(API)が必要です。この本では、これらのAPIについて説明します。もちろんこれは、ほとんどのユーザーがオペレーティングシステムと対話する主な方法です  
　また、一度に複数のプログラムを実行できることにより、あらゆる種類の新しい質問が生まれます。たとえば、2つのプログラムを特定の時間に実行したい場合、実行する必要がありますか？この質問は、OSのポリシーによって解決されます。ポリシーはこれらのタイプの質問に答えるためにOS内の多くの異なる場所で使用されるため、オペレーティングシステムが実装する基本的なメカニズム(複数のプログラムを一度に実行する能力など)について学びます。  

[4] ＆記号を使用して、同時に4つのプロセスをどのように実行したかに注意してください。そうすることで、tcshシェルのバックグラウンドでジョブが実行されます。つまり、ユーザーは次のコマンドをすぐに発行できます。この場合、実行する別のプログラムです。コマンド間のセミコロンで、tcshで同時に複数のプログラムを実行することができます。別のシェル(bashなど)を使用している場合は、動作が少し異なります。詳細については、オンラインでドキュメントを参照してください。

![](../02/img/fig2_3.png)

## 2.2 メモリの仮想化
さあ、メモリを考えましょう。現代の機械によって提示される物理的記憶のモデルは非常に簡単です。メモリは単なるバイト配列です。メモリを読み取るには、そこに格納されているデータにアクセスできるようにアドレスを指定する必要があります。メモリを書き込む(または更新する)ためには、与えられたアドレスに書き込まれるデータも指定しなければなりません。  
　メモリは、プログラムが実行されているときは常にアクセスされます。プログラムは、すべてのデータ構造をメモリに保持し、ロードやストアなどのさまざまな命令や、作業中にメモリにアクセスする他の明示的な命令によってアクセスします。プログラムの各命令もメモリに記憶されていることを忘れないでください。したがって、メモリは各命令フェッチでアクセスされます。  
　`malloc()`を呼び出してメモリを割り当てるプログラム(図2.3)を見てみましょう。このプログラムの出力はここにあります：  
![](../02/img/fig2_3_1.png)  
　プログラムはいくつかのことを行います。まず、いくつかのメモリを割り当てます(行a1)。次に、メモリ(a2)のアドレスをプリントアウトした後、新たに割り当てられたメモリ(a3)の最初のスロットに番号0を入れます。最後に、ループし、1秒間遅延し、pに保持されているアドレスに格納されている値をインクリメントします。print文ごとに、実行中のプログラムのプロセス識別子(PID)と呼ばれるものも表示されます。このPIDは、実行中のプロセスごとに一意です。  
![](../02/img/fig2_4.png)  
　ここでも、この最初の結果はそれほど興味深いものではありません。新しく割り当てられたメモリはアドレス0x200000にあります。プログラムが実行されると、値がゆっくりと更新され、結果が出力されます。  
　今度は、同じプログラムの複数のインスタンスを再度実行して、何が起こるかを確認します(図2.4)。この例では、実行中の各プログラムが同じアドレス(0x200000)にメモリを割り当てていますが、それぞれが独立して0x200000の値を更新しているようです。これは、実行中の各プログラムが、他の実行中のプログラム[5]と同じ物理メモリを共有するのではなく、独自のプライベートメモリを持つかのようです。  
　確かに、OSがメモリを仮想化しているので、これがまさにここで起こっていることです。各プロセスは、独自のプライベート仮想アドレス空間(アドレス空間と呼ばれることもある)にアクセスし、OSは何らかの形でマシンの物理メモリにマップします。実行中のプログラム内のメモリ参照は、他のプロセス(またはOS自体)のアドレス空間には影響しません。実行中のプログラムに関する限り、それ自身に物理的な記憶があります。しかし、実際には、物理メモリはオペレーティングシステムによって管理される共有リソースです。まさしくこのすべてがどのように達成されたかは、本書の最初の部分の主題であり、仮想化の話題です。

## 2.3 同時実行
この本の別の主なテーマは並行性です。この概念的な用語は、同じプログラム内で同時に(すなわち、同時に)多くのことを処理する際に発生する多数の問題を指すために使用され、対処されなければいけません。並行性の問題は、オペレーティングシステム自体の中で最初に生じました。上記の仮想化の例でもわかるように、OSは多くのことを一度に処理しています。最初に1つのプロセスを実行し、次に別のプロセスなどを実行しています。

[5] この例が機能するには、アドレス空間のランダム化が無効になっていることを確認する必要があります。ランダム化は、特定の種類のセキュリティ上の欠陥に対する優れた防御となり得ることが判明しました。スタックストーミング攻撃を介してコンピュータシステムに侵入する方法を学びたい場合は、特に自分自身でそれについて詳しく読むことができます。

![](../02/img/fig2_5.png)

残念ながら、並行性の問題はもはやOS自体に制限されなくなりました。実際、現代のマルチスレッドプログラムは同じ問題を抱えています。マルチスレッドプログラムの例を示してみましょう(図2.5)。  
この例を今は完全に理解していないかもしれません(そして、後の章、同時実行の本の節でそれについてもっと学びます)、基本的な考え方は簡単です。メインプログラムは、`Pthread_create()`[6]を使用して2つのスレッドを作成します。スレッドは、他の関数と同じメモリ空間内で実行される関数と考えることができます。複数のスレッドを同時にアクティブにすることができます。この例では、各スレッドは`worker()`というルーチンで実行を開始します。このルーチンでは、ループ内のカウンタをループ回数だけインクリメントします。  

[6] 実際の呼び出しは小文字にする必要があります。`pthread create()`大文字のバージョンは独自のラッパーで、`pthread create()`を呼び出し、戻りコードが呼び出しが成功したことを示します。詳細については、コードを参照してください。

>> 問題のクラウド：正しい並行プログラムを作る方法  
>> 同じメモリ空間内に複数のスレッドを同時に実行すると、どうすれば正しく動作するプログラムを構築できますか？どのプリミティブがOSから必要ですか？どのような仕組みがハードウェアによって提供されるべきですか？どのようにそれらを使用して並行性の問題を解決できますか？

　以下は、変数ループの入力値を1000に設定してこのプログラムを実行すると何が起こるかを示すものです。ループの値によって、2人の作業者のそれぞれがループ内の共有カウンタを増分する回数が決まります。ループの値を1000に設定してプログラムを実行すると、カウンタの最終値はどのようになるでしょうか？  
![](../02/img/fig2_5_1.png)  
　おそらく推測したように、2つのスレッドが終了すると、カウンタの最終値は2000になり、各スレッドはカウンタを1000回インクリメントします。実際、ループの入力値をNに設定すると、プログラムの最終出力は2Nになると予想されます。しかし、実際はそれほど単純ではありません。同じプログラムを実行しますが、ループの値が高いほど、何が起こるか見てみましょう。  
![](../02/img/fig2_5_2.png)  
　この実行では、100,000の入力値を与えたときに最終値200,000を取得する代わりに、最初に143,012を取得します。次に、プログラムを2回実行すると、間違った値を取得するだけでなく、前回とは異なる値を取得します。実際には、高い値のループで繰り返しプログラムを実行すると、正しい答えが得られることがあります。それでなぜこれが起こっているのですか？  
　判明したように、これらの奇妙で珍しい結果の理由は、命令がどのように実行されるかに関連しており、これは一度に1つです。残念ながら、共有カウンタがインクリメントされる上記のプログラムの重要な部分は、カウンタの値をメモリからレジスタにロードする命令と、レジスタをインクリメントする命令と、メモリに格納する命令の3つの命令です。これらの3つの命令は原子的に(一度に)実行されないため、奇妙なことが起こる可能性があります。並行性のこの問題は、この本の第2部で詳しく説明します。  
![](../02/img/fig2_6.png)

## 2.4 永続性
　コースの3番目の主要テーマは持続性です。システムメモリでは、DRAMのようなデバイスが値を揮発性の方法で格納するため、データを簡単に失う可能性があります。電源が切れるかシステムがクラッシュすると、メモリ内のデータは失われます。したがって、データを永続的に保存できるようにするためには、ハードウェアとソフトウェアが必要です。このようなストレージは、ユーザーがデータを大事にしているため、あらゆるシステムにとって重要です。  
　ハードウェアは、ある種の入力/出力またはI/Oデバイスの形で提供されます。現代のシステムでは、ハードドライブは長寿命の情報のための共通リポジトリですが、ソリッドステートドライブ(SSD)もこの分野で進んでいます。  
　通常、ディスクを管理するオペレーティングシステムのソフトウェアは、ファイルシステムと呼ばれます。したがって、ユーザがシステムのディスク上に信頼できる効率的な方法で作成したファイルを格納する責任があります。  
　CPUとメモリ用にOSが提供する抽象化とは異なり、OSはアプリケーションごとにプライベートの仮想化ディスクを作成しません。むしろ、ユーザーはよく、ファイル内の情報を共有したいと考えられています。例えば、Cプログラムを書くときは、最初にEmacs[7]などのエディタを使ってCファイル(emacs -nw main.c)を作成し編集することができます。いったん完了したら、コンパイラを使用してソースコードを実行可能ファイルにすることができます(gcc -o main main.cなど)。作業が終了したら、新しい実行可能ファイル(./mainなど)を実行することができます。したがって、異なるプロセス間でファイルがどのように共有されているかを見ることができます。まず、Emacsはコンパイラへの入力として機能するファイルを作成します。コンパイラはその入力ファイルを使用して新しい実行可能ファイルを作成します(多くの手順で、詳細についてはコンパイラコースを参照してください)。最後に、新しい実行可能ファイルが実行されます。そして、新しいプログラムが生まれました！  
　これをよりよく理解するために、いくつかのコードを見てみましょう。図2.6に、文字列"hello world"を含むファイル(/tmp/file)を作成するためのコードを示します。

[7] Emacsを使うべきです。viを使用している場合、おそらく何か問題があります。実際のコードエディタではないものを使用している場合、それはさらに悪化します。

# 問題のクラウド：データを永続的に保存する方法
　ファイルシステムは、永続データの管理を担当するOSの一部です。そうするためにはどのような技術が必要ですか？高性能でこれを実現するためにはどのような仕組みとポリシーが必要ですか？ハードウェアとソフトウェアの不具合に直面して、信頼性はどのように達成されましたか？  
　このタスクを達成するために、プログラムはオペレーティングシステムに3回の呼び出しを行います。最初に、`open()`の呼び出しがファイルを開き、それを作成します。2番目の`write()`はファイルに何らかのデータを書き込みます。3番目の`close()`は、ファイルを閉じて、プログラムがこれ以上データを書き込んでいないことを示します。これらのシステムコールは、オペレーティングシステムのファイルシステムと呼ばれる部分にルーティングされます。ファイルシステムは要求を処理し、ユーザーに何らかのエラーコードを返します。  
　実際にディスクに書き込むためにOSが何をしているのか疑問に思うかもしれません。私たちはあなたを見せてくれるでしょうが、まず目を閉じることを約束しなければなりません。ファイルシステムはかなりの作業をしなければなりません。まず、ディスク上のどこにこの新しいデータが存在するかを把握し、ファイルシステムが維持しているさまざまな構造でそれを追跡します。そうするためには、基盤となるストレージデバイスにI/O要求を発行し、既存の構造を読み込んだり、更新(書き込み)する必要があります。デバイスドライバ[8]を書いた人なら誰もが知っているように、デバイスをあなたのために何かすることは、複雑で詳細なプロセスです。低レベルのデバイスインタフェースとその正確な意味についての深い知識が必要です。幸いにも、OSは、システムコールを通じてデバイスにアクセスする標準的でシンプルな方法を提供します。したがって、OSは標準ライブラリと見なされることがあります。  
　もちろん、デバイスがどのようにアクセスされるか、ファイルシステムがそのデバイス上でデータを永続的に管理する方法については、より多くの詳細があります。パフォーマンス上の理由から、ほとんどのファイルシステムでは、最初にそのような書き込みをしばらくの間遅延させて、より大きなグループにバッチすることを望んでいます。書き込み中のシステムクラッシュの問題を処理するために、ほとんどのファイルシステムには、ジャーナリングやコピーオンライトなどの複雑な書き込みプロトコルが組み込まれています。書き込みシーケンス中にエラーが発生した場合、後で合理的な状態に回復することができます。異なる一般的な操作を効率的にするために、ファイルシステムは、単純なリストから複雑なツリーに至るまで、多くの異なるデータ構造とアクセス方法を採用しています。この本の第3部では、デバイスとI/O、ディスク、RAID、ファイルシステムについて詳しく説明します。

[8] デバイスドライバは、特定のデバイスをどう扱うかを知っているオペレーティングシステムのコードです。デバイスとデバイスドライバについては後で詳しく説明します。

## 2.5 デザイン目標
　OSはCPU、メモリ、ディスクなどの物理的なリソースを取り、それらを仮想化するという考えを持っています。これは、並行性に関連する難しくて厄介な問題を処理します。また、ファイルを永続的に保存するため、長期的に安全になります。このようなシステムを構築したいと考えているので、設計と実装に集中し、必要に応じてトレードオフを行うためにいくつかの目標を念頭に置いていきたいと考えています。適切なトレードオフのセットを見つけることは、システム構築の鍵です。  
最も基本的な目標の1つは、システムを便利で使いやすいものにするために、いくつかの抽象化を構築することです。抽象は、私たちがコンピュータサイエンスでやるすべてにとって基本的なものです。抽象化は、大規模なプログラムを小さく分かりやすく分け、アセンブリなどを考えずにC[9]のような高級言語で記述したり、論理ゲートを考えずにコードを書いたり、構築したりすることができますトランジスタについてあまり考えずにゲートから外に出るプロセッサー。抽象化は非常に基本的なので、時にはその重要性を忘れることもありますが、ここでは取り上げません。したがって、各セクションでは、時間の経過とともに発展した主要な抽象化についていくつか議論し、OSの部分について考える方法を説明します。  
　オペレーティングシステムの設計と実装の1つの目標は、高性能を提供することです。これを言うもう1つの方法は、OSのオーバーヘッドを最小限に抑えることです。仮想化とシステムを使いやすくすることは価値がありますが、コストはかかりません。したがって、我々は過度のオーバーヘッドなしに仮想化やその他のOS機能を提供するよう努めなければなりません。これらのオーバーヘッドは、余分な時間(より多くの命令)と余分なスペース(メモリ内またはディスク上)といういくつかの形式で発生します。可能であれば、どちらか一方または両方、または両方を最小化するソリューションを探します。しかし、完璧さは必ずしも達成可能なわけではなく、私たちが気づくことを学び、(適切な場合には)容認するものです。  
　もう1つの目標は、アプリケーション間、およびOSとアプリケーション間の保護を提供することです。同時に多くのプログラムを実行できるようにしたいので、悪意のある、または偶発的な悪い行為が、他の行為に悪影響を及ぼさないようにしたいです。私たちは確かに、アプリケーションが(それがシステム上で動いているすべてのプログラムに影響するように)OSそのものを傷つけることはできないようにしたい。保護は、オペレーティングシステムの根底にある主な原則の1つ、つまり隔離の原則の中心にあります。プロセスを互いに隔離することは、保護の鍵であり、したがってOSが何をしなければならないかの根底にあります。  
　オペレーティングシステムもノンストップで実行する必要があります。それが失敗すると、システム上で実行されているすべてのアプリケーションも失敗します。この依存性のために、オペレーティングシステムはしばしば高い信頼性を提供するように努めています。オペレーティングシステムがますます複雑になり(時には何百万行ものコードを含む)、信頼性の高いオペレーティングシステムを構築することは非常に難しい課題です。実際には、現場で行われている多くの研究(BS + 09 、SS + 10])は、この正確な問題に焦点を当てています。  
　他の目標は理にかなっています。エネルギー効率はますます緑の世界で重要です。悪意のあるアプリケーションに対するセキュリティ(実際には保護の拡張)は、特に高度にネットワーク化されたこれらの時期には重要です。OSは小型で小型のデバイス上で動作するため、モビリティはますます重要になっています。システムの使用方法によっては、OSの目標が異なるため、少なくともわずかに異なる方法で実装される可能性があります。しかし、わかるように、OSを構築する方法について私たちが提示する多くの原則は、さまざまなデバイスに役立ちます。  

[9] C言語を高級言語と呼ぶことに反対する人もいます。これはOSのコースであることを覚えておいてください。ここでは、いつもアセンブリでコードを書く必要がないのがうれしいです！

## 2.5 いくつかの歴史
　この紹介を終える前に、オペレーティングシステムがどのように開発されたかを簡単に説明しましょう。人間によって構築されたシステムのように、エンジニアは設計上重要なことを学んだので、時間の経過とともにオペレーティングシステムに良いアイデアが蓄積されました。ここでは、いくつかの主要な開発について説明します。開発方法については、Brinch Hansenの優れたオペレーティングシステムの歴史をご覧ください[BH00]。

### 初期のオペレーティングシステム：ジャストライブラリ
　当初は、オペレーティングシステムはあまり働きませんでした。基本的には、一般的に使用される関数の単なるライブラリのセットでした。例えば、システムの各プログラマーに低レベルのI/O処理コードを書き込ませる代わりに、「OS」はそのようなAPIを提供し、したがって開発をより容易にします。  
　通常、これらの古いメインフレームシステムでは、人間のオペレータによって制御されるように、1つのプログラムが一度に1つずつ実行されました。現代のOSが何をすると思いますか(たとえば、ジョブを実行する順序を決めるなど)は、このオペレータによって実行されました。あなたがスマートな開発者だったなら、あなたはこのオペレータに親切であり、あなたの仕事をキューの前に移動させるかもしれません。  
　この計算モードはバッチ処理と呼ばれ、多数のジョブが設定され、オペレータによって「バッチ」で実行されるためです。その時点では、コンピュータはコストのためにインタラクティブな方法で使用されていませんでした。コンピュータの前に座って使用することは、あまりにも高価でした。ほとんどの場合、1時間に数十万ドルの費用がかかりました[BH00]。  
### ライブラリを超えて：保護
　オペレーティングシステムは、一般的に使用されるサービスの単純なライブラリではなく、マシンを管理する上でより中心的な役割を果たしました。これの重要な側面の1つは、OSに代わって実行されるコードが特別であることの認識でした。デバイスの制御を持っていたので、通常のアプリケーションコードとは異なった扱いをするべきです。どうしてでしょうか？別の方法としてディスク上のどこからでもアプリケーションを読み込めるようにしたらどうでしょうか？任意のプログラムがファイルを読み取ることができるように、プライバシーの概念が窓から出てきます。したがって、ライブラリとしてファイルシステムを実装することは意味がありません。代わりに、何かが必要でした。  
　したがって、Atlasコンピューティングシステム[K + 61、L78]によって開発されたシステムコールのアイデアが発明されました。OSルーチンをライブラリとして提供する代わりに(ここでは、アクセスするためのプロシージャコールを行うだけです)、ハードウェア命令とハードウェアの特別なペアを追加して、OSへの移行をより正式で制御されたプロセスにすることでした。  
　システムコールとプロシージャコールとの間の主な相違点は、システムコールがハードウェア特権レベルを上げると同時にOSに制御(すなわち、ジャンプ)を転送することです。ユーザーアプリケーションは、ユーザーモードと呼ばれるもので実行されます。つまり、ハードウェアによってアプリケーションが実行できる処理が制限されます。

たとえば、ユーザーモードで実行されているアプリケーションは、ディスクへのI/O要求を開始したり、物理メモリページにアクセスしたり、ネットワーク上でパケットを送信したりすることはできません。システムコールが開始されると(通常はトラップと呼ばれる特別なハードウェア命令によって)、ハードウェアは事前に指定されたトラップハンドラ(以前に設定したOS)に制御を移し、同時にカーネルモードに特権レベルを上げます。

カーネルモードでは、OSはシステムのハードウェアに完全にアクセスできるため、I/O要求の開始やプログラムで使用可能なメモリの増加などが可能です。OSが要求を処理すると、特別なreturn-from-trap命令を介してユーザーに制御が戻され、ユーザーモードに戻ります。同時に、アプリケーションが中断した場所に制御を戻します。

### マルチプログラミングの時代
　オペレーティングシステムが本当に始まったのは、メインフレームを超えたコンピューティングの時代、ミニコンピュータの時代でした。デジタル機器のPDPファミリのような古典的なマシンは、コンピュータを非常に手頃な価格にしました。したがって、大規模組織ごとに1つのメインフレームを持つ代わりに、組織内の少数の人が自分のコンピュータを持つ可能性があります。驚くことではないが、このコスト低下の大きな影響の1つは、開発者の活動の増加であった。よりスマートな人々はコンピュータを手に入れ、コンピュータシステムをより面白く美しいものにしました。  
　特に、マルチプログラミングは、マシンリソースをより有効に利用したいという要望のために普及しました。一度に1つのジョブを実行するのではなく、複数のジョブをメモリにロードし、それらのジョブ間で迅速に切り替えてCPU使用率を向上させます。I/Oデバイスが遅いため、この切り替えは特に重要でした。そのI/Oが処理されている間にCPU上でプログラムが待機するのは、CPU時間の無駄でした。代わりに、別の仕事に切り替えてしばらく運転してみてはどうですか？  
　I/Oと割り込みの存在下でマルチプログラミングとオーバーラップをサポートしたいという要望は、多くの方向性に沿ったオペレーティングシステムの概念開発における革新を強いられました。メモリ保護などの問題が重要になりました。あるプログラムが別のプログラムのメモリにアクセスすることはできません。マルチプログラミングによって導入された並行性の問題に対処する方法を理解することも重要でした。割り込みがあってもOSが正しく動作していることを確認することは大きな課題です。本書の後半で、これらの問題と関連するトピックを検討します。  
　その実用的な進歩の1つは、UNIXオペレーティングシステムの導入でした。これは主にベル研究所のKen Thompson(およびDennis Ritchie)(電話会社)のおかげです。UNIXは、さまざまなオペレーティングシステム(特にMultics [O72]、TENEX [B + 72]やBerkeley TimeSharing System [S + 68]など)から多くの優れたアイデアを得ましたが、まもなく、このチームは世界中の人々にUNIXのソースコードを含むテープを出荷していました。多くの人が関与してシステムに追加されました。詳細はAside(次のページ)を参照してください。  


### 現代
　ミニコンピュータの向こうには、新しいタイプのマシンが安くて速く、そして大衆にとって、今日のパーソナル・コンピュータまたはPCが登場しました。ワークグループごとにミニコンピュータを共有するのではなく、低コストでデスクトップごとに1台のマシンを使用できるようになり、アップルの初期のマシン(Apple IIなど)とIBM PCを中心に、新しいタイプのマシンがすぐにコンピューティングの支配的な役割を果たすようになりました。  
　残念なことに、オペレーティングシステムでは、初期のシステムがミニコンピュータの時代に学んだ教訓を忘れてしまった(または知らなかった)ので、最初はPCが大きく飛躍しました。例えば、DOSのような初期のオペレーティングシステム(マイクロソフトのディスクオペレーティングシステム)は、メモリ保護が重要ではないと考えていました。したがって、悪意のある(または、あまりにもプログラムの悪い)アプリケーションは、メモリ全体を書くことができます。Mac OSの第1世代(v9以前)は、ジョブスケジューリングに協力的なアプローチを取りました。したがって、偶発的に無限ループに突き当たったスレッドがシステム全体を引き継ぎ、再起動を強制する可能性があります。この世代のシステムに欠けているOS機能の痛ましいリストは長いですが、ここでの議論のためには長すぎます。  
　幸運なことに、何年もの苦しみの後、ミニコンピュータのオペレーティングシステムの古い機能がデスクトップに乗り始めました。たとえば、Mac OS X/macOSには、そのような成熟したシステムから期待されるすべての機能を含む、UNIXが中心です。Windowsは、特にWindows NTを始めとして、Microsoft OSテクノロジの飛躍的な飛躍を踏まえて、コンピューティングの歴史において多くの素晴らしいアイデアを採用しています。今日の携帯電話でも、Linuxなどのオペレーティングシステムが稼動していますが、これは1980年代にPCを走らせたもの(1970年代のミニコンピュータに似ています)に似ています。OS開発の全盛期に開発された優れたアイデアが現代の世界に浸透していることがわかりました。さらに優れた点は、これらのアイデアが引き続き発展し、より多くの機能を提供し、ユーザーやアプリケーションにとって現代のシステムをより良くすることです。  

>> ASIDE：UNIXの重要性  
>>　オペレーティングシステムの歴史においてUNIXの重要性を誇張することは困難です。以前のシステム(特に、MITの有名なMulticsシステム)の影響を受けて、UNIXは多くの素晴らしいアイデアを集め、シンプルで強力なシステムを作りました。オリジナルの"Bell Labs"の基盤となったUNIXは、より強力なワークフローを形成するために一緒に接続できる、強力で強力なプログラムを構築する統一的な原則でした。コマンドを入力するシェルは、このようなメタレベルプログラミングを可能にするパイプなどのプリミティブを提供しました。したがって、より大きなタスクを達成するためにプログラムをまとめるのが容易になりました。たとえば、単語"foo"を含むテキストファイルの行を見つけて、そのような行がいくつ存在するかを調べるには、grep foo file.txt | wc -lと入力して、grepとwcを使用します。  
　UNIX環境はプログラマや開発者にとってもフレンドリーで、新しいCプログラミング言語のコンパイラも提供していました。プログラマーが自分のプログラムを書いたり共有したりすることが容易になり、UNIXは非常に人気がありました。そして、おそらく、著者たちは、初期の形式のオープンソースソフトウェアである、誰かに尋ねた人に無料でコピーを寄贈したことを多分助けてくれたでしょう。  
また、コードのアクセシビリティと可読性も非常に重要でした。Cで書かれた美しく小さなカーネルを持っている人は、カーネルで遊ぶように他の人を招き、新しくてクールな機能を追加しました。たとえば、Bill Joyが率いるBerkeleyの企業グループは、高度な仮想メモリ、ファイルシステム、およびネットワークサブシステムを備えた素晴らしいディストリビューション(Berkeley Systems Distribution、またはBSD)を作りました。Joyは後にSun Microsystemsを共同設立しました。  
　残念ながら、UNIXの普及は、企業が所有権と利益を主張しようとするにつれて少し遅くなりました。しかし、弁護士の不幸な(しかし共通の)結果が関与しています。多くの企業にはSun MicrosystemsのSunOS、IBMのAIX、HPのHPUX、SGIのIRIXなどの独自の変種がありました。AT＆T/Bell Labsと他のプレイヤーの間の法的争いは、UNIX上で暗い雲を投げかけ、Windowsが導入されPC市場の多くを占めるように生き残るかどうか、多くの人が疑問に思っていました。

>> ASIDE：そしてその後のLinux  
>>　幸運なことに、UNIXの場合、Linus Torvaldsという若いフィンランドのハッカーは、元のシステムの背後にある原則とアイデアに大いに借りて、コードベースではなく、合法性の問題を避けて独自のUNIXバージョンを作成することに決めました。彼は世界中の多くの人たちの助けを得て、まもなくLinuxが生まれました(そして現代のオープンソースソフトウェアの動き)インターネット時代が到来すると、Google、Amazon、Facebookなどの大部分の企業は無料で、ニーズに合わせて簡単に変更できるように、Linuxを実行することを選択しました。確かに、これらの新会社の成功がそのようなシステムが存在しなかったと想像するのは難しいです。  
　スマートフォンが支配的なユーザー向けプラットフォームになるにつれて、Linuxは同じ理由の多くのために(Android経由で)そこにも拠点を見つけました。そしてSteve Jobsは彼のUNIXベースのNeXTStepオペレーティング環境をAppleに任せ、デスクトップ上でUNIXを普及させました(Appleの技術の多くのユーザーはおそらくこの事実を知らないでしょう)。そして、UNIXは今まで以上に重要な存在です。コンピューティングの神は、あなたがそれらを信じているならば、この素晴らしい結果に感謝しなければなりません。

## 2.7 概要
　したがって、私たちはOSについて紹介しています。今日のオペレーティングシステムでは、システムを比較的使いやすくし、今日使用しているほとんどすべてのオペレーティングシステムは、本書で説明する開発の影響を受けています。  
　残念なことに、時間的な制約のため、本書でカバーしていないOSのいくつかの部分があります。たとえば、オペレーティングシステムには多くのネットワーキングコードがあります。私たちはそれについてもっと学ぶためにあなたにネットワーキングクラスを取ることを任せます。同様に、グラフィックスデバイスは特に重要です。最後に、オペレーティングシステムの書籍の中には、セキュリティに関する大きな話題があります。OSは実行中のプログラム間で保護を提供し、ユーザーにファイルを保護する能力を与えなければならないという意味でそうするでしょうが、セキュリティコースで見つかる、より深刻なセキュリティ問題を掘り下げません。  
　しかし、CPUとメモリの仮想化の基本、並行性、デバイスやファイルシステムによる永続性など、重要なトピックがたくさんあります。心配しないでください！カバーすべき多くの土地がありますが、そのほとんどは非常に涼しく、道路の終わりには、コンピュータシステムが実際にどのように機能するかについて考えます。

#参考文献

[BS+09] “Tolerating File-System Mistakes with EnvyFS”  
Lakshmi N. Bairavasundaram, Swaminathan Sundararaman, Andrea C. Arpaci-Dusseau, RemziH. Arpaci-Dusseau  
USENIX ’09, San Diego, CA, June 2009  
A fun paper about using multiple file systems at once to tolerate a mistake in any one of them.  

[BH00] “The Evolution of Operating Systems”  
P. Brinch Hansen  
In Classic Operating Systems: From Batch Processing to Distributed Systems Springer-Verlag, New York, 2000  
This essay provides an intro to a wonderful collection of papers about historically significant systems.  

[B+72] “TENEX, A Paged Time Sharing System for the PDP-10”  
Daniel G. Bobrow, Jerry D. Burchfiel, Daniel L. Murphy, Raymond S. Tomlinson  
CACM, Volume 15, Number 3, March 1972  
TENEX has much of the machinery found in modern operating systems; read more about it to see how much innovation was already in place in the early 1970’s.

[B75] “The Mythical Man-Month”  
Fred Brooks  
Addison-Wesley, 1975  
A classic text on software engineering; well worth the read.  

[BOH10] “Computer Systems: A Programmer’s Perspective”  
Randal E. Bryant and David R. O’Hallaron  
Addison-Wesley, 2010  
Another great intro to how computer systems work. Has a little bit of overlap with this book — so if you’d like, you can skip the last few chapters of that book, or simply read them to get a different perspective on some of the same material. After all, one good way to build up your own knowledge is to hear as many other perspectives as possible, and then develop your own opinion and thoughts on the matter. You know, by thinking!

[G85] “The GNU Manifesto”  
Richard Stallman, 1985  
Available: https://www.gnu.org/gnu/manifesto.html  
A huge part of Linux’s success was no doubt the presence of an excellent compiler, gcc, and other relevant pieces of open software, all thanks to the GNU effort headed by Richard Stallman. Stallman is quite a visionary when it comes to open source, and this manifesto lays out his thoughts as to why; well worth the read.

[K+61] “One-Level Storage System”  
T. Kilburn, D.B.G. Edwards, M.J. Lanigan, F.H. Sumner  
IRE Transactions on Electronic Computers, April 1962  
The Atlas pioneered much of what you see in modern systems. However, this paper is not the best read. If you were to only read one, you might try the historical perspective below [L78].

[L78] “The Manchester Mark I and Atlas: A Historical Perspective”  
S. H. Lavington  
Communications of the ACM archive  
Volume 21, Issue 1 (January 1978), pages 4-12  
A nice piece of history on the early development of computer systems and the pioneering efforts of the Atlas. Of course, one could go back and read the Atlas papers themselves, but this paper provides a great overview and adds some historical perspective.

[O72] “The Multics System: An Examination of its Structure”  
Elliott Organick, 1972  
A great overview of Multics. So many good ideas, and yet it was an over-designed system, shooting for too much, and thus never really worked as expected. A classic example of what Fred Brooks would call the “second-system effect” [B75].

[PP03] “Introduction to Computing Systems:  
From Bits and Gates to C and Beyond”  
Yale N. Patt and Sanjay J. Patel  
McGraw-Hill, 2003  
One of our favorite intro to computing systems books. Starts at transistors and gets you all the way up to C; the early material is particularly great.

[RT74] “The UNIX Time-Sharing System”  
Dennis M. Ritchie and Ken Thompson  
CACM, Volume 17, Number 7, July 1974, pages 365-375  
A great summary of UNIX written as it was taking over the world of computing, by the people who wrote it.

[S68] “SDS 940 Time-Sharing System”  
Scientific Data Systems Inc.  
TECHNICAL MANUAL, SDS 90 11168 August 1968  
Available: http://goo.gl/EN0Zrn  
Yes, a technical manual was the best we could find. But it is fascinating to read these old system documents, and see how much was already in place in the late 1960’s. One of the minds behind the Berkeley Time-Sharing System (which eventually became the SDS system) was Butler Lampson, who later won a Turing award for his contributions in systems.

[SS+10] “Membrane: Operating System Support for Restartable File Systems”  
Swaminathan Sundararaman, Sriram Subramanian, Abhishek Rajimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, Michael M. Swift  
FAST ’10, San Jose, CA, February 2010  
The great thing about writing your own class notes: you can advertise your own research. But this paper is actually pretty neat — when a file system hits a bug and crashes, Membrane auto-magically restarts it, all without applications or the rest of the system being affected.  

\newpage

## 3.仮想化に関する対話

教授：そして、オペレーティングシステム上の3つの主要な部分のうち、最初の部分である仮想化のお話になります。

学生：しかし、偉大なる先生、仮想化とは何ですか？

教授：桃があると想像してください。

学生：桃？ (信用できない)

教授：はい、桃です。それを物理的な桃と呼ぶことにしよう。しかし、この桃を食べたい多くの人がいるとしましょう。それぞれ食べたい人全員に桃をあげることができたら全員幸せですよね。私たちは食べる桃を仮想化された桃と呼んでいます。私たちは何とか一つの物理的な桃からこれらの仮想化された桃を多く作り出します。そして、重要なことは：この錯覚では、彼らは物理的な桃を持っているように見えますが、実際にはそうではありません。

学生：桃を分けあっているように見えるけど、実際は平等に配られていないことを知らないのですか？

教授：そうです！

学生：しかし、たった一つの桃があります。

教授：はい。そして...？

学生：もし私が一つの桃しかもっていないことを知っていて、他の人と桃を分けていたら、私は桃が平等に配られていないことに気づくと思う。

教授：そうです！いい視点ですね。しかし、桃を多く食べたい人がいるのです。ほとんどの時間彼らは昼寝や何か他のことをしているので、あなたはその桃を奪って、しばらくの間他の誰かに桃を与えることができます。それで、私たちは多くのバーチャル・ピーチの錯覚を作り出します。one peach for each person!

学生：悪いキャンペーンのスローガンのように聞こえます。あなたはコンピュータについて話していますが、本当にコンピュータ専攻の教授ですか？

教授：まぁ待ちなさい若者よ、あなたにこれからより具体的な例を出しましょう。良いアイデアを思いついた！最も基本的なリソースであるCPUを具体的な例として取りあげましょう。 1つのシステムに1つの物理CPUがあるとします(現在は2つまたは4つ以上の場合があります)。仮想化は、単一のCPUを使用して、システム上で実行されているアプリケーションに対して多くの仮想CPUのように見せかけるものです。したがって、各アプリケーションは使用する独自のCPUを持っていると考えていますが、実際には1つしかありません。したがって、OSは美しい錯覚を作り出しました。これが、CPUを仮想化です。

学生：うわー！それは魔法のように聞こえますね。もっと教えてください！それはどのように機能するのですか？

教授：そのような言葉が出てくるということは準備ができているようですね。

学生：もちろんです！私は教授を認めなければならないようですね…ただちょっと心配しているのは、教授が再び桃のことについて話を始めることなんですが…。

教授：心配しないでください。私は桃が好きではないので。それでは、始めましょう！

\newpage

# 4. The Abstraction: The Process

この章では、OSがユーザに提供するもっとも基本的な抽象概念の1つ、すなわちプロセスについて説明します。プロセスの定義は、非公式には非常に単純です。実行中のプログラム[V+65、BH70]です。プログラム自体は特に意味のないものです。ディスク上に、一連の命令(例えば、いくつかの静的なデータ)があります。これらのバイトを取り込んで実行させるのは、オペレーティングシステムであり、プログラムを何か役に立つものに変えるものです。


一度に複数のプログラムを実行したいと思うことがよくあります。たとえば、Webブラウザー、メールプログラム、ゲーム、音楽プレーヤーなどを実行したいデスクトップパソコンやノートパソコンを考えてみましょう。実際、典型的なシステムは、見かけ上、数十から数百のプロセスを同時に実行している可能性があります。そうすることで、システムが使いやすくなります。CPUが利用可能かどうか心配する必要は決してありません。単にプログラムを実行するだけです。

>> 問題：  
>> どのように多くのCPUがあるように見せるのでしょうか？利用可能な物理CPUはほんのわずかですが、OSはどのようにしてこのCPUをほぼ無限に供給しているような錯覚をさせるのでしょうか？

OSはCPUを仮想化してこの錯覚を作り出します。1つのプロセスを実行し、それを停止し、別のプロセスを実行するなど、実際には1つの物理CPUしか存在しない場合でも、多くの仮想CPUが存在するという錯覚をつくりだすようなことがあります。CPUのtime sharingと呼ばれるこの基本的な技術は、ユーザーが好きなだけ多くの並行プロセスを実行できるようにします。しかし、並行処理はパフォーマンスとtrade offです。CPUを共有する必要がある場合は、それぞれがゆっくりと実行されるため、パフォーマンスが低下します。

CPUの仮想化を実装し、それをうまく実装するためには、低レベルのマシンといくつかの高レベルのインテリジェンスが必要です。私たちは、低レベルの機構メカニズム(low-level machinery mechanisms)と呼んでいます。メカニズムとは、必要な部分を実装する低レベルのメソッドまたはプロトコルです。たとえば、コンテキストスイッチを実装する方法を後で学習します。これにより、OSにあるプログラムの実行を停止し、特定のCPU上で別のプログラムを実行できるようになります。このtime sharing mechanismはすべての最新のOSで採用されています。

これらのメカニズムの上には、OSのインテリジェンスの一部がポリシーの形で存在します。ポリシーとは、OS内で何らかの決定を下すためのアルゴリズムです。たとえば、CPU上で実行可能なプログラムの複数があれば、どのプログラムをOSで実行する必要がありますか？OSのスケジューリング方針は、どのプログラムが実行されているかなどの仕事量に関する情報や、パフォーマンス指標など、履歴情報を使用してこの決定を行います。対話型パフォーマンスを最適化するシステム、またはスループット)を決定することができます。

>> TIP: USE TIME SHARING (AND SPACE SHARING)  
>> Time sharingは、OSがリソースを共有するために使用する基本的な手法です。リソースをある法則に従って少しずつ、次に少しずつ使用することを許可することにより、問題のリソース(例えば、CPUまたはネットワークリンク)を多くの人が共有することができます。time sharingの対応は、リソースを使用したい人の間で(空間内で)分け合う空間共有です。たとえば、ディスクスペースは当然スペース共有されたリソースです。ディスクスペースの領域のブロックがファイルに割り当てられると、ユーザーが元のファイルを削除するまで、通常は別のファイルに割り当てられません。

## 4.1 The Abstraction: A Process
実行中のプログラムのOSによって提供される抽象化は、プロセスと呼ばれるものです。上記のように、プロセスは実行中のプログラムのことです。どの瞬間においても、実行中にアクセスまたは影響を与えるシステムのさまざまな部分のインベントリを取ることによって、プロセスを要約することができます。

どのようなプロセスを構成するのかを理解するには、マシンの状態(machine state)を理解する必要があります。実行中にプログラムが読み書きできるものがあるのかを知る必要があります。

プロセスを構成するマシン状態の1つのとしてわかりやすいものは、メモリです。命令はメモリに置かれます。実行中のプログラムが読み書きするデータもメモリに格納されます。したがって、プロセスがアドレス指定できるメモリ(アドレス空間と呼ばれる)はプロセスの一部です。

また、プロセスのマシン状態の一部はレジスタです。多くの命令はレジスタを明示的に読み込んだり更新したりするので、プロセスの実行にとって重要です。

このマシン状態の一部を形成する特別なレジスタがいくつかあることに注意してください。例えば、プログラムカウンタ(PC)(命令ポインタまたはIPと呼ばれることもある)は、プログラムのどの命令が現在実行されているかを示します。同様に、スタックポインタと関連するフレームポインタを使用して、関数パラメータ、ローカル変数、およびリターンアドレスのスタックを管理します。  
最後に、プログラムはよくストレージデバイスにもアクセスします。そのようなI/O情報は、プロセスが現在開いているファイルのリストも含みます。

>> TIP: SEPARATE POLICY AND MECHANISM  
>> 多くのオペレーティングシステムで共通の設計は、低レベルのメカニズム[L+75]から高レベルのポリシーを分離することです。メカニズムは、システムに関する質問への答えを提供するものと考えることができます。たとえば、オペレーティングシステムはコンテキストスイッチをどのように実行するでしょうか？

ポリシーは質問に対する回答を提供します。たとえば、オペレーティングシステムは現在どのプロセスを実行する必要があるでしょうか？これらの2つを分離することで、メカニズムを再考する必要なしにポリシーを簡単に変更できるため、一般的なソフトウェア設計の原則であるモジュール性の一種です。

## 4.2 Process API
次の章まで実際のプロセスAPIの議論を延期しますが、ここではまずオペレーティングシステムのどのインタフェースに何が含まれなければならないかについていくつかの考えを示します。これらのAPIは、何らかの形で最新のオペレーティングシステムで使用できます。

- Create：オペレーティングシステムには、新しいプロセスを作成するためのメソッドが含まれている必要があります。シェルにコマンドを入力するか、アプリケーションアイコンをダブルクリックすると、OSが呼び出されて、指定したプログラムを実行するための新しいプロセスが作成されます。

- Destroy：プロセス作成用のインタフェースがあるため、システムはプロセスを強制的に破棄するインタフェースも提供します。もちろん、多くのプロセスが実行され、完了したばかりのプロセスは終了します。しかしながら、ユーザがそれらを止めることをしたいかもしれないので、暴走プロセスを止めるためのインタフェースは非常に有用です。

- Wait：プロセスが実行を停止するのを待つことが有用な場合があります。したがって、何らかの種類の待機インタフェースが提供されることが多いです。

-  Miscellaneous Control：プロセスを強制終了または待機する以外に、可能な他のコントロールがあることがあります。たとえば、ほとんどのオペレーティングシステムでは、プロセスを中断させて(しばらく実行しないようにする)何らかの方法を提供してから、プロセスを再開します(実行し続けます)。

- Status：通常、プロセスの実行時間や状態など、プロセスに関するステータス情報を得るためのインタフェースがあります。

![](../04/img/fig4_1.PNG)

## 4.3 Process Creation: A Little More Detail
私たちが少し疑問に思うことは、プログラムがプロセスにどのように変換されるかということです。具体的には、OSはどのようにプログラムを起動して実行しているのでしょうか？プロセスの作成は実際にどのように機能するのでしょうか？

プログラムを実行するためにOSが最初にしなければならないことは、そのコードおよび任意の静的データ(例えば、初期化された変数)をメモリにプロセスのアドレス空間にロードすることです。プログラムは、ある種の実行可能形式で、ディスク(または現代のシステムでは、フラッシュベースのSSD)に最初は常駐しています。したがって、プログラムと静的データをメモリにロードするプロセスでは、OSがディスクからこれらのバイトを読み込み、どこかのメモリに配置する必要があります(図4.1を参照)。

初期の(または単純な)オペレーティングシステムでは、ローディングプロセスは一回で完了させます。つまり、プログラムを実行する前に一度ロードをすべて終わらせます。現代のOSは、遅延的です。つまり、プログラムの実行中に必要とされるときにのみコードまたはデータの断片をロードすることによって、少しずつロードさせてプロセスを実行させます。コードやデータの遅延的な読み込みがどのように機能するかを本当に理解するには、ページングとスワッピングのメカニズムについてもっと理解しておく必要があります。今後はメモリの仮想化について議論します。今のところ、何かを実行する前に、重要なプログラムビットをディスクからメモリに取り込むためには、OSが何らかの作業を行う必要があることを覚えておいてください。

コードと静的データがメモリにロードされると、プロセスを実行する前にOSが行う必要のあることがあります。いくつかのメモリは、プログラムのruntime stack(または単にstack)に割り当てなければなりません。既に知っているはずのように、Cプログラムはローカル変数、関数のパラメータ、および戻りアドレスにスタックを使用します。OSはこのメモリを割り当て、プロセスに渡します。また、OSは引数でスタックを初期化します:具体的には、`main()`関数の引数、すなわちargcとargv配列に値を代入します。

OSは、プログラムのヒープ用にいくつかのメモリを割り当てることもできます。Cプログラムでは、明示的に要求された動的に割り当てられたデータにヒープが使用されます。プログラムは`malloc()`を呼び出してheap領域を要求し、`free()`を呼び出して明示的にheap領域を解放します。ヒープは、リンクリスト、ハッシュテーブル、ツリー、その他のデータ構造に必要です。最初のヒープ領域は小さいです。より多くのメモリを割り当てるときは、最初にプログラムが実行されるとき、また、`malloc()`ライブラリAPIを介してより多くのメモリを要求するときです。このとき、heap領域を大きくするためにOSが関与し、より多くのメモリをプロセスに割り当てるかもしれません。

OSは、特に入力/出力(I/O)に関連して、いくつかの他の初期化タスクも行います。たとえば、UNIXシステムでは、各プロセスは標準で入力、出力、およびエラーの3つのopen file discripterをデフォルトで持っています。これらのディスクリプタは、プログラムが端末からの入力を容易に読み取ることができるとともに、出力を画面に出力することを可能にします。永続性(persistence)に関する本の第3部では、I/Oやファイルディスクリプタなどについて学びます。

メモリにコードとスタティックデータをロードすることで、スタックを作成して初期化し、I/O設定に関連する他の作業を行うことで、OSはプログラム実行のステージを(最終的に)設定します。したがって、エントリポイント(プログラムを起動するところ)、最後に`main()`ルーチン(次の章で説明する特殊なメカニズムによって)にジャンプすることによって、CPUはCPUの制御を移します。そこで、新しく作成されたプロセスに渡され、プログラムが実行を開始します。

## 4.4 Process States
プロセスが何であるかについていくつかのアイデアがあるので(私たちはこの概念を改良し続けるつもりですが)、プロセスがどのように作成されるかについて、ある時点でプロセスができるさまざまな状態についてお話しましょう。プロセスがこれらの状態の1つになるという考えは、初期のコンピュータシステム[DV66、V+65]で生まれました。単純化されたビューでは、プロセスは次の3つの状態のいずれかになります。

- Running：実行中の状態では、プロセスがプロセッサ上で実行されています。これは命令を実行していることを意味します。

- Ready：レディ状態では、プロセスは実行準備が整っていますが、何らかの理由でOSがこの瞬間にプロセスを実行しないことを選択しました。

- Blocked：ブロックされた状態では、プロセスが何らかの操作を実行したため、他のイベントが発生するまで実行できません。一般的な例：プロセスがディスクへのI/O要求を開始すると、プロセスがブロックされるため、他のプロセスでプロセッサを使用することができます。

![](../04/img/fig4_2.PNG)

これらの状態をグラフにマッピングすると、図4.2の図になります。この図でわかるように、プロセスは、OSの裁量で、実行可能状態と実行状態の間で移動できます。準備完了から実行中に移行することは、プロセスがスケジュールされていることを意味します。実行中から準備完了に移行すると、そのプロセスはスケジュールされていません。プロセスがブロックされると(例えば、I/O動作を開始することによって)、OSは何らかのイベント(例えば、I/O完了)が発生するまでその状態を維持します。その時点で、プロセスは再び準備完了状態に移行します(OSが決定した場合は、すぐに再実行する可能性があります)  
2つのプロセスがこれらの状態のいくつかをどのように移行するかの例を見てみましょう。まず、2つのプロセスが稼働しているとします。それぞれのプロセスはCPUを使用しています(I/Oはありません)。この場合、各プロセスの状態のトレースは、このようになります(図4.3)。

![](../04/img/fig4_3.PNG)

この次の例では、最初のプロセスが実行した後にI/Oを発行します。その時点で、プロセスはブロックされ、他のプロセスに実行の機会が与えられます。図4.4に、このシナリオのトレースを示します。  
具体的には、Process0はI/Oを開始し、完了するのを待ってブロックされます。ディスクからの読み取りやネットワークからのパケットの待機など、プロセスはブロックされます。OSはProcess0がCPUを使用していないことを認識し、Process1の実行を開始します。Process1が実行されている間、I/Oは完了し、Process0を準備完了に戻します。最後に、Process1が終了し、Process0が実行されて終了します。

![](../04/img/fig4_4.PNG)

この簡単な例でも、OSが決定しなければならないことが、たくさんあることに注意してください。まず、Process0がI/Oを発行している間に、システムはProcess1を実行することを決定しなければなりませんでした。そうすることで、CPUをビジーに保つことによってリソースの使用率が向上します。第2に、システムはI/Oが完了したときにProcess0に戻らないことにしました。これが良い判断かどうかは不明です。みなさんはどう思いますか？これらのタイプの決定は、OSスケジューラによって行われます。これについては、今後いくつかの章で説明します。

## 4.5 Data Structures
OSはプログラムであり、どのプログラムでも、さまざまな関連情報を追跡するいくつかの重要なデータ構造を持っています。たとえば、各プロセスの状態を追跡するために、OSは、準備が整っているすべてのプロセスのプロセスリストと、現在実行中のプロセスを追跡するための追加情報を保持している可能性があります。OSはブロックされたプロセスを何らかの形で追跡しなければなりません。I/Oイベントが完了したら、OSは正しいプロセスを起動して、再度実行する準備をしておく必要があります。  
図4.5は、OSがxv6カーネル[CK+08]の各プロセスについてどのようなタイプの情報を追跡する必要があるかを示しています。Linux、Mac OS X、Windowsなどの「実際の」オペレーティングシステムにも同様のプロセス構造が存在します。それらを見てどのくらい複雑であるかを見てください。  
この図から、OSがプロセスについて追跡している重要な情報をいくつか確認できます。レジスタコンテキストは、停止されたプロセスに対して、そのレジスタの内容を保持する。プロセスが停止すると、そのレジスタはこのメモリ位置に保存されます。これらのレジスタを復元する(すなわち、それらの値を実際の物理レジスタに戻す)ことによって、OSはプロセスの実行を再開することができます。コンテキストスイッチと呼ばれるこの手法については、今後の章で詳しく説明します。

![](../04/img/fig4_5.PNG)

図から、プロセスが実行可能、準備完了、ブロックされている以外の状態がいくつかあることもわかります。場合によっては、プロセスが作成されているときにシステムが初期状態になることがあります。また、プロセスは終了したが、まだクリーンアップされていない最終状態に置くことができます(UNIXベースのシステムでは、ゾンビ状態1と呼ばれます)。この最終状態は、他のプロセス(通常はプロセスを作成した親プロセス)がプロセスの戻りコードを調べ、正常終了したプロセスが正常に実行されたかどうかを確認するのに便利です(UNIXベースシステムでは、彼らは正常にタスクを達成しており、それ以外の場合は0ではない)。終了すると、親は、子の完了を待つために1回の最終コール(例えば、`wait()`)を行い、現在絶滅しているプロセスを参照する関連するデータ構造をクリーンアップできることをOSに示します。


>> ASIDE: DATA STRUCTURE — THE PROCESS LIST  
>> オペレーティングシステムには、さまざまな重要なデータ構造があります。プロセスリストは、重要なデータ構造の一つです。これは単純なものの1つですが、確かに複数のプログラムを同時に実行する能力を持つOSは、システム内のすべての実行中のプログラムを追跡するために、この構造に似たものを持っています。時には、プロセスに関する情報を格納する個々の構造体をProcess Control Block(PCB)と呼ぶこともあります。Process Control Block(PCB)は、各プロセスに関する情報を含むC構造体に関する素晴らしい方法です。

## 4.6 Summary
我々はOSの最も基本的な抽象化を紹介した。プロセスというのは、それは非常に単純に実行中のプログラムとみなされることです。この概念を念頭に置いて、プロセスを実装するために必要な低レベルのメカニズムと、インテリジェントな方法でそれらをスケジューリングするために必要とされるより高いレベルのポリシーを扱います。メカニズムとポリシーを組み合わせることによって、オペレーティングシステムがCPUをどのように仮想化するかを理解していきます。

# 参考文献

[BH70] “The Nucleus of a Multiprogramming System”  
Per Brinch Hansen  
Communications of the ACM, Volume 13, Number 4, April 1970  
This paper introduces one of the first microkernels in operating systems history, called Nucleus. The idea of smaller, more minimal systems is a theme that rears its head repeatedly in OS history; it all began with Brinch Hansen’s work described herein.

[CK+08] “The xv6 Operating System”  
Russ Cox, Frans Kaashoek, Robert Morris, Nickolai Zeldovich  
From: https://github.com/mit-pdos/xv6-public  
The coolest real and little OS in the world. Download and play with it to learn more about the details of how operating systems actually work. We have been using an older version (2012-01-30-1-g1c41342) and hence some examples in the book may not match the latest in the source.

[DV66] “Programming Semantics for Multiprogrammed Computations”  
Jack B. Dennis and Earl C. Van Horn  
Communications of the ACM, Volume 9, Number 3, March 1966  
This paper defined many of the early terms and concepts around building multiprogrammed systems.

[L+75] “Policy/mechanism separation in Hydra”  
R. Levin, E. Cohen, W. Corwin, F. Pollack, W. Wulf  
SOSP 1975  
An early paper about how to structure operating systems in a research OS known as Hydra. While Hydra never became a mainstream OS, some of its ideas influenced OS designers.

[V+65] “Structure of the Multics Supervisor”  
V.A. Vyssotsky, F. J. Corbato, R. M. Graham  
Fall Joint Computer Conference, 1965  
An early paper on Multics, which described many of the basic ideas and terms that we find in modern systems. Some of the vision behind computing as a utility are finally being realized in modern cloud systems.

\newpage

# 5. 間奏：プロセスAPI
>> ASIDE：INTERLUDES  
>> Interludesでは、オペレーティングシステムAPIに重点を置いて、それらを使用する方法など、システムのより実用的な側面について説明します。実用的なことが気に入らなければ、これらの間奏をスキップすることができます。しかし実用的なものが好きなのは、実は実生活では一般的に便利だからです。例えば、企業は、通常、あなたの非実用的なスキルのためにあなたを雇うことはありません。

この中で、UNIXシステムでのプロセス作成について説明します。UNIXは、`fork()`と`exec()`の2つのシステムコールを使用して、新しいプロセスを作成する最も興味深い方法の1つを提供します。3番目のルーチン`wait()`は、作成したプロセスが完了するのを待つプロセスによって使用できます。ここでは、これらのインターフェースをいくつかの簡単な例を挙げてより詳細に提示し、私たちの動機づけを示します。

>> CRUX：プロセスの作成と制御方法  
>> プロセスの作成と制御のためにOSが提示すべきインタフェースは何ですか？これらのインターフェイスは、使いやすさとユーティリティ性を実現するためにどのように設計されるべきですか？

## 5.1 システムコール `fork()`
`fork()`システムコールは新しいプロセス[C63]の作成に使用されます。具体的には、図5.1に示すようなコードを持つ実行プログラムがあります。コードを調べるか、それを入力して実行してください。

![](../05/img/fig5_1.png)

　p1.c.で何が起こったのかもっと詳しく知りましょう。最初に実行を開始すると、プロセスはhello worldメッセージを出力します。そのメッセージにはプロセス識別子(PIDとも呼ばれます)が含まれています。このプロセスのPIDは29146です。UNIXシステムでは、PIDは、(例えば)実行を停止するなど、プロセスで何かをしたい場合にプロセスの名前を付けるために使用されます。  
　ここで面白い部分が始まります。このプロセスは、OSが提供する`fork()`システムコールを呼び出して、新しいプロセスを作成します。作成されるプロセスは、呼び出しプロセスの(ほぼ)正確なコピーです。つまり、OSには、実行中のプログラムp1のコピーが2つあり、両方とも`fork()`システムコールから復帰しようとしているようです。新たに作成されたプロセス(親とは対照的に、子と呼ばれる)は`main()`で動作を開始しません。(「hello、world」メッセージは一度だけ出力されます。むしろ、`fork()`自体を呼び出したかのように、開始します。  
　気づいたかもしれません。子供は正確なコピーではありません。具体的には、アドレス空間(すなわち、自身のプライベートメモリ)のコピー、自身のレジスタ、それ自身のPCなどを持っていますが、`fork()`の呼び出し元に返す値は異なります。具体的には、親が新しく作成された子のPIDを受け取っている間、子はゼロの戻りコードを受け取ります。この区別は、2つの異なるケースを扱うコードを書くのが簡単なので(上記と同様に)便利です。

![](../05/img/fig5_2.png)

　また気づいたかもしれません。出力(p1.cの)は決定的ではありません。子プロセスが作成されると、システムには親プロセスと子プロセスの2つのアクティブなプロセスがあります。単一のCPUを持つシステムで実行していると仮定すると(単純化のため)、その時点で子プロセスまたは親プロセスが実行される可能性があります。上の例では、親がメッセージを最初に出力しました。それ以外の場合は、この出力トレースに示すように、逆の場合があります。

![](../05/img/fig5_2_1.png)

　CPUスケジューラは、すぐに詳細に議論するトピックで、特定の瞬間に実行されるプロセスを決定します。スケジューラは複雑であるため、通常は何を選択するのか、したがってどのプロセスが最初に実行されるのかについて前提はできません。この非決定性は、特にマルチスレッドプログラムでは、いくつかの興味深い問題につながります。したがって、本書の第2部で並行性を学ぶと、非決定性がさらに増えます。

## 5.2 システムコール `wait()`
　これまでのところ、メッセージを出力して終了する子供を作成しました。時には、親プロセスが子プロセスの処理を完了するのを待つことは非常に便利です。このタスクは、`wait()`システムコール(またはより完全な兄弟`waitpid()`)で実行されます。詳細は図5.2を参照してください。  
　この例(p2.c)では、親プロセスは`wait()`を呼び出して、子プロセスの実行が終了するまでその実行を遅延させます。子が終了すると、`wait()`は親に戻ります。  
　上記のコードに`wait()`を追加すると、出力が決定的になります。

![](../05/img/fig5_2_2.png)

　このコードでは、子が常に最初に表示されることがわかりました。なぜ我々はそれを知っていますか？前と同じように最初に走って、親の前にプリントするかもしれません。しかし、親が最初に実行されると、すぐに`wait()`が呼び出されます。このシステムコールは、子が実行されて終了するまで返されません。したがって、親が最初に実行されても、子が実行を終了するのを丁寧に待ってから`wait()`が戻り、親がそのメッセージを表示します。

## 5.3 最後、システムコール `exec()`
　プロセス作成APIの最終的かつ重要な部分は、`exec()`システムコールです。このシステムコールは、呼び出し元のプログラムとは異なるプログラムを実行する場合に便利です。たとえば、p2.cの`fork()`の呼び出しは、同じプログラムのコピーを実行し続ける場合にのみ便利です。しかし、多くの場合、別のプログラムを実行する必要があります。`exec()`はそれだけを行います(図5.3)。  
　この例では、子プロセスは`execvp()`を呼び出して、ワードカウントプログラムであるプログラムwcを実行します。実際には、ソースファイルp3.c上でwcを実行するので、ファイル内にいくつの行、単語、およびバイトがあるかがわかります。

![](../05/img/fig5_2_3.png)
![](../05/img/fig5_3.png)

　`fork()`システムコールは異常です。犯罪のパートナーである`exec()`は普通ではありません。実行可能ファイルの名前(wcなど)といくつかの引数(p3.cなど)を指定すると、実行可能ファイルからコード(および静的データ)をロードし、現在のコードセグメント(および現在の静的データ)を上書きします。プログラムのメモリ空間のヒープおよびスタックおよび他の部分が再初期化されます。その後、OSは単にそのプログラムを実行し、そのプロセスのargvとして引数を渡します。したがって、新しいプロセスは作成されません。むしろ、現在実行中のプログラム(以前はp3)を別の実行中のプログラム(wc)に変換します。子の中で`exec()`の後では、p3.cが決して走っていないかのようです。`exec()`への呼び出しが成功すると、決して戻りません。

## 5.4 どうして？APIの動機付け
　もちろん、あなたが持っているかもしれない1つの大きな疑問：なぜ新しいプロセスを作成する単純な行為でなければならないものに対して、このような奇妙なインタフェースを構築するのでしょうか？`fork()`と`exec()`の分離はUNIXシェルを構築する上で不可欠です。なぜなら、シェルが`fork()`の呼び出しの後、`exec()`の呼び出しの前にコードを実行できるからです。このコードは実行しようとしているプログラムの環境を変更することができ、したがって様々な興味深い機能を容易に構築することができます。

>> ヒント：それを得る(ラムプーンの法律)  
>> Lampson氏が彼の著名な「コンピュータシステム設計のヒント」[L83]で述べているように、「それを正しくしてください。抽象化もシンプルさも、それを正しくするための代替手段ではありません」。時には、正しいことをしなければならない時もあります。プロセス作成のためのAPIを設計する方法はたくさんあります。しかし、`fork()`と`exec()`の組み合わせは単純で非常に強力です。ここでは、UNIXデザイナーは単にそれを正しく理解しました。そしてLampsonはしばしば「正しい」と言いましたので、私たちはその名誉の中で法律を名づけました。

　シェルは単なるユーザープログラムです。それはプロンプトを表示し、何かを入力するのを待ちます。次に、コマンド(実行可能プログラムの名前に加えて任意の引数)を入力します。ほとんどの場合、シェルはファイルシステム内で実行可能ファイルがどこにあるのかを確認し、`fork()`を呼び出してコマンドを実行する新しい子プロセスを作成し、`exec()`のいくつかの変種を呼び出してコマンドを実行します。`Wait()`を呼び出してコマンドを実行することで完了します。子が完了すると、シェルは`wait()`から戻り、プロンプトをもう一度出力して、次のコマンドの準備をします。  
　`fork()`と`exec()`を分離することで、シェルは便利なものを簡単に扱うことができます。
```
prompt> wc p3.c > newfile.txt
```

　上記の例では、プログラムwcの出力が出力ファイルnewfile.txtにリダイレクトされます(より大きい記号はリダイレクトがどのように示されているかです)。シェルがこのタスクを達成する方法は非常に簡単です。子が作成されるとき、`exec()`を呼び出す前に、シェルは標準出力を閉じてファイルnewfile.txtを開きます。これにより、すぐに実行されるプログラムwcからの出力は、画面ではなくファイルに送信されます。  
　図5.4は、これを正確に実行するプログラムを示しています。このリダイレクションが機能する理由は、オペレーティングシステムがファイル記述子をどのように管理するかという仮定によるものです。具体的には、UNIXシステムはゼロでフリーファイル記述子を探し始めます。この場合、STDOUT FILENOが最初に使用可能になり、したがって`open()`が呼び出されるときに割り当てられます。その後、子プロセスによる`printf()`などのルーチンによる標準出力ファイル記述子への後続の書き込みは、画面ではなく新しく開いたファイルに透過的にルーティングされます。  
　次に、p4.cプログラムを実行した結果を示します。
![](../05/img/fig5_3_1.png)
![](../05/img/fig5_4.png)

　あなたは、この出力に関して(少なくとも)2つの面白い小物を気づくでしょう。まず、p4を実行すると、何も起こっていないかのように見えます。シェルはコマンドプロンプトを表示するだけで、すぐに次のコマンドの準備ができます。しかし、そうではありません。プログラムp4は実際に`fork()`を呼び出して新しい子を作成し、`execvp()`の呼び出しによってwcプログラムを実行しました。出力がファイルp4.outputにリダイレクトされたため、画面に出力が表示されません。次に、出力ファイルをキャッチすると、wcを実行したときに期待されるすべての出力が見つかることがわかります。  
　UNIXパイプも同様の方法で実装されますが、`pipe()`システムコールが実装されています。この場合、1つのプロセスの出力はカーネル・パイプ(すなわち、キュー)に接続され、別のプロセスの入力はその同じパイプに接続されます。したがって、1つのプロセスの出力がシームレスに次の入力に使用され、長くて有効なコマンドのチェーンを一緒につなげることができます。簡単な例として、ファイル内の単語を探し、その単語が何回出現したかを数えます。パイプとユーティリティはgrepとwcで簡単です。 grep -o foo file | wc -l とプロンプトに入力します。  
　最後に、プロセスAPIを高レベルでスケッチしただけですが、これらの呼び出しが学習され、消化されることについての詳細がはるかに多くあります。本書の第3部でファイルシステムについて話すときに、たとえばファイル記述子についてもっと学びます。今のところ、`fork()`/`exec()`の組み合わせは、プロセスを作成して操作する強力な方法であると言うだけで十分です。

>> ASIDE：RTFM - 人のページを読む
>> 　この本では、特定のシステムコールやライブラリ呼び出しを参照するときに、マニュアルページやマニュアルページを読むことを何度も指示します。マニュアルページは、UNIXシステム上に存在するオリジナルのドキュメント形式です。Webと呼ばれるものが存在する前にそれらが作成されたことに気付きます。  
>> 　マニュアルページを読むのに時間を費やすことは、システムプログラマの成長の鍵となる一歩です。それらのページにはたくさんの便利な小技が隠されています。特に便利なページには、使用しているシェル(tcsh、bashなど)のマニュアルページと、プログラムが作るシステムコール(戻り値とエラー条件が存在するかどうかを確認するためのもの)があります。  
>> 　最後に、マニュアルページを読むことで、いくつかの恥ずかしさを軽減できます。`fork()`の複雑さについて同僚に尋ねると、彼らは単に「RTFM」と返答するかもしれません。これは、あなたの同僚の読書の手引きを静かに促す方法です。

## 5.5 APIのその他の部分
　`fork()`、`exec()`、`wait()`以外にも、UNIXシステムのプロセスとやりとりするための多くのインタフェースがあります。たとえば、`kill()`システムコールは、スリープ状態に陥るか、死ぬか、その他の有用な命令を含む、シグナルをプロセスに送信するために使用されます。実際、シグナルサブシステム全体は、シグナルを受信して処理する方法を含め、プロセスに外部イベントを提供する豊富なインフラストラクチャを提供します。  
　便利な多くのコマンドラインツールがあります。たとえば、psコマンドを使用すると、実行中のプロセスを確認できます。psに渡す有用なフラグのマニュアルページを読んでください。ツールのトップは、システムのプロセスやCPUや他のリソースがどれくらい消費しているかを表示するので、非常に役立ちます。最後に、システムの負荷をすばやく把握するために使用できるさまざまな種類のCPUメーターがあります。たとえば、私たちはMacintosh Toolbars上で実行されているRaging MenaceソフトウェアのMenuMetersを常に保持しているので、いつどのCPUが使用されているかを見ることができます。一般的に、何が起こっているかについてのより多くの情報は、より良いものです。

## 5.6 要約
`fork()`、`exec()`、`wait()`のような、UNIXプロセスの作成を扱ういくつかのAPIを紹介しました。しかし、私たちは表面を見ただけです。詳細については、StevensとRago [SR05]、特にプロセス制御、プロセス関係、および信号に関する章を読んでください。そこには学ぶべきことががたくさんあります。

>> ASIDE：コーディングの原点
>> コーディングの宿題は、現代のオペレーティングシステムが提供しなければならない基本的なAPIのいくつかの経験を得るために、実際のマシンで実行するコードを書く小さな練習です。結局のところ、(おそらく)あなたはコンピュータ科学者なので、コーディングするのは正しいでしょうか？もちろん、本当に専門家になるためには、少し時間をかけてマシンをハッキングする必要があります。実際には、いくつかのコードを書いて、それがどのように動作するかを見るためのあらゆる言い訳を見つけてください。時間を費やし、あなたができることを知っている賢いマスターになりなさい。

## 宿題(コード)
この課題では、読んだばかりのプロセス管理APIに精通する必要があります。 心配する必要はありません - それは聞こえるよりもさらに楽しいです！ 一般的には、コードを書くことができるくらい多くの時間を見つけた方がずっと良いでしょう。

## 問題
1. `fork()`を呼び出すプログラムを記述します。 `fork()`を呼び出す前に、メインプロセスに変数(たとえばx)にアクセスさせ、その値を何か(例えば100)に設定させます。 子プロセスの変数はどのような値ですか？ 子と親の両方がxの値を変更すると、変数には何が起こりますか？
2. ファイルをオープンする(`open()`システムコールで)プログラムを作成し、`fork()`を呼び出して新しいプロセスを作成します。 子と親の両方が`open()`によって返されたファイル記述子にアクセスできますか？ 同時にファイルに書き込むとき、つまり同時に実行するとどうなりますか？
3. `fork()`を使用して別のプログラムを作成します。 子プロセスは "hello"を出力する必要があります。 親プロセスは "さようなら"を印刷する必要があります。 子プロセスが常に最初に印刷されるようにする必要があります。 あなたは親で`wait()`を呼び出さずにこれを行うことができますか？
4. `fork()`を呼び出し、`exec()`の何らかの形式を呼び出してプログラム/ bin / lsを実行するプログラムを記述します。 `execl()`、`execle()`、`execlp()`、`execv()`、`execvp()`、および`execvP()`を含むexec()のすべてのバリエーションを試すことができるかどうかを確認してください。 なぜ同じ基本的な呼び出しの多くの変種があると思いますか？
5. 子プロセスが親プロセスで終了するのを待つために`wait()`を使うプログラムを書いてください。 `wait()`は何を返しますか？ 子で`wait()`を使用するとどうなりますか？
6. `Wait()`の代わりに`waitpid()`を使用して、前のプログラムのわずかな変更を書き込んでください。 `waitpid()`はいつ有用でしょうか？
7. 子プロセスを作成し、子プロセスで標準出力(STDOUT FILENO)を閉じるプログラムを記述します。 子が記述子を閉じた後に`printf()`を呼び出して出力をプリントするとどうなりますか？
8. 2つの子を作成し、`pipe()`システムコールを使用して、標準出力をもう一方の標準入力に接続するプログラムを記述します。

# 参考文献
[C63] “A Multiprocessor System Design”  
Melvin E. Conway  
AFIPS ’63 Fall Joint Computer Conference  
New York, USA 1963  
An early paper on how to design multiprocessing systems; may be the first place the term fork() was used in the discussion of spawning new processes.  

[DV66] “Programming Semantics for Multiprogrammed Computations”  
Jack B. Dennis and Earl C. Van Horn  
Communications of the ACM, Volume 9, Number 3, March 1966  
A classic paper that outlines the basics of multiprogrammed computer systems. Undoubtedly had great influence on Project MAC, Multics, and eventually UNIX.

[L83] “Hints for Computer Systems Design”  
Butler Lampson  
ACM Operating Systems Review, 15:5, October 1983  
Lampson’s famous hints on how to design computer systems. You should read it at some point in your life, and probably at many points in your life.

[SR05] “Advanced Programming in the UNIX Environment”  
W. Richard Stevens and Stephen A. Rago  
Addison-Wesley, 2005  
All nuances and subtleties of using UNIX APIs are found herein. Buy this book! Read it! And most importantly, live it.

\newpage

# 6. Mechanism: Limited Direct Execution

CPUを仮想化するためには、オペレーティングシステムは、一見同時に実行している多くのジョブの中で何らかの形で物理CPUを共有する必要があります。基本的なアイデアは簡単です。プロセスを1つ実行してから別のプロセスを実行するなどです。このようにCPUをtime sliceで共有することにより、仮想化が実現されます。

しかし、そのような仮想化機構を構築するには、いくつかの課題があります。1つはパフォーマンスです。システムに過度のオーバーヘッドを加えずに、どのように仮想化を実装できるでしょうか？もう1つは資源管理です。CPUを制御できなくなることなしにプロセスを効率的に実行するにはどうすればよいでしょうか？OSにとっては特に資源管理が重要です。というのも、OSは資源に関しての担当者だからです。資源管理がなければ、プロセスは単に永遠に実行するだけでマシンを乗っ取ることができてしまい、アクセスを許可したくない情報にアクセスすることもできてしまいます。したがって、制御を失わずに高いパフォーマンスを得ることは、オペレーティングシステムを構築する上での中心的な課題の1つです。


>> THE CRUX:HOW TO EFFICIENTLY VIRTUALIZE THE CPU WITH CONTROL(CPUをコントロールと効率的に仮想化する方法)  
OSは、システムの制御を維持しながら効率的にCPUを仮想化する必要があります。そのためには、ハードウェアとオペレーティングシステムの両方のサポートが必要になります。OSは、その作業を効果的に達成するために、しばしば賢明なハードウェアサポートを使用します。(優れているハードウェアサポートであるほど効率が上がる。OSサポートも同様)

## 6.1 Basic Technique: Limited Direct Execution
プログラムが期待どおりの速さで動作するようにするために、OS開発者は限定されたdirect executionと呼ばれる技術を思いついたわけではありません。アイデアの「direct executin」の部分は単純です。プログラムをCPU上で直接実行するだけです。したがって、OSがプログラムの実行を開始したいときには、プロセスリストにプロセスエントリを作成し、プロセスリストにいくつかのメモリを割り当て、プログラムコードをメモリにロードし(ディスクから)、エントリポイントを見つけます(`main()`ルーチンやそれに似たもの)、ジャンプしてユーザーのコードの実行を開始します。図6.1は、この基本的なdirect executionプロトコルを示しています(まだ制限はありません)。通常の呼び出しを使用して、戻り先である、プログラムの`main()`にジャンプし、後でカーネルに戻ります。

![](../06/img/fig6_1.PNG)

シンプルに聞こえるでしょう？しかし、このアプローチでは、CPUを仮想化するためのいくつかの問題が生じます。最初はの問題は簡単です。プログラムを実行するだけの場合、OSはプログラムを効率的に実行しながら、プログラムで実行したくないことをOSがどのように判断することができますか？2つ目は、プロセスを実行しているときに、オペレーティングシステムが実行を停止して別のプロセスに切り替える方法です。つまり、CPUを仮想化するために必要なtime sliceを実装しますか？

これらの質問に答えるにあたって、CPUを仮想化するために必要なことをもっとよく理解していきます。これらの技術を開発する際には、名前の「限定された」部分がどこから生じているのかもわかります。OSを実行しているプログラムに制限を加えることなく、OSは何の制御もされないまるで「単なるライブラリ」です。夢のあるオペレーティングシステム実現するための非常に悲しい現状のOSの状態です！

## 6.2 Problem #1: Restricted Operations
直接実行には高速という明白な利点があります。プログラムはハードウェアCPU上でネイティブに実行されるため、期待どおりの速さで実行されます。しかし、CPU上で実行すると、ある問題が生じてしまいます。例えば、ディスクにI/O要求を発行したり、CPUやメモリなどのより多くのシステムリソースにアクセスするなど、何らかの制限された操作を実行したい場合はどうなるでしょうか？


>> THE CRUX: HOW TO PERFORM RESTRICTED OPERATIONS(制限付き操作を実行する方法)  
>> プロセスはI/Oなどの制限付きの操作を実行できる必要がありますが、プロセスがシステム全体を完全に制御することはできません。OSとハードウェアはどのように連携して動作するのでしょうか？

>> ASIDE: WHY SYSTEM CALLS LOOK LIKE PROCEDURE CALLS(システムコールがプロシージャコールのように見える理由)  
>> `open()`や`read()`のようなシステムコールへの呼び出しがC言語の典型的な手続き呼び出しとまったく同じように見えるのはなぜかと思うでしょう。つまり、プロシージャコールのように見える場合、システムはシステムコールであるとはどのように判断できるでしょうか？実は、プロシージャコールですが、そのプロシージャコールの中に隠されているのは有名なトラップ命令なのです。  
>> 具体的には、`open()`(たとえば)を呼び出すと、Cライブラリにプロシージャコールを実行しています。そこでは、`open()`やその他のシステムコールのいずれにしても、ライブラリは、カーネルとの間で合意した呼び出し規約を使用して、引数を既知の場所(スタックや特定のレジスタ)、システムコール番号をよく知られた場所に(スタックまたはレジスタ上に)入れ、前述のトラップ命令を実行します。  
>> トラップ後のライブラリのコードは、戻り値をアンパックし、システムコールを発行したプログラムに制御を返します。したがって、システムコールを実行するCライブラリの部分は、引数を正しく処理して値を正しく返し、ハードウェア固有のトラップ命令を実行するために慣習に注意深く従う必要があるため、アセンブリで手作業でコーディングされます。では、なぜあなたが個人的にOSにトラップするアセンブリコードを書く必要がないのでしょうか？それは、誰かがすでにあなたのためにそのアセンブリを書いてくれているからです。

1つのアプローチは、I/Oやその他の関連する操作の観点から、任意のプロセスに必要なものを実行させることです。しかしそうすることは、望ましい多くの種類のシステムの構築を妨げるでしょう。たとえば、ファイルへのアクセスを許可する前に、アクセス権をチェックするファイルシステムを構築するとしましょう。(アノニマスユーザの権限でアクセス権をチェックをするものを構築する)そうすると、ディスク全体の読み書きができなくなり、すべての保護が失われしまいます。

したがって、我々が取るアプローチは、ユーザモードとして知られる新しいプロセッサモードを導入することです。ユーザーモードで実行されるコードは、できることで制限されています。たとえば、ユーザーモードで実行している場合、プロセスはI/O要求を発行できません。そのようにすると、プロセッサは例外を発生させます。OSはプロセスを終了させる可能性があります。

ユーザーモードとは対照的に、オペレーティングシステム(またはカーネル)が実行されるカーネルモードは、I/O要求の発行やすべてのタイプの制限付き命令の実行などの特権操作を含む実行可能なコードです。

ただし、ディスクからの読み取りなど、何らかの特権操作を実行する場合、ユーザーの処理はどうすればよいでしょうか？これを可能にするために、ほぼすべての現代のハードウェアは、ユーザープログラムがシステムコールを実行する能力を提供します。Atlas [K + 61、L78]のような古代のマシンで開発されたシステムコールでは、カーネルは、ファイルシステムへのアクセス、プロセスの作成と破棄、他のプロセスとの通信など、特定の重要な機能をユーザープログラムに慎重に公開することができます。より多くのメモリを割り当てることができます。ほとんどのオペレーティングシステムは数百個の呼び出しを提供します(詳細はPOSIX標準を参照してください)[P10]。初期のUnixシステムでは、約20コールのより簡潔なサブセットが公開されていました。

>> TIP: USE PROTECTED CONTROL TRANSFER(保護された制御転送を使用する)  
>> ハードウェアは、異なる実行モードを提供することによってOSを支援する。ユーザーモードでは、アプリケーションはハードウェアリソースに完全にアクセスできません。カーネルモードでは、OSはマシンの全リソースにアクセスできます。カーネルやトラップからユーザーモードのプログラムに戻すための特別な指示や、OSがトラップテーブルがメモリ上にあるハードウェアに指示するための指示も提供されています。

システムコールを実行するには、プログラムが特別なトラップ命令を実行する必要があります。この命令は同時にカーネルにジャンプし、特権レベルをカーネルモードに上げます。一度カーネル内で実行されると、システムは必要な特権操作(許可されている場合)を実行できるようになり、呼び出しプロセスに必要な作業を行うことができます。終了すると、OSは特別なreturn from trap命令(トラップ帰還命令)を呼び出します。これは、呼び出し元のユーザープログラムに戻り、同時に特権レベルをユーザーモードに戻します命令です。

トラップを実行するときは、OSがreturn from trap命令を発行したときに正しく戻るために、呼び出し元のレジスタを十分に保存する必要があるという点で、ハードウェアは少し注意する必要があります。たとえばx86では、プロセッサはプログラムカウンタ、フラグ、その他のいくつかのレジスタをプロセスごとのカーネルスタックにプッシュします。return from trapはこれらの値をスタックからポップし、ユーザー・モード・プログラムの実行を再開します(詳細については、インテルのシステム・マニュアル[I11]を参照してください)。他のハードウェアシステムでは異なる表記法が使用されていますが、基本コンセプトはプラットフォーム間で似ています。

ここで、詳細な説明を飛ばしていた重要なものが1つあります。トラップはどのコードをOS内部でどのように実行するのかを知っているでしょうか？ということです。答えは明白で、呼び出し元のプロセスはジャンプするアドレスを指定できません(プロシージャを呼び出すときと同じように)。もし、呼び出し元のプロセスがアドレスを指定できたとしましょう。そうすると、プログラムがカーネル内のどこにでもジャンプすることができるといった、非常に悪い考えになってしまうのです。したがって、カーネルは、トラップで実行されるコードを慎重に制御する必要があります。

カーネルは、起動時にトラップテーブルを設定することで、そのようにします。マシンが起動すると、特権(カーネル)モードで実行されるので、必要に応じてマシンのハードウェアを自由に設定することができます。OSが最初に行うことの1つは、例外的なイベントが発生したときに実行するコードをハードウェアに伝えることです。たとえば、ハードディスク割り込みが発生したとき、キーボード割り込みが発生したとき、またはプログラムがシステムコールを行うときに、どのコードを実行すべきでしょうか？OSは、これらのトラップハンドラの場所をハードウェアに通知します。通常、何らかの特別な指示があります。ハードウェアに通知されると、マシンが次にリブートされるまで、これらのハンドラの位置が記憶されます、これにより、システムコールおよび他の例外的なイベントが発生したときに何をするか(すなわち、ジャンプするコード)を知ります。

![](../06/img/fig6_2.PNG)

正確なシステムコールを指定するには、通常、各システムコールにシステムコール番号が割り当てられます。したがって、ユーザコードは、呼び出したいのシステムコール番号をレジスタまたはスタック上の指定された位置に配置する役割を担います。OSはトラップハンドラ内でシステムコールを処理するときにこの番号を調べ、有効であることを確認し、有効であれば対応するコードを実行します。このレベルの間接指定は、保護の一種として機能します。ユーザーコードは、ジャンプ先の正確なアドレスを指定することはできず、numberを介して特定のサービスを要求する必要があります。

最後に、トラップテーブルがどこにあるかをハードウェアに伝える命令を実行できることは、非常に強力な機能です。つまり、特権操作でもあります。この命令をユーザモードで実行しようとすると、ハードウェアはこれを許可しません。(ヒント：adios、問題のプログラム)。もし、あなた自身のトラップテーブルをインストールすることができたら、システムはどんな恐ろしいことができるでしょうか？そんなマシンをあなたは引き継ぎたいですか？

タイムライン(図6.2の時間が下がるにつれて)は、プロトコルをまとめたものです。各プロセスには、カーネルに出入りする際にレジスタ(汎用レジスタとプログラムカウンタを含む)が(ハードウェアによって)保存され、そこから復元されるカーネルスタックがあると仮定します。

LDEプロトコルには2つのフェーズがあります。最初の(ブート時に)カーネルはトラップテーブルを初期化し、CPUはその後の使用のためにその場所を覚えています。カーネルは特権命令(すべての特権命令は太字で強調表示されています)を介して行います。

2番目のプロセス(プロセス実行時)では、カーネルは、プロセスの実行を開始するためにreturn from trap命令を使用する前に、いくつかのことを設定します(たとえば、プロセスリストにノードを割り当て、メモリを割り当てる)。CPUをユーザーモードに切り替え、プロセスの実行を開始します。プロセスがシステムコールを発行することを望む場合、プロセスはそれを処理するOSに再びトラップし、再びトラップからプロセスへの戻り値を介して制御を返します。プロセスはその作業を完了し、`main()`から戻ります。これは通常、プログラムを適切に終了するスタブコードに戻ります(たとえば、OSにトラップする`exit()`システムコールを呼び出して)。この時点で、OSはクリーンアップされ、実行されます。

## 6.3 Problem #2: Switching Between Processes
次のdirect executionの問題は、プロセス間の切り替えを実現することです。プロセス間の切り替えは簡単ですね。OSは1つのプロセスを停止し、別のプロセスを開始するだけで済むはずです。ではいったい何が大きな問題だろうか？しかし、実際にはややこしいことです。具体的には、プロセスがCPU上で実行されている場合、これは定義上、OSが実行されていないことを意味します。OSが稼働していない場合、どうすれば何ができるのですか？(ヒント：できない)これは哲学的に聞こえますが、実際の問題です。つまり、CPUで実行されていない場合、OSがアクションを実行する方法がないです。

>> THE CRUX: HOW TO REGAIN CONTROL OF THE CPU(CPUの制御を元に戻す方法)
>> どのようにして、オペレーティングシステムはCPUを制御してプロセス間を切り替えることができますか？

### A Cooperative Approach: Wait For System Calls
過去にいくつかのシステムが採用してきたアプローチ(例えば、Macintoshオペレーティングシステムの旧バージョン[M11]や古いXerox Altoシステム[A79])は、協調的アプローチとして知られています。このスタイルでは、OSはシステムのプロセスが合理的に動作することを信頼します。長時間実行されるプロセスは、CPUが定期的にCPUを放棄して、OSが他のタスクを実行することを決定できると想定されます。

したがって、このユートピアの世界で友好的なプロセスがCPUを放棄するのはどのようなものでしょうか？ほとんどのプロセスは、例えば、ファイルを開いて読み込んだり、別のマシンにメッセージを送信したり、新しいプロセスを作成したりするなど、システムコールを頻繁に行うことによって、CPUのOSへの制御をかなり頻繁に転送します。このようなシステムには、明示的なシステムコールが含まれていることが多く、OSに制御を渡す以外は何もしないので、他のプロセスを実行できます。

アプリケーションは、何か違法行為をした場合にも、OSに制御を移します。たとえば、アプリケーションがゼロで割るか、アクセスできないメモリにアクセスしようとすると、OSにトラップが生成されます。その後、OSはCPUの制御を再度行います(問題のプロセスを終了させる可能性があります)。

したがって、協調スケジューリングシステムでは、システムコールや何らかの不正な操作が行われるのを待って、OSがCPUの制御を取り戻します。また、あなたはこの受動的なアプローチが理想よりも劣っているのではないかと考えているかもしれません。たとえば、プロセス(悪意のあるバグや完全なバグなど)が無限ループで終了し、システムコールを実行しない場合などはどうなりますか？OSは何をすることができますか？

>> TIP: DEALING WITH APPLICATION MISBEHAVIOR(誤った動きをするアプリケーションの処理の仕方)
>> オペレーティングシステムは、設計(悪意のある)または事故(バグ)のいずれかを行うべきでない何かをしようとするプロセスに対処しなければならないことがよくあります。最新のシステムでは、OSがこのような不正行為を処理しようとする方法は、単に犯人を終了させることです。ワンストライク！バッターアウト！やり方としては残酷かもしれませんが…。ところで、違法にメモリにアクセスしたり、違法な命令を実行しようとすると、OSは何をすべきでしょうか？

### A Non-Cooperative Approach: The OS Takes Control
ハードウェアの追加の助けがなければ、プロセスがシステムコール(またはミス)を拒否してOSに制御を戻すことができない場合、OSはまったく何もできません。実際、協調的なアプローチでは、プロセスが無限ループに陥ってしまったときの唯一の手段は、コンピュータシステムのすべての問題に対する古くからの解決策であるマシンを再起動することです。

>> THE CRUX: HOW TO GAIN CONTROL WITHOUT COOPERATION(協力なしに制御を得る方法)
>> プロセスが協調的でなくても、OSはどのようにしてCPUを制御できますか？不正なプロセスがマシンを占有しないようにするには、OSは何ができますか？

>> TIP: USE THE TIMER INTERRUPT TO REGAIN CONTROL(制御装置をリセットするためにタイマ割り込みを使用する)
>> タイマー割込みを追加することで、プロセスが非協調的な方法で動作しても、CPU上でOSを再実行することができます。したがって、このハードウェア機能は、OSがマシンの制御を維持するのを助ける上で不可欠です。

その答えは単純です。何年も前にコンピュータシステムを構築している多くの人々によって発見されました。それは、timer interrupt[M + 63]を使うことです。タイマー装置は非常に多くのミリ秒ごとに割り込みを発生させるようにプログラムすることができます。割り込みが発生すると、現在実行中のプロセスが停止し、OS内の事前設定された割り込みハンドラが実行されます。この時点で、OSはCPUの制御を取り戻しました。したがって、現在のプロセスを停止し、別のプロセスを開始することができます。

以前にシステムコールで説明したように、OSはタイマ割り込みが発生したときに実行するコードをハードウェアに通知する必要があります。したがって、ブート時には、OSはまさに実行するコードをハードウェアに通知することを行います。第2に、ブートシーケンス中にも、OSはタイマーを開始しなければいけません。これはもちろん特権操作です。タイマーが開始されると、OSは制御が最終的に特権操作で戻されるという点、OSが自由にユーザープログラムを実行できるという点で安全です。タイマーは、(特権操作でもある)電源をオフにすることができます。これについては、同時実行性のところでより詳しく説明します。

割り込みが発生したとき、特に割り込みが発生したときに実行されていたプログラムの状態を十分に保存して、その後のreturn from trap命令が実行中のプログラムを正常に再開できるようにするには、ハードウェアに若干の責任があることに注意してください。この一連のアクションは、カーネルへの明示的なシステムコールトラップ中のハードウェアの振る舞いと非常によく似ています。具体的には、return from trap命令を用いて、さまざまなレジスタが(カーネルスタックに)簡単に保存と復元が行われます。

### Saving and Restoring Context
OSが制御を取り戻したので、システムコールを介して、またはタイマー割り込みを介してより強力に、現在実行中のプロセスを継続して実行するか、別のプロセスに切り替えるかを決定する必要があります。この決定はスケジューラと呼ばれるオペレーティングシステムの一部によって行われます。次のいくつかの章でスケジューリング方針について詳しく説明します。

切り替えが決定された場合、OSはコンテキストスイッチと呼ばれる低レベルのコードを実行します。コンテキストスイッチは概念的に単純です。すべてのOSが実行する必要があるのは、現在実行中のプロセス(例えば、カーネルスタック上にある)にいくつかのレジスタ値を保存し、すぐに実行するプロセスのいくつかを復元することです(そのカーネルスタックから)。そうすることにより、OSは、return from trap命令が最終的に実行されたときに、実行中のプロセスに戻るのではなく、別のプロセスの実行を再開するようにします。

![](../06/img/fig6_3.PNG)

現在実行中のプロセスのコンテキストを保存するために、OSは、汎用レジスタ、PC、および現在実行中のプロセスのカーネルスタックポインタを保存するために、いくつかの低レベルのアセンブリコードを実行し、PCを起動し、すぐに実行されるプロセスのためにカーネルスタックに切り替えます。スタックを切り替えることによって、カーネルは1つのプロセス(中断されたプロセス)のコンテキストでスイッチコードへの呼び出しに入り、別のプロセス(直ちに実行されるもの)のコンテキストで戻ります。そして、OSが最終的にreturn from trap命令を実行すると、直ちに実行されるプロセスが現在実行中のプロセスになります。したがって、コンテキストスイッチは完了です。

プロセス全体のタイムラインを図6.3に示します。この例では、プロセスAが実行されていて、タイマー割り込みによって中断されています。ハードウェアはレジスタをカーネルスタックに保存し、カーネルに入ります(カーネルモードに切り替える)。タイマ割込みハンドラでは、OSは実行中のプロセスAからプロセスBに切り替えることを決定します。その時点で、現在のレジスタ値を(Aのプロセス構造に)慎重に保存する`switch()`ルーチンを呼び出し、(そのプロセス構造体のエントリから)Bを処理し、具体的にはBのカーネルスタックを使用するようにスタックポインタを変更することによってコンテキストを切り替えます。最後に、OSはreturn from trap命令を使用して、Bのレジスタを復元して実行します。

このプロトコル中に起こるレジスタセーブ/リストアには2種類のタイプがあることに注意してください。1つは、タイマ割り込みが発生したときです。この場合、実行中のプロセスのユーザー・レジスタは、そのプロセスのカーネル・スタックを使用して、ハードウェアによって暗黙的に保管されます。2つめは、OSがAからBに切り替えることを決定したときです。この場合、カーネルレジスタはソフトウェア(すなわちOS)によって明示的に保存されますが、今回はプロセスのプロセス構造内のメモリに保存されます。後者の動作では、Aがカーネルにトラップされた後、Bがカーネルにトラップされ、システムが実行します。

そのようなスイッチがどのように制定されているかを理解するために、図6.4にxv6のコンテキストスイッチコードを示します。あなたがそれを理解できるかどうかを確認してください(そうするためには、xv6といくつかのxv6について知っておく必要があります)。コンテキスト構造oldとnewは、それぞれ古いプロセスのプロセス構造と新しいプロセスのプロセス構造にあります。  
![](../06/img/fig6_4.PNG)

## 6.4 Worried About Concurrency?
「システムコール中にタイマー割り込みが発生するとどうなるのですか？」または「1つの割り込みを処理して別のものが起きたときにどうなりますか？」と思っている人もいます。カーネルで扱うのが難しくないのですか？

答えはYES!です。OSは、割り込みやトラップの処理中に別の割り込みが発生した場合に何が起こるかについて、実際に懸念する必要があります。これは、実際にはこの本の第2部全体の正確な話題であり、並行性に関するものです。それまでは詳細な議論を延期する予定です。

あなたの知識欲求を掻き立てるために、OSがどのようにこれらのトリッキーな状況を処理するかの基本をいくつか紹介します。OSが行う簡単なことの1つは、割り込み処理中に割り込みを無効にすることです。これにより、1つの割り込みが処理されるときに、他の割り込みがCPUに渡されることはありません。もちろん、OSはそうすることに注意する必要があります。なぜなら、割り込みを長時間無効にすると割り込みが失われる可能性があるからです(つまり、技術的には悪い)。

また、オペレーティングシステムは、内部データ構造への同時アクセスを保護するために、多くの洗練されたロック方式を開発しています。これにより、複数のアクティビティをカーネル内で同時に実行することができます。特に、マルチプロセッサで便利です。しかし、この本の次の記事では並行性について説明しますが、このようなロックは複雑になり、さまざまな興味深い見つけにくいバグにつながります。

>> ASIDE: HOW LONG CONTEXT SWITCHES TAKE(コンテクストスイッチにどのくらい時間がかかるか)  
>> とある好奇心がある人が質問をしたとしましょう。たとえば、「コンテキストスイッチのようなものがどれくらい時間がかかりますか？」または、「あるいはシステムコールはどのくらい時間がかかりますか？」という質問のために、lmbench [MS96]というツールがあります。  
上記の質問のことを正確に測定するだけでなく、関連性のあるいくつかのパフォーマンス指標も測定します。結果は、時間の経過と共にかなり改善され、プロセッサの性能を大まかに追跡しています。たとえば、1996年にLinux 1.3.37を200 MHz P6 CPU上で実行すると、システムコールは約4マイクロ秒かかり、コンテキストスイッチは約6マイクロ秒でした[MS96]  
現代のシステムは、1〜2GHzまたは3GHzのプロセッサを搭載したシステムでは、1マイクロ秒未満の結果でほぼ一桁の性能を発揮します。すべてのオペレーティングシステムの動作がCPUのパフォーマンスを追跡するわけではないことに注意してください。Ousterhoutが観察しているように、多くのOS操作はメモリを大量に消費しており、メモリ帯域幅はプロセッサ速度のように大幅に向上していません[O90]。  
したがって、あなたの仕事量によっては、最新かつ最高のプロセッサーを購入しても、希望通りの速度でOSが動作するわけではありません。


## 6.5 Summary
我々は、限定されたdirect executionと総称するCPU仮想化を実装するためのいくつかの重要な低レベルのメカニズムについて説明しました。基本的な考え方は簡単です。CPU上で実行したいプログラムを実行するだけですが、ハードウェアをセットアップして、OSの支援なしにプロセスができることを制限するようにしてください。この一般的なアプローチは実生活でも行われます。たとえば、子供がいると考えてみましょう。危険なものを入れたキャビネットをロックし、電気ソケットをカバーします。このように部屋が準備されたら、部屋の最も危険な面が制限されていることを知って、赤ちゃんは自由に歩き回ること(ベビープルーフ)ができます。

同様の方法で、OSも考えることができます。OSは最初に(ブート時に)トラップハンドラを設定し、割り込みタイマーを起動し、制限されたモードでプロセスを実行するだけで、CPUを「ベビープルーフ」します。そうすることで、OSがプロセスを効率的に実行でき、特権操作を実行するためにOSの介入を必要とするだけでなく、CPUを長時間独占してスイッチアウトする必要がある場合もあります。

したがって、CPUを仮想化するための基本的なメカニズムがあります。しかし、重要な質問は答えられていません。それは、「どのプロセスが所定の時間に実行すべきですか？」という質問です。これは、スケジューラが答えなければならない質問です。したがって、私たちの次の調査トピックはスケジューラになります。

>> TIP: REBOOT IS USEFUL  
>> 以前は、協調的なプリエンプションのもとでの無限ループ(および類似の動作)に対する唯一の解決策は、マシンを再起動することでした。このハックを嘲笑するかもしれませんが、研究者は再起動(または一般的にソフトウェアの一部を起動)は堅牢なシステムを構築する上で非常に有用なツールになることを示しています[C+04]  
具体的には、再起動はソフトウェアを既知のテスト済みの状態に戻すので便利です。また、再起動すると、そうでなければ扱いにくい古いまたは漏れたリソース(例えば、メモリ)を再利用する。最後に、再起動は簡単に自動化できます。これらの理由から、システム管理ソフトウェアの大規模クラスタ・インターネット・サービスでは、マシンをリセットして上記の利点を得るために、一連のマシンを定期的に再起動することは珍しいことではありません。  
したがって、次に再起動すると、ちょっと醜いハックを制定するだけではありません。むしろ、コンピュータシステムの動作を改善するために時間をかけてテストされたアプローチを使用しています。やったぜ！

# 参考文献

[A79] “Alto User’s Handbook”  
Xerox Palo Alto Research Center, September 1979  
Available: http://history-computer.com/Library/AltoUsersHandbook.pdf  
An amazing system, way ahead of its time. Became famous because Steve Jobs visited, took notes, and built Lisa and eventually Mac.

[C+04] “Microreboot — A Technique for Cheap Recovery”  
George Candea, Shinichi Kawamoto, Yuichi Fujiki, Greg Friedman, Armando Fox  
OSDI ’04, San Francisco, CA, December 2004  
An excellent paper pointing out how far one can go with reboot in building more robust systems.

[I11] “Intel 64 and IA-32 Architectures Software Developer’s Manual”  
Volume 3A and 3B: System Programming Guide  
Intel Corporation, January 2011  

[K+61] “One-Level Storage System”  
T. Kilburn, D.B.G. Edwards, M.J. Lanigan, F.H. Sumner  
IRE Transactions on Electronic Computers, April 1962  
The Atlas pioneered much of what you see in modern systems. However, this paper is not the best one to read. If you were to only read one, you might try the historical perspective below [L78].

[L78] “The Manchester Mark I and Atlas: A Historical Perspective”  
S. H. Lavington  
Communications of the ACM, 21:1, January 1978  
A history of the early development of computers and the pioneering efforts of Atlas.

[M+63] “A Time-Sharing Debugging System for a Small Computer”  
J. McCarthy, S. Boilen, E. Fredkin, J. C. R. Licklider  
AFIPS ’63 (Spring), May, 1963, New York, USA  
An early paper about time-sharing that refers to using a timer interrupt; the quote that discusses it: “The basic task of the channel 17 clock routine is to decide whether to remove the current user from core and if so to decide which user program to swap in as he goes out.”

[MS96] “lmbench: Portable tools for performance analysis”  
Larry McVoy and Carl Staelin  
USENIX Annual Technical Conference, January 1996  
A fun paper about how to measure a number of different things about your OS and its performance. Download lmbench and give it a try.

[M11] “Mac OS 9”  
January 2011  
Available: http://en.wikipedia.org/wiki/Mac OS 9  

[O90] “Why Aren’t Operating Systems Getting Faster as Fast as Hardware?”  
J. Ousterhout  
USENIX Summer Conference, June 1990  
A classic paper on the nature of operating system performance.  

[P10] “The Single UNIX Specification, Version 3”  
The Open Group, May 2010  
Available: http://www.unix.org/version3/  
This is hard and painful to read, so probably avoid it if you can.  

[S07] “The Geometry of Innocent Flesh on the Bone:  
Return-into-libc without Function Calls (on the x86)”  
Hovav Shacham  
CCS ’07, October 2007  
One of those awesome, mind-blowing ideas that you’ll see in research from time to time. The author shows that if you can jump into code arbitrarily, you can essentially stitch together any code sequence you like (given a large code base); read the paper for the details. The technique makes it even harder to defend against malicious attacks, alas.

\newpage

# 7. Scheduling: Introduction

現時点で、実行中のプロセスの低レベルのメカニズム(例えば、コンテキストの切り替え)は明確にわかっていないといけません。そうでない場合は、1つまたは2つの章に戻って、低レベルのメカニズムがどのように機能するかの説明を読んで理解してください。しかし、OSスケジューラが採用している高度なポリシーについてはまだ理解していません。これからは、さまざまな人々が長年にわたって開発してきた一連のスケジューリング方針(時には専門分野と呼ばれる)を提示します。

スケジューリングの起点は、実際には、コンピュータシステム以前からあります。初期のアプローチは運用管理の分野から取り上げられ、コンピュータに適用されました。このことは少しも不思議でないでしょう、流れ作業での組み立てやその他多くのことがらにはスケジューリングが必要であり、同様の問題の多くはそこにも存在するのですから。したがって、私たちの問題は以下のようになります。

>> THE CRUX: HOW TO DEVELOP SCHEDULING POLICY(スケジューリング方針を開発する方法)  
>> スケジューリング方針について考えるための基本的なフレームワークをどのように開発する必要があるでしょうか？主要な前提は何でしょうか？どのような指標が重要になるでしょうか？コンピュータシステムの初期段階ではどのような基本的なアプローチが使用されているのでしょうか？

## 7.1 Workload Assumptions
適応可能なポリシーの範囲に入る前に、まずシステム内で実行されているプロセスについて簡単に仮定を立てましょう。時にはまとめて仕事量と呼ばれます。仕事量を決定することは、ポリシー作成の重要な部分であり、仕事量について知るほどポリシーが細かく調整されます。

私たちがここで作った仕事量の仮定はほとんど非現実的ですが、それは問題ありません。私たちが行くにつれてリラックスし、最終的には...(思考停止)すべてに対応できる運用スケジューリング規律になるでしょう。
システムで実行されているプロセス(ジョブとも呼ばれる)について、次の前提を設定します。

1. 各ジョブは同じ時間実行されます。
2. すべてのジョブが同時に到着します。
3. 開始されると、各ジョブは完了まで実行されます。
4. すべてのジョブはCPUのみを使用する(すなわち、I/Oを実行しない)
5. 各ジョブの実行時間は既知である。

これらの前提の多くは非現実的であると言いましたが、オーウェルの動物園ではいくつかの動物が他の動物よりも同等であるように[O45]、いくつかの仮定はこの章の他の仮定よりも非現実的です。特に、各ジョブの実行時間はわかっているかもしれません。スケジューラはすべてを把握しているでしょう。しかし、スケジューラはすべてを把握できるようになりますが、すぐにはうまくいかないでしょう。

## 7.2 Scheduling Metrics
仕事量の前提をする以外にも、スケジューリング・メトリックという異なるスケジューリング・ポリシーを比較できるようにするには、もう1つ必要です。メトリックは、何かを測定するために使用するものであり、スケジューリングに意味があるさまざまなメトリックがあります。

しかし今のところ、単一の指標、つまりターンアラウンド・タイムを単に取るだけで、私たちの人生を単純化しましょう。ジョブの所要時間は、ジョブが完了した時刻からジョブがシステムに到着した時刻を引いた時刻として定義されます。より正式には、ターンアラウンドタイム$T_{turnaround}$は次の通りです:

$T_{turnaround} = T_{completion} - T_{arrival}$ (7.1)

すべての仕事が同時に到着すると仮定しているので、今のところ$T_{arrival} = 0$、したがって$T_{turnaround} = T_{completion}$.です。この事実は、前述の前提条件を緩和するにつれて変化します。ターンアラウンドタイムはパフォーマンスメトリックであることに注意してください。もう一つの測定基準は、Jainの公平指数(Jain's Fairness Index [J91])によって測定された公平さです。パフォーマンスと公平さは、スケジューリングにおいてよく不安定にさせるものです。例えば、スケジューラは性能を最適化することができるが、いくつかのジョブが実行されるのを防止し、公平性を低下させるといったことが考えられます。この難解は「人生はいつも完璧というわけではない」ことを示しています。

## 7.3 First In, First Out (FIFO)
実装可能な最も基本的なアルゴリズムは、FIFO(First In、First Out)スケジューリング、またはFirst Come、First Served FCFS)と呼ばれます。  
FIFOにはいくつかの肯定的な特性があります。これは明らかに単純で実装が容易です。そして、私たちの前提を考えれば、それはかなりうまくいくでしょう。簡単な例を一緒に考えましょう。ほぼ同時に($T_{arrival} = 0$). )、3つのジョブがシステムA、B、Cに到着したとします。FIFOは三つのうちからどれかをとって仕事を最初にしなければいけないので、Aが同時に到着したときに、AはCの前に僅差で到着したばかりのBの前に僅差で到着したと仮定しましょう。これらのジョブの平均所要時間はどのくらいですか？

![](../07/img/fig7_1.PNG)

図7.1から、Aは10で終了し、Bは20で、Cは30で終了したことがわかります。したがって、3つのジョブの平均所要時間は単純に$\frac{10+20+30}{3}=20$。ターンアラウンドタイムの計算は簡単です。

５つの前提条件のうちの1つを緩和させましょう。特に、仮説1を緩和して、それぞれのジョブが同じ時間実行されると仮定しないようにしてください。FIFOは今どのように機能しますか？FIFOのパフォーマンスを落とさせるためには、どのような仕事量を構築できますか？  
とりあえず、異なる長さのジョブがどのようにFIFOスケジューリングの問題を引き起こすかを示す例を見てみましょう。特に3つのジョブ(A、B、C)を仮定しますが、この時間Aは100秒、BとCはそれぞれ10回実行されます。

![](../07/img/fig7_2.PNG)

図7.2に示すように、ジョブAは100秒間完全に実行されてから、BまたはCが実行されます。したがって、システムの平均所要時間は高くなります.110秒($\frac{100 + 110 + 120}{3} = 110$)です。  
この問題は、一般的に、コンボイエフェクト[B+79]と呼ばれ、リソースの比較的短い潜在的なコンシューマの数が、重量のあるリソースコンシューマの後ろにキューイングされます。このスケジューリングのシナリオでは、食料雑貨品店での１コマを思い出させるかもしれません。レジに並んでいるときにすでに、あなたの目の前に人がいて、食料品などがいっぱいのカートが3つあります。しばらくレジ精算に時間がかかるでしょう。  
だから何をすべきでしょうか？さまざまな時間に実行される新しいジョブの現実に対処するために、より良いアルゴリズムを開発するにはどうすればよいでしょうか？

>> TIP: THE PRINCIPLE OF SJF(SJFの原則)  
>> SJFは、顧客(または、この場合は職種)ごとに認識されるターンアラウンドタイムが重要なシステムに適用できる一般的なスケジューリングの原則を表します。問題の施設が顧客満足を気にしている場合、SJFを考慮した可能性があります。たとえば、食料雑貨品店には、買う数少ない買い物客が、品数が多い買い物をする家族の後ろにつかないようにするため、一般に「10項目以下」のレジがあります。

## 7.4 Shortest Job First (SJF)
非常に単純なアプローチがこの問題を解決することが判明しました。実際には、オペレーションリサーチ[C54、PV56]から持ってきたアイデアであり、コンピュータシステムのジョブのスケジューリングに適用されます。この新しいスケジューリング規律はShortest Job First(SJF)と呼ばれています。この名前はポリシーを完全に記述しているため、覚えやすいはずです。まず最短のジョブを実行し、次に最短のジョブを実行します。

![](../07/img/fig7_3.PNG)

上記の例をSJFのスケジューリング方針として考えてみましょう。図7.3は、A、B、Cを実行した結果を示しています。この図から、SJFが平均ターンアラウンド時間に関してより優れたパフォーマンスを発揮する理由が分かりやすくなったといいでしょう。Aの前でBとCを実行するだけで、SJFは110秒から50秒( $\frac{10+20+120}{3} = 50$)に短縮し、2倍以上の改善をもたらします。  
事実、同時に到着するジョブについての仮定を仮定すると、SJFが実際に最適なスケジューリングアルゴリズムであることを証明することができます。しかし、理論や運用研究ではなく、システムに関する研究ですので証明することはしないです。

>> ASIDE: PREEMPTIVE SCHEDULERS  
>> バッチ・コンピューティングの昔は、多くの非プリエンプティブ・スケジューラーが開発されました。このようなシステムでは、新しいジョブを実行するかどうかを検討する前に、各ジョブを完了するまで実行します。事実上すべての現代のスケジューラはプリエンティブです、つまり、別のプロセスを実行するために1つのプロセスを停止させます。これは、スケジューラが以前に学んだメカニズムを採用していることを意味します。具体的には、スケジューラはコンテキストスイッチを実行して、実行中のプロセスを一時停止し、別のプロセスを再開(または開始)することです。

この結果から、我々はSJFでスケジューリングするための良いアプローチを考え付きますが、私たちの仮定はまだかなり非現実的です。もう一度リラックスして考えてみましょう。具体的には、仮定2を対象とすることができ、現在ではジョブをすべてではなくいつでも到着できると仮定しています。これはどのような問題につながるでしょうか？ここでは、例を使って問題を再び説明しましょう。今度は、Aがt = 0に到着し、100秒間実行する必要があると仮定します.BとCはt = 10に到達し、それぞれ10秒間実行する必要があります。純粋なSJFでは、図7.4のようなスケジュールになります。

![](../07/img/fig7_4.PNG)

この図から分かるように、Aの直後にBとCが到着したにもかかわらず、Aが完了するまで待たされ、同じ護送船の問題を抱えています。これら3つのジョブの平均所要時間は103.33秒( $\frac{100+(110−10)+(120−10)}{3}$)です。スケジューラは何をするべきでしょうか？

## 7.5 Shortest Time-to-Completion First (STCF)
この懸念に対処するためには、想定を緩和する必要があります(ジョブは完了するまで実行する必要があります)。また、スケジューラ自体の中にいくつかの仕組みが必要です。思い出してみましょう。前回の議論でtimer interrruptsとコンテクストスイッチの話題がありました。それらを当てはめて考えてみましょう。BとCが到着すると、スケジューラは何か他のことを行うことができます。つまり、ジョブAをプリエンティブして別のジョブを実行するとAは後で実行されます。

我々の定義によるSJFは、非プリエンティブなスケジューラであり、上述の問題を抱えています。幸いにも、STJF(最短完了までの最短完了)またはPSJF(Preemptive Shortest Job First)スケジューラ[CK68]と呼ばれるSJFにプリエンプションを追加するスケジューラがあります。新しいジョブがシステムに入るたびに、STCFスケジューラーは残りのジョブ(新しいジョブを含む)の中で最も時間が残っていないものを判別し、そのジョブをスケジュールします。したがって、この例では、STCFはAを優先してBとCを完了させます。終了したときにのみ、Aの残りの時間がスケジュールされます。図7.5に例を示します。

![](../07/img/fig7_5.PNG)

その結果、50秒($\frac{(120-0)+(20-10)+(30-10)}{3}$)の大幅に改善された平均所要時間が得られます。私たちの新しい前提を考慮すると、STCFは確かに最適です。すべての仕事が同時に到着した場合にSJFが最適であるとすれば、おそらくSTCFの最適性を見ることができます。

## 7.6 A New Metric: Response Time
したがって、ジョブの長さを知り、ジョブがCPUのみを使用し、唯一のメトリックがターンアラウンドタイムだった場合、STCFは素晴らしいポリシーになります。実際、多くの初期のバッチ・コンピューティング・システムでは、これらのタイプのスケジューリング・アルゴリズムが採用されていました。しかし、time sliceのマシンの導入はそれらの考えをすべて変えました。ユーザーは端末に座り、システムからのインタラクティブなパフォーマンスを要求するようになりました。そして、新しいメトリックが生まれました。それは応答時間です。応答時間は、ジョブがシステムに到着してからスケジュールされた最初の時間までの時間として定義されます。より正式には以下の式になります。

$T_{response} = T_{f irstrun}-T_{arrival}$ (7.2)

たとえば、上記のスケジュール(Aが時刻0に到着し、BとCが時刻10に到着した場合)では、各ジョブの応答時間は、ジョブAの場合は0、Bの場合は0、Cの場合は10 平均：3.33)になります。STCFと関連する分野は、特にレスポンスタイムには適していません。たとえば、3つのジョブが同時に到着した場合、3つ目のジョブは、前の2つのジョブが完全に実行されるのを待ってから、1回だけスケジュールされます。ターンアラウンドタイムには優れていますが、このアプローチは応答時間とインタラクティブ性にとって非常に悪いものです。実際には、ターミナルに座って入力し、システムからの応答を見るのに10秒待たなければならないと想像してください。したがって、まだ別の問題が残っています。「応答時間に敏感なスケジューラを構築するにはどうすればよいのか」ということです。

![](../07/img/fig7_6.PNG)

![](../07/img/fig7_7.PNG)

## 7.7 Round Robin
この問題を解決するために、我々は古典的にラウンドロビン(RR)スケジューリング[K64]と呼ばれる新しいスケジューリングアルゴリズムを導入します。基本的な考え方は簡単です。ジョブを完了して実行する代わりに、RRはtime sliceで(時にはスケジューリングクォンタムと呼ばれます)ジョブを実行し、実行キューの次のジョブに切り替えます。ジョブが終了するまで繰り返し実行します。このため、RRは時々time slicingと呼ばれます。タイムスライスの長さはタイマ割り込み期間の倍数でなければならないことに注意してください。したがって、タイマーが10ミリ秒ごとに割り込みする場合、タイムスライスは10,20、または10ミリ秒の任意の他の倍数になる可能性があります。

RRをより詳細に理解するために、例を見てみましょう。3つのジョブA、B、Cが同時にシステムに到着し、それぞれが5秒間実行したいとします。SJFスケジューラは、各ジョブを実行してから別のジョブを実行します(図7.6)。対照的に、タイムスライスが1秒のRRはジョブをすばやく循環します(図7.7)。RRの平均応答時間は、SJFの場合、$\frac{0+1+2}{3} = 1$で、平均応答時間は$\frac{0+5+10}{3} = 5$です。

ご覧のとおり、タイムスライスの長さはRRにとって重要です。応答時間メトリックの下でRRのパフォーマンスが良好なほど短くなればなります。ただし、タイムスライスを短くすることは時には問題になります。例えば、突然コンテキスト切り替えのコストが全体のパフォーマンスにかかってきます。したがって、タイムスライスの長さを決定することは、システム設計者とのトレードオフをもたらします。つまりシステムが応答しなくなるまでタイムスライスを長くするというのは、コンテクストスイッチのコストをなくす長さにもなります。

>> TIP: AMORTIZATION CAN REDUCE COSTS  
>> コストカットの一般的な手法は、あるオペレーションに固定費がかかる場合にシステムで一般的に使用されます。そのコストをより少なくする(すなわち、より少ない回数を実行することによって)、システムへの総コストが低減されます。例えば、タイムスライスが10msに設定され、コンテキストスイッチコストが1msである場合、約10％の時間がコンテキスト切り替えにかかり、浪費されます。このコストをカットしたい場合、タイムスライスを例えば100msに増加させます。この場合、1％未満の時間がコンテキスト切り替えに費やされるため、タイムスライスのコストがカットされます。

コンテキスト切替えのコストは、ただ単にいくつかのレジスタを保存して復元するOSの動作からは生じないことに注意してください。ここでいうコストというのは、プログラムが実行されると、CPUキャッシュ、TLB、分岐予測子、およびその他のオンチップハードウェアに大量の状態が構築されます。別のジョブに切り替えると、この状態がフラッシュされ、現在実行中のジョブに関連する新しい状態が持ち込まれます。これは、パフォーマンスコスト[MB91]を顕著にする可能性があります。

応答時間が私たちの唯一のメトリックであるなら、妥当なタイムスライスを持つRRは優れたスケジューラです。しかし、ターンアラウンドタイムはどうですか？上記の例をもう一度見てみましょう。それぞれ5秒の実行時間を有するA、B、およびCは同時に到着し、RRは(長い)1秒タイムスライスを有するスケジューラである。上記の画像から、Aは13で終了し、Bは14で終了し、Cは15で終了し、平均14であることがわかります。これはかなりひどい！

それはターンアラウンドタイム基準であるならば、RRは確かに最悪なポリシーの一つです。RRがやっていることは、できるだけ各ジョブを伸ばすことです。各ジョブを短い時間だけ実行してから、次のジョブに移動します。ターンアラウンドタイムはジョブが終了したときだけ気にするので、RRはほとんどの場合pessimalであり、多くの場合単純なFIFOよりも悪いです。

より一般的には、公正である(すなわち、小さな時間スケールでアクティブプロセス間でCPUを均等に分割する)RRなどのポリシーは、ターンアラウンドタイムなどでは、ほとんど機能しません。不公平の場合は、より短い仕事を完了まで実行することができますが、応答時間を犠牲にするしかありません。代わりに公正さを評価すると、応答時間は短縮されますが、ターンアラウンドタイムを犠牲にしています。この種のトレードオフはシステムでは一般的です。

我々は2種類のスケジューラを開発しました。最初のタイプ(SJF、STCF)はターンアラウンドタイムを最適化しますが、応答時間には悪いです。第2のタイプ(RR)は応答時間を最適化するが、ターンアラウンドには悪いです。また、仮定4(そのジョブはI/Oを実行しない)と仮定5(各ジョブの実行時間が分かっていること)という2つの仮定が緩和される必要があります。次に、これらの仮定に取り組んでみましょう。

>> TIP: OVERLAP ENABLES HIGHER UTILIZATION(オーバーラップはより高い活用を可能にする)  
>> 可能であれば、システムを最大限に活用するために操作を重複させてみてください。オーバーラップは、ディスクI/Oを実行するときやリモートマシンにメッセージを送信するときなど、多くの異なるドメインで役立ちます。いずれの場合でも、操作を開始してから他の作業に切り替えることは良いアイデアであり、システムの全体的な利用率と効率を向上させます。

## 7.8 Incorporating I/O
まず、仮定4を緩和します。もちろん、すべてのプログラムがI/Oを実行します。何も入力しなかったプログラムを想像してみましょう。毎回同じ出力を生成します。アウトプットのないものを想像してみましょう。そのプログラムは実行していても問題ありません。

スケジューラは、現在実行中のジョブがI/O中にCPUを使用しないため、ジョブがI/O要求を開始したときに決定することを明確に示しています。I/O完了を待ってブロックされます。I/Oがハード・ディスク・ドライブに送信されると、ドライブの現在の入出力負荷に応じて、プロセスが数ミリ秒以上ブロックされることがあります。したがって、スケジューラは、その時点でCPU上に別のジョブをスケジュールする必要があります。

スケジューラは、I/Oが完了したときにも決定を下す必要があります。これが発生すると割り込みが発生し、OSが実行されI/Oを発行したプロセスがブロックされて元の準備状態に戻ります。もちろんその時点でジョブを実行することもできます。OSはどのように各仕事を扱うべきでしょうか？

この問題をよりよく理解するには、それぞれ50ミリ秒のCPU時間が必要な2つのジョブAとBがあるとします。しかし、1つの明らかな違いがあります。Aは、10ミリ秒間実行され、その後、Bは単純に50ミリ秒のCPUを使用しないI/Oを実行しないのに対し、I/O要求(I/Oは、それぞれが10ミリ秒を取ることがここで仮定)を発行します。スケジューラはまずAを実行し、次にBを実行します(図7.8)。

![](../07/img/fig7_8.PNG)

STCFスケジューラーを構築しようとしているとします。このようなスケジューラは、Aが5つの10msサブジョブに分割されているのに対し、Bは単なる50msのCPU時間が必要であるという事実をどのように説明するのでしょうか？明らかに、I/Oを考慮する方法を考慮せずに1つのジョブを実行し、次に他のジョブを実行するだけでは意味がありません。

一般的なアプローチは、Aの各10msサブジョブを独立したジョブとして扱うことです。従って、システムが始動するときその選択は、10msAまたは50msBをスケジュールするかどうかです。STCFでは選択肢が明確です。この場合、より短いものを選択します。次に、Aのサブジョブが完了し、Bのみが残され実行が開始されます。次に、Aの新しいサブジョブが提出され、Bをプリエンプトして10ミリ秒間実行されます。そうすることで、オーバーラップが可能になり、別のプロセスのI/Oが完了するのを待っている間にCPUがあるプロセスによって使用されます。このようにシステムをより有効に利用することができます。(図7.9参照)

![](../07/img/fig7_9.PNG)

スケジューラーがI/Oをどのように組み込むのかを見ていきます。スケジューラは、各CPUバーストをジョブとして扱うことにより、「インタラクティブ」なプロセスが頻繁に実行されるようにします。これらの対話式ジョブはI/Oを実行していますが、CPUを大量に使用する他のジョブが実行されるため、プロセッサーをうまく活用できます。

## 7.9 No More Oracle
I/Oの基本的なアプローチでは、スケジューラは各ジョブの長さを知っていることを前提にしています。前にも述べたように、これはおそらく最悪の仮定である可能性があります。実際、一般的な汎用OS(私たちが気にするOSのような)では、OSは通常、各ジョブの長さについてほとんど知りません。なので、SJF/STCFのような振る舞いを各ジョブの長さを知らずに構築するにはどうすればよいでしょうか？さらに、RRスケジューラで見たアイディアのいくつかをどのように組み込んで、応答時間も非常に良いものにすればいいでしょうか？

## 7.10 Summary
スケジューリングの背後にある基本的なアイデアを紹介し、2つのアプローチを開発しました。最初のジョブは最短のジョブを実行し、したがってターンアラウンドタイムを最適化します。2番目のジョブはすべてのジョブを交互に実行し、応答時間を最適化します。両方が悪いのは、一方だけが良いことです。残念なことに、システムでは一般的なトレードオフです。

私たちはまた、I/Oをどのように組み込むかを図で見てきましたが、まだ問題を解決できていません。次に、最近の過去を使用して将来を予測するスケジューラを構築することによって、この問題を解決する方法を見ていきます。このスケジューラはmulti-level feedback queueと呼ばれ、次の章のトピックです。

#参考文献

[B+79] “The Convoy Phenomenon”  
M. Blasgen, J. Gray, M. Mitoma, T. Price  
ACM Operating Systems Review, 13:2, April 1979  
Perhaps the first reference to convoys, which occurs in databases as well as the OS.

[C54] “Priority Assignment in Waiting Line Problems”  
A. Cobham  
Journal of Operations Research, 2:70, pages 70–76, 1954  
The pioneering paper on using an SJF approach in scheduling the repair of machines.  

[K64] “Analysis of a Time-Shared Processor”  
Leonard Kleinrock  
Naval Research Logistics Quarterly, 11:1, pages 59–73, March 1964  
May be the first reference to the round-robin scheduling algorithm; certainly one of the first analyses of said approach to scheduling a time-shared system.

[CK68] “Computer Scheduling Methods and their Countermeasures”  
Edward G. Coffman and Leonard Kleinrock  
AFIPS ’68 (Spring), April 1968  
An excellent early introduction to and analysis of a number of basic scheduling disciplines.

[J91] “The Art of Computer Systems Performance Analysis:Techniques for Experimental Design, Measurement, Simulation, and Modeling”  
R. Jain  
Interscience, New York, April 1991  
The standard text on computer systems measurement. A great reference for your library, for sure.

[O45] “Animal Farm”  
George Orwell  
Secker and Warburg (London), 1945  
A great but depressing allegorical book about power and its corruptions. Some say it is a critique of Stalin and the pre-WWII Stalin era in the U.S.S.R; we say it’s a critique of pigs.

[PV56] “Machine Repair as a Priority Waiting-Line Problem”  
Thomas E. Phipps Jr. and W. R. Van Voorhis  
Operations Research, 4:1, pages 76–86, February 1956  
Follow-on work that generalizes the SJF approach to machine repair from Cobham’s original work; also postulates the utility of an STCF approach in such an environment. Specifically, “There are certain types of repair work, ... involving much dismantling and covering the floor with nuts and bolts, which certainly should not be interrupted once undertaken; in other cases it would be inadvisable to continue work on a long job if one or more short ones became available (p.81).”

[MB91] “The effect of context switches on cache performance”  
Jeffrey C. Mogul and Anita Borg  
ASPLOS, 1991  
A nice study on how cache performance can be affected by context switching; less of an issue in today’s systems where processors issue billions of instructions per second but context-switches still happen in the millisecond time range.

[W15] “You can’t have your cake and eat it”  
http://en.wikipedia.org/wiki/You can’t have your cake and eat it  
Wikipedia, as of December 2015  
The best part of this page is reading all the similar idioms from other languages. In Tamil, you can’t “have both the moustache and drink the soup.”

\newpage

# 8. Scheduling:The Multi-Level Feedback Queue

この章では、マルチレベルフィードバックキュー(MLFQ)と呼ばれるスケジューリングの最もよく知られたアプローチの1つを開発するという課題に取り組んでいきます。マルチレベルフィードバックキュー(MLFQ)スケジューラは、Corbato et alによって生み出されました。互換性のあるタイムシェアリングシステム(CTSS)として知られているシステムで1962年に[C+62]、Multicsと言われるOSに適応させた。後に、Corbatoに最高の名誉であるTuring Awardを授与されました。そのスケジューラはその後、いくつかの最新のシステムでの実装に、長年にわたり改良を重ねてきてました。

MLFQが対処しようとしている基本的な問題は2つです。まず、ターンアラウンドタイムを最適化したいと思います。これは、前のメモで見たように、短いジョブを先に実行することによって行われます。残念ながら、OSは、SJF(またはSTCF)のようなアルゴリズムが必要とするジョブが実行される時間を一般に知りません。

第2に、MLFQは、対話型ユーザ(すなわち、スクリーンに座って、プロセスが終了するのを待っているユーザ)に応答するシステムを、応答時間を最小限にするようにしたい。残念ながら、Round Robinのようなアルゴリズムは応答時間を短縮しますが、ターンアラウンド時間は最悪です。したがって、私たちの問題は、「一般的にプロセスについて何も知らないので、これらの目標を達成するためにスケジューラをどのように構築できますか？」ということです。スケジューラは、システムの実行中に、実行中のジョブの特性をどのように学習して、スケジューリングをより適切に行うことができるのでしょうか？  

> THE CRUX:HOW TO SCHEDULE WITHOUT PERFECT KNOWLEDGE?(完全な知識なしにスケジュールを設定するにはどうすればよいでしょうか？)  
>> インタラクティブなジョブの応答時間を最小限に抑えながら、ジョブの長さを前もって知らなくてもターンアラウンドタイムを最小限に抑えるスケジューラを設計するにはどうすればよいでしょうか？

>> TIP: LEARN FROM HISTORY(歴史から学ぶ)  
>> マルチレベルフィードバックキューは、将来を予測するために過去から学習するシステムの優れた例です。このようなアプローチは、オペレーティングシステム(およびハードウェアブランチ予測子やキャッシングアルゴリズムを含む、コンピュータサイエンスの他の多くの場所)で一般的です。このようなアプローチは、ジョブが行動の段階を持ち、予測可能な場合に機能します。もちろん、それらは簡単に間違っている可能性があり、システムが全く知識なしで持っているよりも悪い決定を下さないように、そのような技術には注意を払わなければなりません。

## 8.1 MLFQ: Basic Rules
このようなスケジューラを構築するために、この章ではマルチレベルフィードバックキューの基本的なアルゴリズムについて説明します。多くの実装されたMLFQの詳細は異なりますが[E95]、ほとんどのアプローチは似ています。

私たちの処理では、MLFQにはいくつかの異なるキューがあり、それぞれ異なる優先順位が割り当てられています。任意の時点で、実行準備が整っているジョブは単一のキューにいます。MLFQは優先度を使用して、所定の時間に実行するジョブを決定します。優先度の高いジョブ(つまり、上位キューのジョブ)が選択されて実行されます。

もちろん、複数のジョブがキューに存在し、同じ優先度を持つ可能性があります。この場合、それらのジョブの間でラウンドロビンスケジューリングを使用します。

したがって、MLFQの最初の2つの基本的なルールに到達します。

- ルール1：優先度(A)> 優先度(B)の場合、Aが実行されます(Bは実行されません)。
- ルール2：優先順位(A)= 優先順位(B)の場合、AとBはRRで実行されます。

したがって、MLFQスケジューリングのカギとなる考えは、スケジューラが優先順位を設定する方法にあります。MLFQは、各ジョブに固定の優先順位を与えるのではなく、観察された動作に基づいてジョブの優先順位を変更します。例えば、キーボードからの入力を待っている間にジョブが繰り返しCPUを放棄した場合、MLFQは対話型プロセスがどのように動作するのかというと、優先度を高く保ちます。その代わりに、ジョブがCPUを集中的に長時間使用すると、MLFQは優先順位を下げます。このようにして、MLFQはプロセスが実行されるにつれてプロセスを学習しようとし、ジョブの履歴を使用して将来の動作を予測します。

与えられた瞬間に待ち行列がどのように見えるかを描写すると、次のようなものが見えます(図8.1)。図では、2つのジョブ(AとB)が最も高い優先順位にあり、ジョブCは中間にあり、ジョブDは最も低い優先順位にあります。MLFQがどのように動作するかについての現在の知識があれば、スケジューラはAとBの間でタイムスライスを交互に切り替えるだけです。poor jobsであるCとDは決して実行されないでしょう！

![](../08/img/fig8_1.PNG)

もちろん、いくつかのキューの静的なスナップショットを表示しても、MLFQの仕組みは分かりません。私たちが必要とするのは、時間の経過とともにどのようにジョブ優先度が変化するかを理解することです。そして、この本の章をはじめて読んでいる人にとっては驚いたことでしょう。実はこれが、次にやることなのです。

## 8.2 Attempt #1: How To Change Priority
我々は今、MLFQがジョブの存続時間に応じて優先順位をどのように変更するのか(したがって、それがどのキューにあるか)を決定する必要があります。これを実行するには、短期間実行している(頻繁にCPUを解放するかもしれない)インタラクティブなジョブと、CPU時間のかかる実行時間の長い「CPUバウンド」のジョブ応答時間は重要ではありません。優先度調整アルゴリズムでの最初の試みは次のとおりです。

- ルール3：ジョブがシステムに入ると、ジョブは最高優先順位(一番上のキュー)に配置されます。
- ルール4a：ジョブが実行中にタイムスライス全体を使い切った場合、ジョブの優先順位が下げられます(つまり、1つ下のキューに移動します)。
- ルール4b：タイムスライスが立ち上がる前にジョブがCPUを放棄した場合、ジョブは同じ優先順位のままです。

### Example 1: A Single Long-Running Job
いくつかの例を見てみましょう。まず、システム内で長時間実行されているジョブがあった場合にどうなるかを見ていきます。図8.2は、3つのキュースケジューラで時間の経過と共にこのジョブに何が起こるかを示しています。

![](../08/img/fig8_2.PNG)

この例でわかるように、ジョブは最高優先順位(Q2)で入力します。10msの単一のタイムスライスの後、スケジューラはジョブの優先度を1つ下げるので、ジョブはQ1上にあります。Q1でタイムスライスを実行した後、ジョブは最終的にシステム内の最も低い優先順位(Q0)に下げられ、残りはそのまま残ります。かなりシンプルでしょう？

### Example 2: Along Came A Short Job
もっと複雑な例を見て、MLFQがSJFに近づこうとしていることを願ってみましょう。この例では、長時間CPUを集中的に使用するジョブであるAと短時間実行する対話式ジョブであるBの2つのジョブがあります。Aがしばらく実行されていて、Bが到着したとします。何が起こるでしょうか？MLFQはBを実行する時にSJFと似ているでしょうか？図8.3は、このシナリオの結果を示しています。A(黒で表示)は、最も優先度の低いキューで実行されています(長時間実行されるCPUインテンシブジョブと同様)。B(灰色で示されている)は時刻T = 100に到着し、したがって最高の待ち行列に挿入される。実行時間が短い(わずか20ms)ので、Bは2つのタイムスライスでボトムキューに達する前に完了します。そして、Aは実行を再開します。(低い優先度で)

![](../08/img/fig8_3.PNG)

この例から、アルゴリズムの主要な目的の1つを理解できます。短期間の仕事であろうと長期間の仕事であろうと、それは短い仕事であろうと仮定しているため、ジョブの優先順位は高く与えられます。実際に短い仕事であれば、すぐに実行され完了します。短い仕事でなければ、ゆっくりとキューの下に移動していくため、すぐに、より長いバッチのようなプロセスであることがすぐに証明されます。このようにして、MLFQはSJFに似ています。

### Example 3: What About I/O?
ここで、いくつかのI/Oを持つ例を見てみましょう。ルール4bが上で述べたように、プロセスがタイムスライスを使い切る前にプロセッサを放棄した場合、そのプロセスを同じ優先度に保ちます。このルールの目的は簡単です。たとえば、インタラクティブジョブが(キーボードやマウスからのユーザー入力を待つなどして)多くのI/Oを実行している場合、タイムスライスが完了する前にCPUを放棄します。そのような場合、私たちはその仕事にペナルティを課すことはせず、単にジョブを同じレベルに保ちます。

![](../08/img/fig8_4.PNG)

図8.4は、これがどのように動作するかの例を示しています。長時間実行されるバッチジョブA(黒で表示)、I/Oを実行する前にCPUを1msだけ必要とするインタラクティブジョブB(灰色で表示)があります。つまり、ジョブAとジョブBはCPUの競合を起こす要因があります。MLFQのアプローチは、BがCPUを解放し続けるため、Bを最優先度に保ちます。Bがインタラクティブジョブの場合、MLFQはインタラクティブジョブをすばやく実行するという目標を達成します。

### Problems With Our Current MLFQ
このように、我々は基本的なMLFQを持っています。長時間実行されるジョブ間でCPUを公平に共有し、短時間またはI/O集約型のインタラクティブなジョブをすばやく実行できるように、かなり良い仕事をしているようです。しかし残念ながら、これまで開発してきたアプローチには重大な欠陥があります。あなたはどんなことを思いつくことができますか？(ちょっと立ち止まって考えてみましょう)

まず最初に、飢餓問題があります。システムにインタラクティブなジョブが多すぎると、すべてのCPU時間を消費するため、長時間実行されるジョブは決してCPU時間が与えられません。このシナリオでも、これらの仕事について少し考えていきたいと思います。

2番目に、頭のいいユーザは自分のプログラムをスケジューラのゲームに書き直すことができます。スケジューラーのゲームは、スケジューラーを騙して多くの利益を得ようとするずるい行為のことを一般的にさします。私たちが記述したアルゴリズムは、次の攻撃の影響を受けやすいです。タイムスライスが終了する前に、I/O操作を発行してCPUを解放します。そうすることで、同じキューに留まることができ、CPU時間の割合が高くなります。(CPUを放棄する前に99％のタイムスライスを実行するなどして)正しく実行された場合、ジョブはCPUをほぼ独占することができます。

最後に、プログラムは時間の経過とともに行動を変えるかもしれません。CPUバウンド(実行時間の中がジョブ)は、インタラクティブ(実行時間の短いジョブ)に移行する可能性があります。現在のアプローチでは、このようなジョブに対しては考慮されていないため、システム内の他の対話式ジョブと同様に扱われません。

## 8.3 Attempt #2: The Priority Boost
ルールを変更して、飢餓の問題を回避できるかどうかを見てみましょう。CPUバウンドジョブが公平に扱われることを保証するために、どうすればよいのでしょうか？ここでの簡単な考え方は、システム内のすべてのジョブの優先度を定期的に高めることです。これを達成するには多くの方法がありますが、単純なことをしましょう。一番上のキューにすべてを投げるようにします。したがって、新しいルールは以下の通りになります。

- ルール5：ある期間Sの後、システム内のすべてのジョブを一番上のキューに移動します。

この新しいルールは、1度に2つの問題を解決します。まず、プロセスが枯渇しないことが保証されます。トップキューに座ると、ジョブはCPUを他の優先順位の高いジョブとラウンドロビン方式で共有し、最終的にサービスを受け取ります。第2に、CPUバウンドジョブがインタラクティブになった場合、スケジューラは優先順位ブーストを受信すると、それを適切に処理します。

![](../08/img/fig8_5.PNG)

例を見てみましょう。このシナリオでは、短時間実行される2つのインタラクティブなジョブ、長時間実行されるジョブがCPU競合する場合の動作を示します。図8.5に2つのグラフを示します。左には優先順位の上昇はないので、2つの短い仕事が到着すると長時間実行されるジョブは飢餓状態に陥ってしまいます。右側には、50ミリ秒ごとに優先順位が上がります(値が小さすぎる可能性がありますが、ここでは例として使用しています)。したがって、長時間実行されるジョブが進歩し、50 msごとに最高の優先度を持ち、したがって定期的に実行されます。

もちろん、期間Sの追加について質問したいことがあるでしょう。それはSはどのように設定すべきですか？ということです。よく研究されているシステム研究者であるJohn Ousterhout [O11]は、それらを正しく設定するためにある種の黒魔術を必要としていたため、システムのvoo-doo定数でそのような値を呼んでいました。残念ながら、Sにはその加減を考える必要があります。Sが高すぎると、長く実行されるジョブは飢餓状態になってしまう可能性があります。逆に低すぎると、短時間実行されるインタラクティブなジョブがCPUの適切なシェアを得られない可能性があります。

## 8.4 Attempt #3: Better Accounting
解決すべきもう一つの問題があります。私たちのスケジューラーのズルを防ぐ方法はどうするでしょう？あなたが推測したように、実際の原因はルール4aと4bであり、タイムスライスの有効期限が切れる前にCPUを放棄することでジョブが優先順位を保持できるようになります。つまりこれらに対して何をするべきでしょうか？

ここでの解決策は、MLFQの各レベルでのCPU時間のより良いアカウンティングを実行することです。つまり、あるレベルで使用されているプロセスをどれだけtime sliceするかを忘れる代わりに、スケジューラは追跡しておく必要があります。具体的には、プロセスがCPU割り当てを使用すると、次の優先順位のキューに降格されます。1つの長いバーストでタイムスライスを使用するのか、小さなバーストで使用するのかは関係ありません。したがって、ルール4aと4bを次の単一ルールに書き換えます。

- ルール4：ジョブが所定のレベルでtime sliceを使い切れば(CPUを何回諦めたかにかかわらず)、その優先順位は低下します(つまり、1つのキューに移動します)。

![](../08/img/fig8_6.PNG)

例を見てみましょう。図8.6は、古いルール4aと4b(左側)とズル対策をした新しいルール4を使用して、スケジューラを試してみると仕事量がどうなるかを示しています。タイムスライスの直前にI/Oが終了し、CPU時間を支配します。このような保護機能を使用すると、プロセスのI/O動作に関係なく、キューレベルをゆっくりと移動させることができ、CPUの不公平な分担をになることはできません。

## 8.5 Tuning MLFQ And Other Issues
MLFQスケジューリングでは、他にもいくつかの問題があります。1つの大きな問題は、そのようなスケジューラをどのようにパラメータ化するかです。たとえば、いくつのキューが必要になるでしょうか？タイムスライスはキューごとにどのくらいの大きさにする必要があるでしょうか？飢餓状態を回避し、行動変化(長いジョブから短いジョブ)があったとき、どのくらいの頻度で優先順位を上げるべきでしょうか？これらの質問には簡単な答えはないので、仕事量の経験とそれに続くスケジューラのチューニングだけが満足できるバランスにつながっていきます。

![](../08/img/fig8_7.PNG)

例えば、ほとんどのMLFQの変形では、異なるキュー間でタイムスライスの長さを変えることができます。高優先度キューには通常、短い時間スライスが与えられます。結局、対話型ジョブから構成され、それらの間で迅速に交互になることが合理的です。(例えば、10ミリ秒以下)一方、低優先度キューには、CPUにバインドされた長時間実行ジョブが含まれています。したがって、より長い時間スライスがうまく機能します(例えば、100ms)。図8.7は、2つの長時間実行ジョブが最高キューで10ミリ秒、中に20個、最低で40個実行される例を示しています。

>> TIP: AVOID VOO-DOO CONSTANTS (OUSTERHOUT’S LAW)  
>> 可能な限り、voo-doo定数を避けることは良いアイデアです。残念ながら、上記の例のように困難です。システムが良い価値を学ばせるようにすることもできますが、それは簡単ではありません。そのため、設定ファイルにデフォルトのパラメータ値が入力されています。一応、何かが正常に動作していないときに熟練した管理者が微調整できます。  
しかし、一般的にこれらは変更されないままにされることが多いので、デフォルトがフィールドでうまくいくでしょう。このヒントは、私たちの古いOSの教授であるJohn Ousterhoutによってもたらされました。それで、私たちはそれをOusterhoutの法則と呼んでいます。

Solaris MLFQの実装では、Time Sharing scheduling class(TS)が特に簡単に構成できます。プロセスの優先度がライフタイム全体でどのように変更され、どのタイムスライスがどのくらいの期間、どのくらいの頻度でジョブの優先度を上昇させるかを正確に決定する一連のテーブルを提供します[AD00]。管理者はスケジューラを異なる方法で動作させるために、このテーブルを使用することができます。テーブルのデフォルト値は60キューであり、タイムスライスの長さは20ミリ秒(最優先)から数百ミリ秒(最低)までゆっくりと増加し、プライオリティは1秒程度でキューレベルが上昇していきます。

他のMLFQスケジューラでは、この章で説明しているテーブルやルールは使用しません。むしろ数式を使って優先順位を調整します。たとえば、FreeBSDスケジューラ(バージョン4.3)は、ジョブの現在の優先度を計算するために式を使用し、プロセスが使用したCPUの量[LM+89]に基づいています。さらに、使用時間の経過とともに減衰し、ここで記載された方法とは異なる方法で望んだ優先順位ブーストを提供します。そのような減衰利用アルゴリズムとそのプロパティ[E95]の優れた概要については、Epemaの論文を参照してください。

最後に、多くのスケジューラには、あなたが今後、知るかもしれないある他の機能がいくつかあります。たとえば、一部のスケジューラでは、オペレーティングシステムの作業に最も高い優先順位が設定されています。したがって、典型的なユーザジョブは、システム内で最高レベルの優先順位を得ることができません。一部のシステムでは、ユーザーのアドバイスによって優先順位を設定することもできます。たとえば、コマンドラインユーティリティniceを使用すると、ジョブの優先度を(多少)増減させることができ、したがって、任意の時点で実行する可能性を増減できます。詳細については、manページを参照してください。

## 8.6 MLFQ: Summary
マルチレベルフィードバックキュー(MLFQ)と呼ばれるスケジューリング手法について説明しました。複数のレベルのキューがあり、フィードバックを使用して特定のジョブの優先順位を判断する理由がわかりました。歴史はそのガイドです。つまり、時間の経過とともにどのように行動するかに注意を払い、それに応じて対応していきます。

>> TIP: USE ADVICE WHERE POSSIBLE(可能性のあるアドバイスを使用する)  
>> オペレーティングシステムは、システムのあらゆるプロセスに対して最適な方法は知らないため、ユーザーや管理者がOSにいくつかのヒントを提供するためのインターフェイスを提供すると便利なことがよくあります。OSは必ずしもそれに注意を払う必要はないので、そのようなヒントのアドバイスと呼ぶことが多いです。  
>> しかし、より良い決定をするためにアドバイスを考慮することが必要でしょう。このようなヒントは、スケジューラ(例えば、nice)、メモリマネージャ(例えば、madvise)、およびファイルシステム(例えば、通知プリフェッチおよびキャッシング[P+95])を含む、OSの多くの部分において有用である。

MLFQルールの洗練されたセットは、この章全体に広がっています。

- ルール1：優先度(A)> 優先度(B)の場合、Aが実行されます(Bは実行されません)。
- ルール2：優先順位(A)= 優先順位(B)の場合、A＆BはRRで実行されます。
- ルール3：ジョブがシステムに入ると、ジョブは最高優先順位(一番上のキュー)に配置されます。
- ルール4：ジョブが所定のレベルで時間割り当てを使い切れば(CPUを何回諦めたかにかかわらず)、その優先順位が下げられます(つまり、1つのキューに移動します)。
- ルール5：ある期間Sの後、システム内のすべてのジョブを一番上のキューに移動します。

MLFQは次の理由から面白いです。ジョブの性質に関する前情報を要求するのではなく、ジョブの実行を観察し、それに応じて優先順位を付けます。このようにして、短期間のインタラクティブなジョブのための優れた全体的なパフォーマンス(SJF/STCFに似ています)を提供することができ、長期にわたるCPU集約的なジョブのために公平な割り当てをします。このため、BSD UNIX派生物[LM+89、B86]、Solaris [M06]、およびWindows NT以降のWindowsオペレーティングシステム[CS97]を含む多くのシステムが、基本スケジューラとしてMLFQの形式を使用しています。

# 参考文献

[AD00] “Multilevel Feedback Queue Scheduling in Solaris”  
Andrea Arpaci-Dusseau  
Available: http://www.ostep.org/Citations/notes-solaris.pdf  
A great short set of notes by one of the authors on the details of the Solaris scheduler. OK, we are probably biased in this description, but the notes are pretty darn good.

[B86] “The Design of the UNIX Operating System”  
M.J. Bach  
Prentice-Hall, 1986  
One of the classic old books on how a real UNIX operating system is built; a definite must-read for kernel hackers.

[C+62] “An Experimental Time-Sharing System”  
F. J. Corbato, M. M. Daggett, R. C. Daley  
IFIPS 1962  
A bit hard to read, but the source of many of the first ideas in multi-level feedback scheduling. Much of this later went into Multics, which one could argue was the most influential operating system of all time.

[CS97] “Inside Windows NT”  
Helen Custer and David A. Solomon  
Microsoft Press, 1997  
The NT book, if you want to learn about something other than UNIX. Of course, why would you? OK, we’re kidding; you might actually work for Microsoft some day you know.

[E95] “An Analysis of Decay-Usage Scheduling in Multiprocessors”  
D.H.J. Epema  
SIGMETRICS ’95  
A nice paper on the state of the art of scheduling back in the mid 1990s, including a good overview of the basic approach behind decay-usage schedulers.

[LM+89] “The Design and Implementation of the 4.3BSD UNIX Operating System”  
S.J. Leffler, M.K. McKusick, M.J. Karels, J.S. Quarterman  
Addison-Wesley, 1989  
Another OS classic, written by four of the main people behind BSD. The later versions of this book, while more up to date, don’t quite match the beauty of this one.

[M06] “Solaris Internals: Solaris 10 and OpenSolaris Kernel Architecture”  
Richard McDougall  
Prentice-Hall, 2006  
A good book about Solaris and how it works.

[O11] “John Ousterhout’s Home Page”
John Ousterhout
Available: http://www.stanford.edu/˜ouster/
The home page of the famous Professor Ousterhout. The two co-authors of this book had the pleasure of
taking graduate operating systems from Ousterhout while in graduate school; indeed, this is where the
two co-authors got to know each other, eventually leading to marriage, kids, and even this book. Thus,
you really can blame Ousterhout for this entire mess you’re in.

[P+95] “Informed Prefetching and Caching”
R.H. Patterson, G.A. Gibson, E. Ginting, D. Stodolsky, J. Zelenka
SOSP ’95
A fun paper about some very cool ideas in file systems, including how applications can give the OS
advice about what files it is accessing and how it plans to access them

\newpage

# 9. Scheduling: Proportional Share
この章では、proportional share scheduler(比例共有スケジューラ)と呼ばれる別のタイプのスケジューラを検討します。これは、時々 fair share schedulerとも呼ばれます。比例共有は、単純な概念に基づいています。つまり、処理時間や応答時間を最適化する代わりに、スケジューラは各ジョブがCPU時間の一定割合を取得することを保証します。

比例配分スケジューリングの優れた現代的な例は、Waldspurger and Weihl [WW94]の研究で発見され、宝くじスケジューリングと呼ばれています。しかし、この考えは確かにはるかに古いです[KL88]。基本的な考え方は非常に単純です。たいていの場合、宝くじを開催して、次に実行するプロセスを決定します。より頻繁に実行されるプロセスには、宝くじに勝つチャンスが与えられます。

>> CRUX: HOW TO SHARE THE CPU PROPORTIONALLY(CPUを比例的に共有する方法)  
>> 比例的にCPUを共有するスケジューラを設計するにはどうすればよいでしょうか？そうするための鍵となる仕組みは何でしょうか？スケジューラはどれくらい効果的でしょうか？

## 9.1 Basic Concept: Tickets Represent Your Share
基本的な宝くじスケジューリングはの基本概念の1つです。具体的には、プロセス(またはユーザーなど)が受け取るべきリソースのシェアを表すために使用されるチケットです。プロセスが持つチケットの割合は、問題のシステムリソースのシェアを表します。

例を見てみましょう。AとBの2つのプロセスを想像してみましょう。さらにAには75のチケットがあり、Bには25しかありません。したがって、AはCPUの75％、Bは残りの25％を受け取ります。

宝くじのスケジューリングは、頻繁に(例えば、毎回のタイムスライス)毎回宝くじを開催することによって、確率論的に(決定論的ではない)決まっていきます。宝くじを開催することは簡単です。スケジューラはチケットがいくつあるかを知る必要があります(この例では100です)。その後、スケジューラは0から99までの番号の勝ちのチケットを選びます。Aが0から74、Bが75から99までのチケットを保留していると仮定すると、勝ちのチケットは単にAかBのどちらが実行されたかを判断するだけです。スケジューラは、その勝利したプロセスの状態をロードし、実行します。

>> TIP: USE RANDOMNESS  
>>  宝くじスケジューリングの最も美しい面の1つは、ランダム性の使用です。決定を下さなければならないときは、ランダム化されたアプローチを使用することは堅牢で簡単な方法です。  
ランダムアプローチには、従来の決定よりも少なくとも3つの利点があります。まず、ランダムはより伝統的なアルゴリズムが扱いにくいかもしれない変わったケースの振る舞いを避けます。たとえば、LRU置換ポリシー(仮想メモリの章で詳細に説明する)を考えてみましょう。多くの場合、適切な置換アルゴリズムであるにもかかわらず、LRUは、いくつかの循環して連続的に起こること(同じ事象)に対して弱いです。一方、ランダムは、そのような最悪の場合はありません。  
二つ目はランダムも軽量であり、状態を追跡するためのものは必要ありません。伝統的な公平スケジューリングアルゴリズムでは、各プロセスがどれだけのCPUを受け取ったかを追跡するには、プロセスごとのアカウンティングが必要です。プロセスごとのアカウンティングは、各プロセスの実行後に更新する必要があります。これをランダムに行うと、プロセスごとの状態の最小限度(例えば、それぞれが有するチケットの数)だけが必要となります。  
最後に、ランダム化はかなり速くなる可能性があります。乱数の生成が迅速であれば、決定を下すこともでき、したがって、速度が必要な多くの場所でランダムを使用することができます。ただし、速いほど、ランダムは疑似乱数に向かう傾向が強くなります。

宝くじスケジューラーの勝利チケットの出力例:
63 85 70 39 76 17 29 41 36 39 10 99 68 83 63 62 43 0 49 49

スケジュールの結果:
```
A   A A   A A A A A A   A   A A A A A A
  B     B             B   B
```

この例からわかるように、宝くじスケジューリングでのランダム化の使用は、望ましい比率に合致する確率的正確性につながりますが、保証はありません。上記の例では、Bは、望ましい25％の割り当てではなく、20のタイムスライス(20％)のうちの4つのみを実行します。しかし、これらの2つのジョブ(長い時間が必要なジョブ)が競合するほど、望ましい割合を達成する可能性が高くなります。

>> TIP: USE TICKETS TO REPRESENT SHARES(代表者に株式を提示するためのチケットを使用する)  
>> 抽選(およびストライド)スケジューリングの設計における最も強力な(そして基本的な)メカニズムの1つは、チケットのスケジューリングです。これらの例では、チケットはCPUのプロセスシェアを表すために使用されていますが、より広範囲に適用できます。たとえば、ハイパーバイザーの仮想メモリ管理に関する最近の作業では、Waldspurgerはチケットを使用してゲストOSのメモリシェア[W02]を表す方法を示しています。なので、所有権の割合を表すためのメカニズムを必要としている場合、このコンセプトはピッタリかもしれません。

## 9.2 Ticket Mechanisms
宝くじスケジューリングでは、チケットを異なる方法で、時には有用な方法で操作するためのメカニズムもいくつか用意されています。1つの方法はチケット通貨のコンセプトです。通貨では、チケットのセットを持つユーザーは、自分の好きな通貨で自分の仕事にチケットを割り当てることができます。システムはその通貨を正しいグローバル値に自動的に変換します。

たとえば、ユーザーAとBにそれぞれ100個のチケットが与えられているとします。ユーザーAは2つのジョブA1とA2を実行しており、ユーザーAの通貨でそれぞれ500チケット(合計1000通)を提供します。ユーザーBは1ジョブのみを実行しており、10チケット(合計10件)を提供します。システムはA1とA2の配分をAの通貨でそれぞれ500からグローバル通貨でそれぞれ50に変換します。同様にB1の10枚のチケットは100枚のチケットに変換されます。抽選はグローバルチケット通貨(200トータル)で行われ、どのジョブが実行されているかが決まります。  
```
User A -> 500 (A’s currency) to A1 -> 50 (global currency)  
       -> 500 (A’s currency) to A2 -> 50 (global currency)  
User B -> 10 (B’s currency) to B1 -> 100 (global currency)  
```  
もう1つの便利なメカニズムはチケットの転送です。転送によって、プロセスはそのチケットを別のプロセスに一時的に渡すことができます。この機能は、クライアントプロセスがクライアントに代わって何らかの作業を要求するメッセージをサーバーに送信するクライアント/サーバー設定で特に便利です。作業をスピードアップするために、クライアントはチケットをサーバーに渡すことができるため、サーバーがクライアントの要求を処理している間にサーバーのパフォーマンスを最大化しようとします。終了すると、サーバーはチケットをクライアントに転送し、以前と同じ状態になります。

最後に、チケットのインフレは時には有用なテクニックです。インフレでは、プロセスが所有するチケットの数を一時的に増減できます。もちろん、互いを信頼しないプロセスとの競合を起こすときは、これはほとんど意味がありません。1つの貪欲なプロセスが、膨大な数のチケットを手に入れてマシンを引き継ぐ可能性があります。どちらかというと、プロセスのグループがお互いを信頼する環境でインフレを適用することができます。このような場合、あるプロセスがCPU時間を必要としていることが分かっている場合、そのプロセスの必要性をシステムに反映させる方法としてチケットの価値を高めることができます。

## 9.3 Implementation
おそらく、宝くじスケジューリングに関する最も驚くべきことは、その実装の単純さです。必要なのは、勝ちのチケットを選ぶための良い乱数ジェネレータ、システムのプロセス(例えば、リスト)を追跡するためのデータ構造、およびチケットの総数です。プロセスをリストにしておくとしましょう。ここでは、A、B、Cの3つのプロセスで構成された例を示します。各プロセスにはいくつかのチケットがあります。  

![](../09/img/fig9_1_1.PNG)  

スケジューリングの決定をするには、最初に、チケットの総数(400)から乱数(勝者)を選択する必要があります。次に、300という数字を選びましょう。次に、簡単なカウンターを使ってリストをトラバースすることで勝者が確定します(図9.1)

![](../09/img/fig9_1.PNG)  

コードはプロセスのリストを調べ、値が勝者を超えるまでカウンタに各チケットの値を追加します。そのようになると、現在のリスト要素が勝者になります。勝利チケットの例が300の場合、次のことが行われます。まず、Aのチケットを計上するためにカウンタを100にインクリメントします。100が300未満であるため、ループは継続します。その後、カウンターは150(Bのチケット)に更新され、まだ300未満です。最後に、カウンタは400(明らかに300を超える)に更新され、したがって、現在のC(勝者)を指してループから抜け出します。

このプロセスを最も効率的にするには、一般的に、チケットの最高数から最低数までソート順にリストを整理することが最善の方法です。順序付けはアルゴリズムの正確さには影響しません。しかし、特にほとんどのチケットを所有するプロセスがいくつかある場合は、リストの反復回数を最小限に抑えることができます。

## 9.4 An Example
宝くじスケジューリングの動きを理解しやすくするために、チケット数が同じ(100回)、ランタイムが同じ(Rは変化します)の2つのジョブの完了時間について簡単に検討します。

このシナリオでは、各ジョブがほぼ同時に終了するようにしたいと思いますが、抽選スケジュールがランダムであるため、あるジョブが他のジョブより先に終了することがあります。この差を定量化するために、簡単な不公平度メトリックUを定義します。Uは、最初のジョブが完了した時刻を2番目のジョブが完了した時刻で割ったものです。例えば、R = 10で、第1のジョブが時間10(および第2のジョブが20)で終了する場合、$ U = \ frac {10} {20} = 0.5$両方のジョブがほぼ同時に終了すると、Uは1に非常に近くなります。このシナリオでは、それが目標です。完全に公平なスケジューラはU = 1を達成します。

![](../09/img/fig9_2.PNG)

図9.2は、2つのジョブ(R)の長さが30回の試行にわたって1から1000まで変化したときの平均不公平をプロットしたものです(結果は章の最後に用意されているシミュレータを介して生成されます)。グラフからわかるように、ジョブの長さがそれほど長くない場合、平均的な不公平さが最悪です。長いタイムスライスのジョブが実行される場合にのみ、宝くじスケジューラは望んだ結果に近づきます。

## 9.5 How To Assign Tickets?
宝くじスケジューリングで取り上げなかった問題の1つは、チケットをジョブに割り当てる方法です。この問題は難しいです。もちろん、システムの動作はチケットの割り当て方法に大きく依存します。1つのアプローチは、ユーザーが最もよく知っていると仮定することです。このような場合、各ユーザーにはいくつかのチケットが渡され、ユーザーは希望どおりに実行する任意のジョブにチケットを割り当てることができます。しかし、このソリューションは非効率です。実際はユーザーが何をすべきかを教えてくれないことがほとんどでしょう。そのため、「チケット割り当て問題」はいまだに残っています。

## 9.6 Why Not Deterministic?
疑問に思うかもしれません。なぜ乱数を使うのでしょうか？上の説明で見たように、ランダム化はシンプルな(そしてほぼ正しい)スケジューラをもたらしますが、時には特に短い時間を超えたスケールで正確な正しい比率を提供しないことがあります。この理由から、Waldspurgerはstride schedulingを発明しました。これは確定的なfair share schedulerです[W95]。

stride schedulingも簡単です。システム内の各ジョブにはstrideがあり、チケットの数に比例して逆になります。上記の例では、ジョブA、B、C(それぞれ100,50,250枚)を使用して、各プロセスが割り当てられたチケットの数で大きな数を除算して、それぞれの歩数を計算することができます。たとえば、チケットの値ごとで10,000を割ると、A、B、Cのstride値は100、200、40になります。この値を各プロセスのstrideと呼びます。プロセスが実行されるたびに、グローバルな進捗状況を追跡するために、strideによってカウンタをインクリメントします(パス値と呼ばれます)。

スケジューラは次に、ストライドとパスを使用して、次に実行するプロセスを決定します。基本的なアイデアは簡単です。いつでも、パスの値が最も低い実行プロセスを選択します。プロセスを実行すると、そのストライドによってパスカウンタがインクリメントされます。擬似コードの実装は、Waldspurger [W95]によって提供されています。

current = remove_min(queue); // クライアントの最小パスを取り出す  
schedule(current); //　リソースを使用する  
current->pass += current->stride; // 現在のストライド値をパスに加算する  
insert(queue, current); // キューの中に戻す  

この例では、ストライド値が100,200、40の3つのプロセス(A、B、C)から始まり、パスの値はすべて最初は0です。したがって、最初はいずれかのプロセスが実行され、それらの合格値も同様に低いからです。Aを選択すると仮定します(任意に、ローパス値が等しいプロセスのいずれかを選択できます)Aが実行されます。タイムスライスを終了すると、パスの値は100に更新されます。次に、パスの値が200に設定されたBを実行します。最後に、パスの値が40にインクリメントされたCを実行します。Cの最小パス値を選んで実行し、パスを80に更新します(Cのストライドは40です)。その後、Cは再び実行され(最低のパス値)、パスは120に上がります。次にAが実行され、パスは200に更新されます(現在はBに等しくなります)。次に、Cは2回以上実行され、パスは160、次に200に更新されます。この時点で、すべてのパス値は再び等しくなり、プロセスは無期限に繰り返されます。図9.3は、時間の経過によるスケジューラの動作を示しています。

図からわかるように、Cは5回、Aは2回、Bは1回実行された。正確には、250、100、50というチケットの値が比例していった。ストライドスケジューリングは各スケジューリングサイクルの終わりにそれらを正確に得ます。

ストライドスケジューリングの正確さを考えれば、どうして宝くじスケジューリングを使うのでしょうか？実は、宝くじのスケジューリングは、ストライドスケジューリングがないという素晴らしい特性を持っています。グローバルな状態がありません。上記のストライドスケジューリングの例の真ん中に新しい仕事が入ったとします。その合格値はどう設定しますか？0に設定します？もし0に設定したとしたら、CPUを独占してしまいます。宝くじスケジューリングでは、プロセスごとにグローバルな状態はありません。どのようなチケットを持っていても新しいプロセスを追加し、一つしかないのグローバル変数を更新して、合計チケット数を追跡し、そこから移動するだけです。このようにして、宝くじは新しいプロセスを合理的な方法で組み込むことをはるかに容易にします。

## 9.7 Summary
比例配分スケジューリングの概念を紹介し、宝くじとストライドスケジューリングという2つの実装について簡単に説明しました。宝くじは比例的なシェアを達成するための巧妙な方法でランダム化を使用します。ストライドは決定的にそうです。両方とも概念的に興味深いものですが、さまざまな理由でCPUスケジューラとして広く普及していません。  
1つは、そのようなアプローチがI/O[AC97]と特によく合わないということです。もう1つは、チケットの割り当てという難しい問題を残すことです。つまり、ブラウザに割り当てられるチケットの数はどのように判断できますか？汎用スケジューラ(以前に議論したMLFQや他の同様のLinuxスケジューラなど)は、より優雅に機能し、より広く展開されています。

結果として、比例配分スケジューラは、これらの問題のいくつか(例えば、株式の割当)が比較的解決しやすい領域において、より有用です。たとえば、仮想化されたデータセンターでは、CPUサイクルの4分の1をWindows VMに割り当て、残りを基本Linuxインストールに割り当てることができます。比例的な共有はシンプルで効果的です。VMWareのESX Serverで今回紹介したような比例的にメモ​​リを共有する方法については、Waldspurger [W02]を参照してください。

# 参考文献

[AC97] “Extending Proportional-Share Scheduling to a Network of Workstations”  
Andrea C. Arpaci-Dusseau and David E. Culler  
PDPTA’97, June 1997  
A paper by one of the authors on how to extend proportional-share scheduling to work better in a clustered environment.

[D82] “Why Numbering Should Start At Zero”  
Edsger Dijkstra, August 1982  
http://www.cs.utexas.edu/users/EWD/ewd08xx/EWD831.PDF  
A short note from E. Dijkstra, one of the pioneers of computer science. We’ll be hearing much more on this guy in the section on Concurrency. In the meanwhile, enjoy this note, which includes this motivating quote: “One of my colleagues — not a computing scientist — accused a number of younger computing scientists of ’pedantry’ because they started numbering at zero.” The note explains why doing so is logical.

[KL88] “A Fair Share Scheduler”  
J. Kay and P. Lauder  
CACM, Volume 31 Issue 1, January 1988  
An early reference to a fair-share scheduler.  

[WW94] “Lottery Scheduling: Flexible Proportional-Share Resource Management”  
Carl A. Waldspurger and William E. Weihl  
OSDI ’94, November 1994  
The landmark paper on lottery scheduling that got the systems community re-energized about scheduling, fair sharing, and the power of simple randomized algorithms.

[W95] “Lottery and Stride Scheduling: Flexible  
Proportional-Share Resource Management”  
Carl A. Waldspurger  
Ph.D. Thesis, MIT, 1995  
The award-winning thesis of Waldspurger’s that outlines lottery and stride scheduling. If you’re thinking of writing a Ph.D. dissertation at some point, you should always have a good example around, to give you something to strive for: this is such a good one.

[W02] “Memory Resource Management in VMware ESX Server”  
Carl A. Waldspurger  
OSDI ’02, Boston, Massachusetts  
The paper to read about memory management in VMMs (a.k.a., hypervisors). In addition to being relatively easy to read, the paper contains numerous cool ideas about this new type of VMM-level memory management.

\newpage

# 10. Multiprocessor Scheduling (Advanced)

この章では、マルチプロセッサスケジューリングの基本について説明します。このトピックは比較的後の章で学ぶようなものが含んでいるので、並行性のトピック(つまり、本の第2の主要な「簡単な部分」)を勉強した後に、もう一度戻ってくるのが最善の方法です。  
マルチプロセッサシステムは、コンピューティング分野のハイエンドでのみ長年存在していますが、ますます普及しており、デスクトップマシン、ラップトップ、さらにはモバイルデバイスにまで浸透しています。複数のCPUコアが1つのチップに集積されたマルチコアプロセッサの登場は、この普及の源です。これらのチップは、作るのは難しいですが「あまり多くの電力を使用することなく、単一のCPUをより速くすること」が現在コンピュータアーキテクトとして求められています。そして、現在私たちは皆、いくつかのCPU(優れたCPU達)をベンダーは私たちに提供してくれています。これはいいことですよね？  
もちろん、1つ以上のCPUになると多くの問題が発生します。主なものは、典型的なアプリケーション(つまり、あなたが書いたCプログラム)は単一のCPUしか使用しないということです。より多くのCPUを追加しても、その単一のアプリケーションをより速く実行することはできません。この問題を解決するには、スレッドを使用して並列実行するようにアプリケーションを書き直す必要があります(詳細は第2章で詳しく説明しています)。マルチスレッド・アプリケーションは、複数のCPUにまたがって作業を分散させることができ、したがって、より多くのCPUリソースが与えられた場合、より速く実行されます。

>> ASIDE: ADVANCED CHAPTERS  
>>この章では、書籍の幅広い部分を真に理解するために必要な材料があります。たとえば、マルチプロセッサスケジューリングに関するこの章では、並行処理の中で最初に読んだ後のほうが意味があります。しかし、それは論理的には本の部分(一般的に)とCPUスケジューリング(具体的には)の部分ではこの順番で合っています。従って、そのような章は順不同になるため、後でもう一度読むことをおすすめします。今回の場合は、並行性の部分の後にもう一度ここを読むことをおすすめします。

アプリケーション以外にも、オペレーティングシステムで発生する新たな問題は、マルチプロセッサスケジューリングの問題です(驚くことではありません)。ここまでは、シングルプロセッサスケジューリングの原則についていくつか議論しました。これらのアイデアをどのように拡張して複数のCPUで動作させることができますか？どんな新しい問題を克服する必要がありますか？したがって、私たちの問題は以下のようになります。

>> CRUX: HOW TO SCHEDULE JOBS ON MULTIPLE CPUS  
>> OSは複数のCPU上でジョブをどのようにスケジュールするべきですか？新しい問題は何でしょうか？同じ古い技法が機能するのですか？、新しいアイデアが必要ですか？

![](../10/img/fig10_1.PNG)

## 10.1 Background: Multiprocessor Architecture
マルチプロセッサスケジューリングを取り巻く新たな問題を理解するためには、シングルCPUハードウェアとマルチCPUハードウェアの新しく根本的な違いを理解する必要があります。この違いは、ハードウェアキャッシュ(図10.1など)の使用と、データが複数のプロセッサでどのように共有されるかを中心にしています。今度は、この問題についてより高いレベルで議論します。詳細は他の場所[CSG99]で利用可能であり、特に上級またはおそらく卒業生のコンピュータアーキテクチャーコースです。

単一のCPUを持つシステムでは、ハードウェアキャッシュの階層があり、一般に、プロセッサがプログラムをより速く実行するのに役立ちます。キャッシュは、(一般的に)システムのメインメモリにある一般的なデータのコピーを保持する小さくて速いメモリです。対照的に、メインメモリはすべてのデータを保持しますが、この大きなメモリへのアクセスは遅くなります。頻繁にアクセスされるデータをキャッシュに保存することで、システムは大規模で低速なメモリを高速に見せることができます。

一例として、明示的なロード命令を発行してメモリから値をフェッチするプログラムと、単一のCPUのみを持つシンプルなシステムを考えます。CPUには小さなキャッシュ(たとえば64 KB)と大きなメインメモリがあります。

プログラムが初めてこの負荷を発生させると、データはメインメモリに格納されます。したがって、フェッチに時間がかかります(おそらく数十ナノ秒、または数百ナノ秒)。プロセッサは、データが再使用されることが予想される場合、ロードされたデータのコピーをCPUキャッシュに格納します。プログラムがこの同じデータ項目を後で再び取り出す場合、CPUは最初にキャッシュ内のデータ項目をチェックします。もしそれが見つかると、データははるかに速く(例えば数ナノ秒)フェッチされるので、プログラムはより速く実行されます。

したがって、キャッシュは局所性の概念に基づいており、その中には時間的局所性と空間的局所性という2つの種類があります。時間的局所性の背後にあるアイデアは、あるデータにアクセスすると、近い将来再びアクセスされる可能性があるということです。変数や命令自体がループ内で繰り返しアクセスされることを想像してください。空間的局所性の背後にある考え方は、プログラムがアドレスxのデータ項目にアクセスすると、xの近くのデータ項目にもアクセスする可能性があるということです。ここでは、配列を介してストリーミングされたプログラム、または順次実行される命令を考える。これらのタイプの局所性は多くのプログラムに存在するため、ハードウェアシステムは、どのデータをキャッシュに入れるかをよく推測し、うまく機能します。

![](../10/img/fig10_2.PNG)

複雑な部分については、図10.2に示すように、1つのシステムに複数のプロセッサを搭載し、1つの共有メインメモリを使用するとどうなりますか？判明したように、複数のCPUを使ったキャッシュははるかに複雑です。

例えば、CPU1上で動作するプログラムがアドレスAでデータ項目(値Dを持つ)を読み込むとします。データはCPU1上のキャッシュにないので、システムはそれを主メモリから取り出し、値Dを得ます。プログラムは次にアドレスAの値を変更し、キャッシュを新しい値D'で更新するだけです。メインメモリにデータを書き込むのが遅いため、システムは(通常)後でそれを行います。次に、OSがプログラムの実行を停止してCPU 2に移動することを決定したと仮定します。プログラムはアドレスAの値を再読み取りします。そのようなデータCPU2のキャッシュは存在しないので、システムは主メモリから値を取り出し、正しい値D'の代わりに古い値Dを得ます。…あれれ～おかしいぞ～？

この一般的な問題はキャッシュ一貫性の問題と呼ばれ、問題の解決に関わるさまざまなことを記述した膨大な研究文献があります[SHW11]。ここでは、すべてのニュアンスをスキップしていくつかの大きなポイントを立てます。コンピュータ・アーキテクチャー・クラス(または3つ)で詳細は確認してください。

基本的な解決策は、ハードウェアによって提供されます。メモリアクセスを監視することによって、ハードウェアは基本的に「正しいこと」が起こり、単一の共有メモリのビューが保持されることを保証できます。バスベースのシステムでこれを行う方法の1つは、バススヌーピング[G83]と呼ばれる古い技術を使用することです。各キャッシュは、それらを主メモリに接続するバスを観察することによってメモリ更新に注意を払います。

CPUがキャッシュ内に保持しているデータ項目の更新を見ると、その変更を認識してそのコピーを無効にするか(つまり、それを自分のキャッシュから削除する)、または更新する(すなわち、キャッシュに新しい値を入れるあまりにも)。上記で暗示されているように、ライトバックキャッシュはこれをもっと複雑にしますが(メインメモリへの書き込みは後で見ることができないため)、基本的な仕組みがどのように機能するか想像することができます。

## 10.2 Don’t Forget Synchronization
キャッシュが一貫性を提供するために、このすべての作業を行うとすれば、プログラム(またはOS自体)は共有データにアクセスする際に何かを心配する必要があるでしょうか？残念なことに、答えは「はい」です。この本の第2部では、並行処理のトピックについて詳しく説明しています。ここでは詳しく説明しませんが、ここでは基本的なアイデアの一部をスケッチ/レビューします(同時実行性に慣れていると仮定します)。

CPU間で共有データ項目や構造体にアクセスする場合(特に更新する場合)、相互排他プリミティブ(ロックなど)を使用して正確性を保証する必要があります(ロックフリーのデータ構造を構築するなどの他のアプローチは複雑で、時には詳細については、並行処理に関するデッドロックの章を参照してください)。たとえば、複数のCPUで同時に共有キューにアクセスしているとします。ロックがなければ、キューから要素を同時に追加または削除することは、基本的な一貫性の定義があっても、期待どおりに機能しません。データ構造を新しい状態に原子的に更新するためのロックが必要です。

![](../10/img/fig10_3.PNG)

これをより具体的にするには、図10.3に示すように、共有リンクリストから要素を削除するために使用されるこのコードシーケンスを想像してください。2つのCPU上のスレッドが同時にこのルーチンに入る場合を想像してください。スレッド1が最初の行を実行すると、tmp変数にheadの現在の値が格納されます。スレッド2が最初の行を実行しても、それ自身のプライベートtmp変数に格納されているheadの値と同じになります(スタックにはtmpが割り当てられ、各スレッドに専用のプライベートストレージが割り当てられます)。したがって、各スレッドがリストの先頭から要素を削除するのではなく、各スレッドは同じヘッド要素を削除しようとします。この場合、4行目のhead要素のダブルフリー化が試行されます同じデータ値を2回返す可能性があります)。

もちろん、解決策は、ロックによって正しいルーチンを作成することです。この場合、単純なミューテックス(例えば、pthread mutex tm;)を割り当ててから、ルーチンの始めにロック(＆m)を追加し、最後にunlock(＆m)を追加すると、問題が解決され、コードが実行されます望んだ通りに。残念なことに、わかるように、このようなアプローチは、特にパフォーマンスに関して問題がないわけではありません。具体的には、CPUの数が増えると、同期共有データ構造へのアクセスが非常に遅くなります。

## 10.3 One Final Issue: Cache Affinity
最後に、キャッシュアフィニティとして知られるマルチプロセッサキャッシュスケジューラを構築する際に、1つの問題が発生します。この概念は単純です。プロセスは特定のCPU上で実行されると、CPUのキャッシュ(およびTLB)に多くの状態ビットを構築します。次回のプロセス実行時には、そのCPU上のキャッシュに状態の一部がすでに存在する場合に、より速く実行されるため、同じCPU上で実行することがかなり有効です。その代わりに毎回異なるCPUでプロセスを実行すると、プロセスが実行されるたびに状態をリロードする必要があるため、プロセスのパフォーマンスは悪化します(ハードウェアのキャッシュ一貫性プロトコルのおかげでおかげで異なるCPU上で正しく動作することに注意してください)したがって、マルチプロセッサスケジューラは、スケジューリングの決定を行う際にキャッシュの親和性を考慮する必要があります。可能であれば、同じCPUにプロセスを保持する方が望ましいでしょう。

## 10.4 Single-Queue Scheduling
この背景に基づいて、マルチプロセッサシステム用のスケジューラを構築する方法について説明します。最も基本的なアプローチは、スケジューリングが必要なすべてのジョブを1つのキューに入れることによって、単一プロセッサスケジューリングの基本フレームワークを単純に再利用することです。このsinglequeue multiprocessor schedulingまたはSQMSを簡潔に呼んでいます。このアプローチには単純さの利点があります。次に実行する最適なジョブを選択し、複数のCPU(2つのCPUがある場合など、実行するには最適な2つのジョブを選ぶ可能性があります)で動作するような既存のポリシーが必要ありません。

しかし、SQMSには明らかな欠点があります。第1の問題はスケーラビリティの欠如です。スケジューラが複数のCPUで正しく動作するように、開発者は上記のようにコードに何らかのロックを挿入します。ロックは、SQMSコードが単一のキューにアクセスして(たとえば、実行する次のジョブを見つけるために)正しい結果が得られるようにします。

残念なことに、ロックはパフォーマンスを大幅に低下させる可能性があります。特に、システム内のCPUの数が増えると、パフォーマンスが低下する可能性があります[A91]。このような単一のロックの競合が増えるにつれて、システムはロックオーバーヘッドに多くの時間を費やし、システムが実行すべき作業を短時間で済ませることができます(注：将来的にはこれを実際に測定するのは素晴らしいことです)。

SQMSの第2の主な問題は、キャッシュ親和性です。たとえば、実行する5つのジョブ(A、B、C、D、E)と4つのプロセッサがあるとします。スケジューリングキューは次のようになります。

![](../10/img/fig10_3_1.PNG)

時間の経過とともに、各ジョブがタイムスライスで実行され、次に別のジョブが選択されたと仮定すると、ここではCPU間で可能なジョブスケジュールがあります:

![](../10/img/fig10_3_2.PNG)

各CPUは単にグローバルに共有されたキューから実行する次のジョブを選択するため、各ジョブはCPUからCPUにバウンドして終了し、キャッシュの親和性の観点からは正反対の動作をします。  
この問題を処理するために、ほとんどのSQMSスケジューラには、可能であればプロセスが同じCPU上で実行し続ける可能性を高めるために、ある種の親和性メカニズムが組み込まれています。具体的には、一部のジョブに対して親和性を提供するかもしれませんが、負荷を均衡させるために他の人を動かします。たとえば、次のようにスケジュールされた同じ5つのジョブを想像してみてください。

![](../10/img/fig10_3_3.PNG)

この構成では、ジョブAからDはプロセッサ間で移動されず、ジョブEのみがCPUからCPUに移行し、ほとんどの場合親和性が維持されます。次回に別のジョブを移行することもできます。これにより、ある種の親和性の公平性も実現します。しかしながら、そのようなスキームを実装することは複雑になります。したがって、SQMSのアプローチには長所と短所があることがわかります。既存のシングルCPUスケジューラを実装するのは簡単ですが、定義上は単一のキューしかありません。ただし、(同期のオーバーヘッドのために)規模が大きくならず、キャッシュの親和性が容易に保持されません。

## 10.5 Multi-Queue Scheduling
シングルキュースケジューラで生じる問題のため、いくつかのシステムでは、複数のキューが選択されます(たとえば、CPUごとに1つ)。この手法をmulti queue multiprocessor scheduling (またはMQMS)と呼びます。MQMSでは、基本スケジューリングフレームワークは複数のスケジューリングキューで構成されています。各キューは、ラウンドロビンなどの特定のスケジューリングルールに従う可能性がありますが、もちろんどのアルゴリズムでも使用できます。ジョブがシステムに入ると、あるヒューリスティック(例えば、ランダムまたは他のジョブよりも少ないジョブを選択する)に従って、正確に1つのスケジューリング・キューに置かれる。したがって、本質的に独立してスケジュールされるため、単一キュー方式で見られる情報の共有と同期の問題は回避されます。

たとえば、CPUが2つしかないシステム(CPU 0とCPU 1というラベルが付いているシステム)があり、いくつかのジョブがシステムに入っているとします(A、B、C、Dなど)。各CPUにスケジューリング・キューがある場合、OSは各ジョブを配置するキューを決定する必要があります。それは次のようなことをするかもしれません：

![](../10/img/fig10_3_4.PNG)

キューのスケジューリング方針に応じて、各CPUは何を実行すべきかを決定する際に、2つのジョブを選択できるようになりました。たとえば、ラウンドロビンを使用すると、次のようなスケジュールが生成されることがあります。

![](../10/img/fig10_3_5.PNG)

MQMSには本質的にスケーラビリティがあるという点でSQMSの明確な利点があります。CPUの数が増えるにつれて、キューの数も増えるので、ロックとキャッシュの競合が中心的な問題ではありません。さらに、MQMSは本質的にキャッシュ親和性を提供します。ジョブは同じCPU上に留まり、キャッシュされたコンテンツをその中に再利用する利点を得ます。
しかし、注意を払っていれば、マルチ・キュー・ベースのアプローチの基本である新しい問題があることがわかります。負荷の不均衡です。上記と同じ設定(4つのジョブ、2つのCPU)があるとしますが、ジョブの1つ(Cなど)が終了したとします。これで、次のスケジューリングキューが作成されたとします。

![](../10/img/fig10_3_6.PNG)

次に、システムの各キューでラウンドロビンポリシーを実行すると、結果として得られるスケジュールが表示されます。

![](../10/img/fig10_3_7.PNG)

この図から分かるように、AはBとDの2倍のCPUを取得しますが、これは望ましい結果ではありません。さらに悪いことに、AとCの両方が終了し、ジョブBとDだけがシステムに残っているとしましょう。スケジューリングキューは次のようになります。

![](../10/img/fig10_3_8.PNG)

その結果、CPU 0はアイドル状態のままです。CPU使用のタイムラインは悲惨です。

![](../10/img/fig10_3_9.PNG)

なので、貧弱なMQMSは何をすべきでしょうか？負荷の不均衡という問題をどのように克服すればよいでしょうか？

>> CRUX: HOW TO DEAL WITH LOAD IMBALANCE  
>> MQMSは、ロード・インバランスをどのように処理して、望ましいスケジューリング目標をより良く達成する必要がありますか？

このクエリーに対する明白な答えは、ジョブを移動することです。これは、マイグレーションとも呼ばれる手法です。1つのCPUから別のCPUへジョブを移行することにより、真のロードバランスを達成できます。明快さを加えるためにいくつかの例を見てみましょう。もう一度、1つのCPUがアイドル状態で、もう1つが何らかのジョブを持っている状況があります。

![](../10/img/fig10_3_10.PNG)

この場合、必要な移行は容易に理解できます。つまり、OSはBまたはDのいずれかをCPU 0に単に移動する必要があります。この単一のジョブの移行結果は均等に均等に分散され、誰もが満足しています。以前の例では、AがCPU 0に、BとDがCPU 1に交互に置かれていたより複雑なケースが発生します。

![](../10/img/fig10_3_11.PNG)

もちろん、可能な他の多くの移行パターンが存在します。しかし、難しい部分が存在します。それは、システムはどのようにしてそのような移行を制定するべきですか？という問題です。1つの基本的なアプローチは、ワーク・スティール[FLR98]として知られる技法を使用することです。ワーク・スティールのアプローチでは、ジョブが少ない(ソース)キューは、別の(ターゲット)キューを時々見て、そのキューがどれくらい完全であるかを確認します。ターゲットキューが(特に)ソースキューよりもいっぱいの場合、ソースはターゲットキューからの1つ以上のジョブを「盗み」、負荷のバランスをとるのに役立ちます。

もちろん、そのようなアプローチには自然な負荷があります。あまりにも頻繁に他のキューを見回すと、オーバーヘッドが大きくなり、スケーリングに問題が発生します。これは、最初に複数のキュースケジューリングを実装する目的全体です。一方、他のキューを非常に頻繁に見ないと、重大な負荷の不均衡に苦しむ危険があります。システムポリシーの設計でよく見られるように、正しい閾値を見つけることは、黒魔術です。

## 10.6 Linux Multiprocessor Schedulers
興味深いことに、Linuxコミュニティでは、マルチプロセッサスケジューラを構築するための共通の解決方法はありませんでした。時間の経過とともに、O(1)スケジューラ、CFS(Complete Fair Scheduler)、BFSスケジューラ(BFS)という3つの異なるスケジューラが発生しました。スケジューラの長所と短所の優れた概要については、Meeheanの論文を参照してください[M11]。ここでは基本のいくつかを要約します。  
O(1)とCFSは複数のキューを使用しますが、BFSは単一のキューを使用し、両方のアプローチが成功することを示しています。もちろん、これらのスケジューラを分ける他の多くの詳細があります。例えば、O(1)スケジューラは、様々なスケジューリング目標を達成するために、時間の経過とともにプロセスの優先度を変更し、次に優先度の高いスケジューラ(前述のMLFQと同様)である。インタラクティビティは特に重要です。これとは対照的に、CFSは決定論的な比例的なアプローチ(前述のように、ストライドスケジューリングに似ています)です。BFSは、3つのキュー間の唯一のキュー方式でもあり、比例配分ですが、EEVDF(EEVDVD)[SA96]というより複雑な方式に基づいています。これらの近代的なアルゴリズムの詳細については、お読みください。あなたは彼らが今やっていることを理解することができるはずです！

## 10.7 Summary
マルチプロセッサスケジューリングにはさまざまなアプローチがあります。シングルキューアプローチ(SQMS)はロードバランスを取ように構築するのは簡単ですが、本質的には多くのプロセッサとキャッシュの親和性にスケーリングすることが難しいです。マルチキュー方式(MQMS)は、スケーラビリティとキャッシュアフィニティをうまく処理しますが、負荷の不均衡に問題があり、より複雑です。どのようなアプローチをとっても、簡単な答えはありません。一般的なスケジューラを構築するのは難しい作業です。小さなコード変更は大きな動作上の違いにつながります。

#参考文献

[A90] “The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors”  
Thomas E. Anderson  
IEEE TPDS Volume 1:1, January 1990  
A classic paper on how different locking alternatives do and don’t scale. By Tom Anderson, very well known researcher in both systems and networking. And author of a very fine OS textbook, we must say.

[B+10] “An Analysis of Linux Scalability to Many Cores Abstract”  
Silas Boyd-Wickizer, Austin T. Clements, Yandong Mao, Aleksey Pesterev, M. Frans Kaashoek, Robert Morris, Nickolai Zeldovich  
OSDI ’10, Vancouver, Canada, October 2010  
A terrific modern paper on the difficulties of scaling Linux to many cores.

[CSG99] “Parallel Computer Architecture: A Hardware/Software Approach”  
David E. Culler, Jaswinder Pal Singh, and Anoop Gupta
Morgan Kaufmann, 1999  
A treasure filled with details about parallel machines and algorithms. As Mark Hill humorously observes on the jacket, the book contains more information than most research papers.

[FLR98] “The Implementation of the Cilk-5 Multithreaded Language”  
Matteo Frigo, Charles E. Leiserson, Keith Randall  
PLDI ’98, Montreal, Canada, June 1998  
Cilk is a lightweight language and runtime for writing parallel programs, and an excellent example of the work-stealing paradigm.

[G83] “Using Cache Memory To Reduce Processor-Memory Traffic”  
James R. Goodman  
ISCA ’83, Stockholm, Sweden, June 1983  
The pioneering paper on how to use bus snooping, i.e., paying attention to requests you see on the bus, to build a cache coherence protocol. Goodman’s research over many years at Wisconsin is full of cleverness, this being but one example.

[M11] “Towards Transparent CPU Scheduling”  
Joseph T. Meehean  
Doctoral Dissertation at University of Wisconsin—Madison, 2011  
A dissertation that covers a lot of the details of how modern Linux multiprocessor scheduling works. Pretty awesome! But, as co-advisors of Joe’s, we may be a bit biased here.

[SHW11] “A Primer on Memory Consistency and Cache Coherence”  
Daniel J. Sorin, Mark D. Hill, and David A. Wood  
Synthesis Lectures in Computer Architecture  
Morgan and Claypool Publishers, May 2011  
A definitive overview of memory consistency and multiprocessor caching. Required reading for anyone who likes to know way too much about a given topic.

[SA96] “Earliest Eligible Virtual Deadline First: A Flexible and Accurate Mechanism for Proportional Share Resource Allocation”  
Ion Stoica and Hussein Abdel-Wahab  
Technical Report TR-95-22, Old Dominion University, 1996  
A tech report on this cool scheduling idea, from Ion Stoica, now a professor at U.C. Berkeley and world expert in networking, distributed systems, and many other things.

\newpage

# 13 The Abstraction: Address Spaces

初期のコンピュータシステムの構築は簡単でした。なぜでしょうか？実はユーザーはあまり期待していなかったのです。「使いやすさ」、「高性能」、「信頼性」など、これらの頭痛に本当に悩まされているのは、期待しているユーザーです。

## 13.1 Early Systems
メモリの観点から、初期のマシンはユーザーにあまり抽象的なものを提供しませんでした。基本的には、マシンの物理メモリは図13.1のようになりました。

![](../13/img/fig13_1.PNG)

OSはメモリにあるルーチン(実際にはライブラリである)であり、物理メモリにある実行中のプログラム(プロセス)が1つあります。この例では64k)、残りのメモリを使用しました。ここには錯覚はほとんどなく、ユーザーはOSからあまり期待していませんでした。当時のOS開発者にとって、設計は簡単だったでしょうか？

## 13.2 Multiprogramming and Time Sharing
しばらくすると、マシンが高価だったため、人々はマシンをより効果的に共有し始めました。このように、マルチプログラミングの時代は、複数のプロセスがある時点で動作する準備が整った状態で生まれました[DV66]。そして、例えば、I/Oを実行することが決定されたときなど、OSはそれらの間で切り替わります。そうすることで、CPUの有効利用が増えました。このような効率の向上は、各マシンのコストが数十万ドルから数百万ドルに達したときに特に重要でした(Macが高価だと思っていました)。

まもなく、人々はより多くの機械を要求し始め、time sharingの時代が生まれました[S59、L60、M62、M83]。具体的には、バッチ・コンピューティングの限界を認識していました。特に、プログラム・デバッグ・サイクルに長い時間がかかっていたため、プログラム・デバッグ・サイクルに疲れてしまったプログラマー自身にとっては限界がありました[CV65]。多くのユーザーが現在実行中のタスクからタイムリーな応答を待っている(または望んでいる)マシンを同時に使用している可能性があるため、対話性の概念が重要になりました。

タイムシェアリングを実装する1つの方法は、すべてのメモリ(図13.1、ページ113)に完全にアクセスできるようにしながら、1つのプロセスを実行してから停止し、すべての状態を何らかの種類のディスク(すべての物理メモリ)を読み込み、他のプロセスの状態をロードし、しばらく実行して、マシンの粗い共有を実行します。[M+63]

残念ながら、このアプローチには大きな問題があります。それは遅すぎます。特にメモリが増えるほどです。レジスタレベルの状態(PC、汎用レジスタなど)の保存と復元は比較的高速ですが、メモリの内容全体をディスクに保存することは残念ながら効果がありません。したがって、私たちがやりたいことは、プロセスをメモリに置き換えながらプロセスを残し、OSが効率的に時間を共有できるようにすることです(図13.2)。

![](../13/img/fig13_2.PNG)

この図には3つのプロセス(A、B、C)があり、それぞれに512KBの物理メモリの一部が彫られています。単一のCPUを想定すると、OSはプロセスの1つ(たとえばA)を実行することを選択し、他のプロセス(BおよびC)は実行待ちのレディキューに入ります。

time sliceが普及するにつれて、おそらくオペレーティングシステムに新しい要求が加えられたと推測できます。特に、複数のプログラムを同時にメモリに常駐させることで、重要な問題が保護されます。プロセスが他のプロセスのメモリを読み書きできるようにしたり、悪化させたりすることは望ましくありません。

## 13.3 The Address Space
しかし、私たちはこれらの厄介なユーザーを念頭に置いておく必要があります。そのためには、OSが物理メモリーの使いやすい抽象化を作成する必要があります。この抽象概念はアドレス空間と呼ばれ、実行中のプログラムのシステム内のメモリのビューです。メモリのこの基本的なOS抽象化を理解することは、メモリがどのように仮想化されているかを理解するうえで重要です。

プロセスのアドレス空間には、実行中のプログラムのすべてのメモリ状態が含まれます。例えば、プログラムのコード(命令)はどこかのメモリになければならないため、アドレス空間にあります。プログラムが実行されている間、スタックを使用して関数呼び出しチェーン内のどこにあるかを追跡し、ローカル変数を割り当て、ルーチンとの間でパラメータと戻り値を渡します。最後に、ヒープは、Cで`malloc()`を呼び出したときやC++やJavaなどのオブジェクト指向の言語で新しいときなど、動的に割り当てられたユーザー管理のメモリーに使用されます。もちろん、他にも静的に初期化された変数などがありますが、ここではコード、スタック、ヒープの3つのコンポーネントを想定します。

![](../13/img/fig13_3.PNG)

図13.3(115ページ)の例では、小さなアドレス空間(16KBのみ)があります。プログラムコードは、アドレス空間の先頭にあります(この例では0から始まり、アドレス空間の最初の1Kにパックされています)。コードは静的であり(したがってメモリに配置するのが簡単なので)、アドレス空間の先頭に配置することができ、プログラムが実行されるにつれてそれ以上のスペースは必要ないことがわかります。

次に、プログラムが実行されている間に拡大(縮小)するアドレス空間の2つの領域があります。それらはヒープ(上部)とスタック(下部)です。それぞれが成長したいと思っているため、アドレス空間の両端に配置することで、このような成長を可能にします。逆方向に成長するだけです。したがって、ヒープはコードの直後(1KB)から始まり、下向きに成長します(ユーザーが`malloc()`でより多くのメモリを要求したとき)。スタックは16KBから始まり、上に向かって(ユーザが手続き呼び出しを行ったときなどに)成長します。ただし、このスタックとヒープの配置は単なる規則です。もし望むのであれば、アドレス空間を別の方法で整理することができます(後で、出てきますが、複数のスレッドがアドレス空間に共存するとき、アドレス空間を分ける良い方法はもうありません)。

もちろん、アドレス空間について説明するとき、OSが実行中のプログラムに提供している抽象概念が記述されています。プログラムは実際には物理アドレス0〜16KBのメモリにありません。むしろ、任意の物理アドレス(複数可)にロードされます。図13.2のプロセスA、B、Cを調べます。そこでは、各プロセスが異なるアドレスのメモリにどのようにロードされているかを見ることができます。したがって、問題は以下のようになります。

>> THE CRUX: HOW TO VIRTUALIZE MEMORY  
>> OSは、単一の物理メモリの上に複数の実行中のプロセス(すべての共有メモリ)のプライベートな、潜在的に大きなアドレス空間のこの抽象化をどのように構築できますか？

OSがこれを実行すると、実行中のプログラムは特定のアドレス(例えば0)でメモリにロードされ、潜在的に非常に大きなアドレス空間(32ビットまたは64ビットなど)を持つと考えているため、現実は全く異なっています。

たとえば、図13.2のプロセスAがアドレス0(仮想アドレスと呼ぶ)で負荷を実行しようとすると、何らかのハードウェアサポートと並行して、OSは実際には負荷が実際には発生しないことを確認する必要があります物理アドレス0に行くのではなく、物理アドレス320KB(Aはメモリにロードされます)に行きます。これは、世界のあらゆる最新のコンピュータシステムの基礎をなすメモリの仮想化の鍵です。

>> TIP: THE PRINCIPLE OF ISOLATION  
>> 信頼性の高いシステムを構築する上で、分離は重要な原則です。2つのエンティティが互いに適切に分離されている場合、これは一方が他方に影響を与えずに失敗することができます。オペレーティングシステムは、プロセスをお互いに分離するように努力し、このようにして、他のプロセスを害するのを防止します。メモリ分離を使用することにより、OSは、実行中のプログラムが基礎となるOSの動作に影響を与えないことをさらに保証します。最近のOSの中には、OSの他の部分からOSの一部を切り離すことによって、さらに孤立しているものもあります。したがって、このようなマイクロカーネル[BH70、R+89、S+03]は、典型的なモノリシックカーネル設計よりも高い信頼性を提供することができます。

## 13.4 Goals
このように、私たちはOSの仕事に、メモリを仮想化するというこの一連のノートに着きます。OSはメモリを仮想化するだけでなく、それはスタイルでそうするでしょう。OSがそうしていることを確認するには、私たちを導くいくつかの目標が必要です。私たちは前にこれらの目標を見てきましたが(序論を考えてください)、我々はそれらをもう一度見ていきますが、確かに繰り返す価値があります。

仮想メモリ(VM)システムの1つの主な目標は透明性です。OSは実行中のプログラムには見えない方法で仮想メモリを実装する必要があります。したがって、プログラムはメモリが仮想化されているという事実を認識すべきではありません。むしろ、プログラムはあたかも独自の物理メモリを持つかのように動作します。背後では、OS(およびハードウェア)は、さまざまなジョブ間でメモリを多重化するためのすべての作業を行い、したがって錯覚を実装します。

VMのもう1つの目標は効率です。OSは、時間の点(すなわち、プログラムをはるかにゆっくりと実行させない)、およびスペース(すなわち、仮想化をサポートするために必要な構造に対してあまりにも多くのメモリを使用しない)の両方において、仮想化をできるだけ効率的にするよう努力すべきです。時間効率の高い仮想化を実現するには、TLBなどのハードウェア機能を含むハードウェアサポートにOSを依存させる必要があります(当然のことながらこれについて学びます)。

最後に、第3のVM目標は保護です。OSはプロセス同士を保護し、OS自体をプロセスから保護する必要があります。1つのプロセスがロード、ストア、または命令フェッチを実行する場合、他のプロセスまたはOS自体のメモリ内容(つまり、アドレス空間外のもの)には何らかの形でアクセスまたは影響を与えるべきではありません。したがって、保護はプロセス間の分離の性質を提供することを可能にします。各プロセスは、他の障害のあるプロセスや悪意のあるプロセスから安全な、独自の隔離された繭で実行されている必要があります。

>> ASIDE: EVERY ADDRESS YOU SEE IS VIRTUAL  
>> ポインタを出力するCプログラムを書いたことがありますか？表示されている値(大きい値、16進数で表示されることが多い)は、仮想アドレスです。あなたのプログラムのコードがどこにあるのだろうか？あなたもそれを印刷することができます。はい、print文で出力することができれば、それは仮想アドレスです。実際、ユーザーレベルのプログラムのプログラマーとして見えるアドレスはすべて仮想アドレスです。メモリを仮想化するトリッキーなテクニックによって、これらの命令とデータ値がマシンの物理メモリのどこにあるのかを知るのは、OSだけです。プログラムのアドレスをプリントアウトすると、それは仮想的なものです。物事がメモリにどのようにレイアウトされているかの錯覚です。OS(およびハードウェア)だけが本当の真実を知っています。  
`main()`ルーチン(コードが存在する場所)、`malloc()`から返されたヒープ割り当て値の値、スタック上の整数の位置を出力する小さなプログラムです。

```c
#include <stdio.h>
#include <stdlib.h>
int main(int argc, char *argv[]) {
    printf("location of code : %p\n", (void *) main);
    printf("location of heap : %p\n", (void *) malloc(1));
    int x = 3;
    printf("location of stack : %p\n", (void *) &x);
    return x;
}
```

>>64bit Macで動かしたときは以下のようになります。  
```
location of code : 0x1095afe50
location of heap : 0x1096008c0
location of stack : 0x7fff691aea64
```
このことから、コードがアドレス空間で最初に来てからヒープになり、スタックがこの大きな仮想空間の反対側にあることが分かります。これらのアドレスはすべて仮想アドレスであり、実際の物理ロケーションから値を取得するためにOSとハードウェアによって変換されます。

次の章では、ハードウェアとオペレーティングシステムのサポートを含め、メモリを仮想化するために必要な基本的なメカニズムについて説明します。また、空き容量を管理する方法や、空き容量が少なくなったときにメモリから解放されるページなど、オペレーティングシステムで発生するより関連性の高いポリシーのいくつかについても調査します。そうすることで、現代の仮想メモリシステムが実際にどのように機能するかを理解していきます。

## 13.5 Summary
私たちは主要なOSサブシステム、仮想メモリの導入を見てきました。VMシステムは、その命令およびデータをすべてその中に保持する、プログラムに大きな、まばらな、プライベートなアドレス空間の錯覚を提供する責任があります。いくつかの重要なハードウェアの助けを借りて、OSはこれらの仮想メモリ参照をそれぞれ取り出し、それらを物理アドレスに変換して、物理メモリに提示して必要な情報を取り出すことができます。OSはこれを、多くのプロセスで同時に実行し、プログラムを互いに保護し、OSを保護するようにします。全体的なアプローチには、多くの仕組み(低レベルマシンがたくさんあります)と、いくつかの重要なポリシーが必要です。重要なメカニズムを最初に説明して、下から上へと学んでいきます。

# 参考文献
[BH70] “The Nucleus of a Multiprogramming System”  
Per Brinch Hansen  
Communications of the ACM, 13:4, April 1970  
The first paper to suggest that the OS, or kernel, should be a minimal and flexible substrate for building customized operating systems; this theme is revisited throughout OS research history.

[CV65] “Introduction and Overview of the Multics System”  
F. J. Corbato and V. A. Vyssotsky  
Fall Joint Computer Conference, 1965  
A great early Multics paper. Here is the great quote about time sharing: “The impetus for time sharing first arose from professional programmers because of their constant frustration in debugging programs at batch processing installations. Thus, the original goal was to time-share computers to allow simultaneous access by several persons while giving to each of them the illusion of having the whole machine at his disposal.”

[DV66] “Programming Semantics for Multiprogrammed Computations”  
Jack B. Dennis and Earl C. Van Horn  
Communications of the ACM, Volume 9, Number 3, March 1966  
An early paper (but not the first) on multiprogramming.

[L60] “Man-Computer Symbiosis”  
J. C. R. Licklider  
IRE Transactions on Human Factors in Electronics, HFE-1:1, March 1960  
A funky paper about how computers and people are going to enter into a symbiotic age; clearly well ahead of its time but a fascinating read nonetheless.

[M62] “Time-Sharing Computer Systems”  
J. McCarthy  
Management and the Computer of the Future, MIT Press, Cambridge, Mass, 1962  
Probably McCarthy’s earliest recorded paper on time sharing. However, in another paper [M83], he claims to have been thinking of the idea since 1957. McCarthy left the systems area and went on to become a giant in Artificial Intelligence at Stanford, including the creation of the LISP programming language. See McCarthy’s home page for more info: http://www-formal.stanford.edu/jmc/

[M+63] “A Time-Sharing Debugging System for a Small Computer”  
J. McCarthy, S. Boilen, E. Fredkin, J. C. R. Licklider  
AFIPS ’63 (Spring), New York, NY, May 1963  
A great early example of a system that swapped program memory to the “drum” when the program wasn’t running, and then back into “core” memory when it was about to be run.

[M83] “Reminiscences on the History of Time Sharing”  
John McCarthy  
Winter or Spring of 1983  
Available: http://www-formal.stanford.edu/jmc/history/timesharing/timesharing.html  
A terrific historical note on where the idea of time-sharing might have come from, including some doubts towards those who cite Strachey’s work [S59] as the pioneering work in this area.

[NS07] “Valgrind: A Framework for Heavyweight Dynamic Binary Instrumentation”  
Nicholas Nethercote and Julian Seward  
PLDI 2007, San Diego, California, June 2007  
Valgrind is a lifesaver of a program for those who use unsafe languages like C. Read this paper to learn about its very cool binary instrumentation techniques – it’s really quite impressive.

[R+89] “Mach: A System Software kernel”  
Richard Rashid, Daniel Julin, Douglas Orr, Richard Sanzi, Robert Baron, Alessandro Forin, David Golub, Michael Jones  
COMPCON 89, February 1989  
Although not the first project on microkernels per se, the Mach project at CMU was well known and influential; it still lives today deep in the bowels of Mac OS X.

[S59] “Time Sharing in Large Fast Computers”  
C. Strachey  
Proceedings of the International Conference on Information Processing, UNESCO, June 1959  
One of the earliest references on time sharing.  

[S+03] “Improving the Reliability of Commodity Operating Systems”  
Michael M. Swift, Brian N. Bershad, Henry M. Levy  
SOSP 2003  
The first paper to show how microkernel-like thinking can improve operating system reliability.

\newpage

# 14 Interlude: Memory API
この章で、UNIXシステムでのメモリ割り当てインタフェースについて説明します。提供されるインターフェイスは非常にシンプルです。

>> CRUX: HOW TO ALLOCATE AND MANAGE MEMORY  
>> UNIX/Cプログラムでは、堅牢で信頼性の高いソフトウェアを構築する上で、メモリの割り当てと管理の方法を理解することが重要です。よく使われるインターフェースは何ですか？どのような間違いを避けるべきですか？

## 14.1 Types of Memory
Cプログラムの実行には、2種類のメモリが割り当てられています。最初のものはスタックメモリと呼ばれ、その割り当てと割り当て解除はコンパイラによって暗黙的に管理されます。このため、自動メモリと呼ばれることもあります。

Cでスタック上のメモリを宣言するのは簡単です。たとえば、xと呼ばれる整数の場合、関数`func()`にあるスペースが必要であるとします。そのような記憶を宣言するには、次のようなことをしてください。
```c
void func() {
    int x; // declares an integer on the stack
    ...
}
```
コンパイラは残りを行い、`func()`を呼び出すときにスタックにスペースを確保します。関数から戻ると、コンパイラはメモリを解放します。したがって、呼び出しの呼び出しを超えて生きる情報が必要な場合は、その情報をスタックに残さないほうがよいでしょう。

ヒープメモリと呼ばれる2種類目のメモリが長持ちするメモリが必要性です。すべての割り当てと割り当て解除は、プログラマによって明示的に処理されます。これを使うことは間違いなく重い責任です！そして確かに多くのバグの原因にもなっています。しかし、注意を払うと、このようなインターフェースを問題なく使用できます。ヒープに整数を割り当てる方法の例を次に示します。

```c
void func() {
int *x = (int *) malloc(sizeof(int));
...
}
```
この小さなコードスニペットに関するいくつかの注意をこれから述べます。最初に、スタックとヒープの両方の割り当てがこの行で行われることに気付くかもしれません。まず、コンパイラは、ポインタ(int * x)の宣言を見たときに整数へのポインタのためのスペースを確保することを知っています。その後、プログラムが`malloc()`を呼び出すと、ヒープ上の整数のためのスペースを要求します。ルーチンはそのような整数のアドレスを返します(成功した場合はアドレス、失敗した場合はNULLを返します)。その後、プログラムで使用するためにスタックに格納されます。

明示的な性質のために、そしてヒープメモリは、より多様な使用法のために、ユーザーとシステムの両方にとってより多くの課題が存在します。

## 14.2 The `malloc()` Call
`malloc()`呼び出しは非常に単純です。ヒープ上の空き容量を求めるサイズを渡し、それが成功し、新たに割り当てられた領域へのポインタを返すか、失敗しNULLを返します。マニュアルページには、mallocを使用するために必要なことが示されています。コマンドラインでman mallocと入力すると、次のように表示されます。
```c
#include <stdlib.h>
...
void *malloc(size_t size);
```
この情報から、mallocを使用するためのヘッダファイルstdlib.hをインクルードするだけで済みます。実際には、これを行う必要はありません。すべてのCプログラムがデフォルトでリンクするCライブラリには、その内部に`malloc()`のコードがあります。ヘッダを追加するだけで、コンパイラは`malloc()`を正しく呼び出すかどうかを確認できます(たとえば、適切な数の引数を適切な型に渡すなど)

単一のパラメータ`malloc()`は、必要なバイト数を単純に記述するタイプtの型です。しかし、ほとんどのプログラマーはここに数字を直接入力しません(10など)。代わりに、様々なルーチンおよびマクロが利用されます。たとえば、倍精度浮動小数点値にスペースを割り当てるには、次のようにします。
```c
double *d = (double *) malloc(sizeof(double));
```
うわー、二重にたくさんあります！この`malloc()`の呼び出しは、`sizeof()`演算子を使用して適切な量のスペースを要求します。これは一般にコンパイル時の演算子とみなされます。つまり、コンパイル時に実際のサイズが分かり、したがってmallocの引数として数値(この場合は8、倍精度の場合は8)が代入されます。このため、`sizeof()`は関数呼び出しではなく演算子として正しく考えられます(関数呼び出しは実行時に行われます)。

>> TIP: WHEN IN DOUBT, TRY IT OUT  
>> 使用しているルーチンや演算子がどのように動作しているかわからない場合は、単純に試してみて、期待どおりに動作するかどうかを確認することに代わるものはありません。マニュアルページやその他のドキュメントを読むのは便利ですが、実際にどのように動作するかは重要です。いくつかのコードを書いてテストしてください。それは間違いなくあなたのコードがあなたの望むように動作することを確認する最善の方法です。

また、変数の名前(型だけでなく)を`sizeof()`に渡すこともできますが、場合によっては結果が得られない場合もありますので注意してください。たとえば、次のコードスニペットを見てみましょう。
```c
int *x = malloc(10 * sizeof(int));
printf("%d\n", sizeof(x));
```
最初の行では、10個の整数の配列のためのスペースを宣言しました。これはきれいで素敵です。しかし、次の行で`sizeof()`を使用すると、4(32ビットマシンの場合)や8(64ビットマシンの場合)などの小さな値が返されます。その理由は、`sizeof()`は、動的に割り当てられたメモリ量ではなく、整数へのポインタの大きさを単に求めていると考えているからです。しかし、`sizeof()`が期待通りに動作することもあります。
```c
int x[10];
printf("%d\n", sizeof(x));
```
この場合、40バイトが割り当てられていることをコンパイラが知るための十分な静的情報があります。注意すべき別の場所は文字列です。文字列のスペースを宣言するときは、関数`strlen()`を使用して文字列の長さを取得し、その末尾のスペースを確保するために1を追加する次のイディオムを使用します。malloc(strlen(s)+ 1)`sizeof()`を使用すると、ここで問題が発生する可能性があります。  
`malloc()`はvoid型へのポインタを返します。こうしている理由は、C言語でアドレスを渡して、プログラマがそれをどうするかを決定させるためです。プログラマーは、キャストと呼ばれるものを使用することによってさらに助けます。上記の例では、プログラマーは`malloc()`の戻り値の型をdoubleへのポインタにキャストします。`malloc()`の結果をキャストすることは、コンパイラやあなたのコードを読んでいる他のプログラマーには、「ええ、私がやっていることは分かっています」と言っている以外に、キャストは本当に何もやってくれません。正確に使うため以外にキャストは必要ありません。


## 14.3 The `free()` Call
結論として、メモリの割り当ては簡単な方程式の一部です。いつ、どのように、そしてメモリを解放するかを知ることは難しい部分です。使用されていないヒープメモリを解放するために、プログラマは単に`free()`を呼び出します。
```c
int *x = malloc(10 * sizeof(int));
...
free(x);
```
このルーチンは`malloc()`によって返された1つの引数をとります。したがって、割り当てられた領域のサイズはユーザーによって渡されず、メモリ割り当てライブラリ自体によって追跡される必要があります。

## 14.4 Common Errors
`malloc()`と`free()`の使用には多くの一般的なエラーがあります。ここでは、学部のオペレーティングシステムコースを教える上で何度も見てきたことがあります。これらの例はすべて、コンパイラからの指令でコンパイルして実行します。正しいCプログラムを構築するにはCプログラムをコンパイルする必要がありますが、(難しい方法で)よく学びたいのであれば十分に知る必要があります。

正確なメモリ管理は、多くの新しい言語が自動メモリ管理をサポートしているという問題がありました。そのような言語では、メモリを割り当てるために`malloc()`に似たもの(通常は新しいものや新しいオブジェクトを割り当てるのに似たもの)を呼び出している間に、空き領域に何かを呼び出す必要はありません。例えば、ガベージコレクタでは、参照していないメモリを見つけ出し、解放します。

### Forgetting To Allocate Memory
多くのルーチンは、呼び出す前にメモリを割り当てることを期待しています。たとえば、ルーチンstrcpy(dst、src)は、ソースポインタからデスティネーションポインタに文字列をコピーします。しかし、あなたが慎重でない場合、あなたはこれを行うかもしれません。
```c
char *src = "hello";
char *dst; // oops! unallocated
strcpy(dst, src); // segfault and die
```
このコードを実行すると、セグメンテーションフォルトにつながる可能性があります。つまり、「メモリに関して何か間違っているよ！もしかしてアホなプログラマ？？やめたら、プログラマ」というコードです。

>> TIP: IT COMPILED OR IT RAN ！= IT IS CORRECT  
>> プログラムがコンパイルされたか、正しく1回または複数回実行されただけであっても、プログラムが正しいとは限りません。多くのイベントは、あなたはそれがうまくいくと思うかもしれませんが、何かの拍子で止まります。一般的な生徒の反応は、「ちょっと待って！プログラムは以前は動いた！」と言い、コンパイラ、オペレーティングシステム、ハードウェア、または教授の責任を(たとえそれを言い表しても)責めます。しかし、問題は通常あなたのコード内にあると思うのが正しいです。あなたがそれらの他のコンポーネントを責める前に、デバッグしてください。

この場合、適切なコードは次のようになります。
```c
char *src = "hello";
char *dst = (char *) malloc(strlen(src) + 1);
strcpy(dst, src); // work properly
```
代わりに、`strdup()`を使用して、あなたの人生をさらに楽にすることができます。詳細はstrdupのmanページを参照してください。

### Not Allocating Enough Memory
関連するエラーで十分なメモリが割り当てられていない場合があります。バッファオーバーフローと呼ばれることもあります。上記の例では、一般的なエラーは、宛先バッファに十分な余裕を持たせることです。
```c
char *src = "hello";
char *dst = (char *) malloc(strlen(src)); // too small!
strcpy(dst, src); // work properly
```
面白いことに、mallocの実装方法やその他の詳細によっては、このプログラムはよく正しく動作します。場合によっては、文字列のコピーが実行されるときに、割り当てられたスペースの最後を過ぎて1バイト分先に書き込まれますが、これは無害で、おそらくは使用されない変数を上書きするだけかもしれません。しかし、場合によっては、これらのオーバーフローは非常に有害であり、実際にはシステム[W06]の多くのセキュリティ脆弱性の原因です。他のケースでは、mallocライブラリは少し余分なスペースを割り当てていますので、あなたのプログラムは実際には他の変数の値を書き換えず、うまく動作します。他の場合では、プログラムは本当に故障しクラッシュするかもしれません。たとえそれが正しく実行されたとしても、正しいことを意味するわけではありません。

### Forgetting to Initialize Allocated Memory
このエラーでは、`malloc()`を適切に呼び出しますが、新しく割り当てられたデータ型にいくつかの値を入力するのを忘れてしまいます。絶対にこれはしないでください！あなたが忘れてしまった場合、プログラムは最終的に、未知の値のデータをヒープから読み出す初期化されていない読み込みに遭遇します。そこに何があるのか誰が知っていますか？あなたが運が良ければ、プログラムがまだ機能するような価値があります(たとえば、ゼロ)。不幸であれば、ランダムなデータや害があるデータに遭遇するでしょう。

### Forgetting To Free Memory
別の一般的なエラーはメモリリークと呼ばれ、メモリを解放するのを忘れたときに発生します。長時間実行するアプリケーションやシステム(OS自体など)では、これは大きな問題です。したがって、一般的に、あなたがメモリのチャンクで完了したら、あなたはそれを解放することを確認する必要があります。注意してほしいことはガベージコレクションを使っている言語では、ここの話は役に立たないことに注意してください。しかし、ガベージコレクションでも、まだメモリの一部を参照している場合、ガベージコレクタはそれを解放しないため、現代の言語でもメモリリークが問題になります。

場合によっては、`free()`を呼び出すのが合理的であるように見えるかもしれません。たとえば、あなたのプログラムは短命であり、すぐに終了します。この場合、プロセスが終了すると、OSは割り当てられたすべてのページをクリーンアップし、したがってメモリリーク自体は発生しません。これは確かに機能しますが、開発するときはおそらく戦略が悪いので、そのような戦略を選ぶことには注意してください。長期的には、プログラマーとしてのあなたの目標の1つは良い戦略を開発することです。そのような戦略の1つは、あなたがどのようにメモリを管理しているのか(Cのような言語で)、あなたが割り当てたブロックを解放することです。あなたがそうしないで逃げることができたとしても、あなたが明示的に割り当てる各バイトを解放する習慣を得るのは良いでしょう。

### Freeing Memory Before You Are Done With It
場合によっては、プログラムの使用が終了する前にメモリが解放されることもあります。そのような間違いは、ぶら下がりポインタと呼ばれ、あなたが推測できるように、それはまた悪いことです。後で使用すると、プログラムがクラッシュしたり、有効なメモリを上書きすることができます(たとえば、`free()`を呼び出した後、`malloc()`をもう一度呼び出すと、余分に解放されたメモリがリサイクルされます)。

### Freeing Memory Repeatedly
プログラムでは、メモリを複数回解放することもあります。これはダブルフリーとして知られています。その結果は未定義です。あなたが想像しているように、メモリ割り当てライブラリは混乱し、あらゆる種類の奇妙なことをするかもしれません。クラッシュは共通の結果です。

### Calling `free()` Incorrectly
私たちが議論している最後の問題は、`free()`の呼び出しが間違っていることです。結局、`free()`はあなたが以前に`malloc()`から受け取ったポインタの1つだけを渡すことを期待しています。あなたが他の価値を渡すと、悪いことが起きる可能性があります。したがって、このような無効な自由は危険であり、もちろん避けなければなりません。

>> ASIDE: WHY NO MEMORY IS LEAKED ONCE YOUR PROCESS EXITS  
>>短命のプログラムを書くときには、`malloc()`を使っていくらかの領域を割り当てるかもしれません。プログラムは実行され、完了しようとしています。終了する直前に`free()`を呼び出す必要がありますか？それが間違っているように見えますが、実際の意味では記憶が失われません。理由は簡単です。実際には、システムには2つのレベルのメモリ管理があります。第1レベルのメモリ管理は、実行時にプロセスにメモリを引き渡すOSによって実行され、プロセスが終了する(または終了する)ときにメモリを取り戻します。第2レベルの管理は、`malloc()`と`free()`を呼び出すときにヒープ内など、各プロセス内にあります。`free()`を呼び出すことに失敗してヒープ内のメモリがリークしても、オペレーティングシステムは、プログラムのすべてのメモリ(コード、スタック、ここではヒープに関連するページを含む)を実行が終了したら再利用します。アドレス空間内のヒープの状態が何であっても、OSはプロセスが終了したときにそれらのページをすべて取り戻すので、解放していないにもかかわらずメモリが失われます。  
>> したがって、短命のプログラムでは、メモリが漏れても操作上の問題が発生しないことがよくあります(ただし、悪い形であると考えられます)。長時間実行しているサーバー(Webサーバーやデータベース管理システムなど、決して終了しないサーバーなど)を作成すると、漏れたメモリははるかに大きな問題になり、メモリが不足するとクラッシュする可能性があります。もちろん、メモリのリークは、オペレーティングシステムそのものの1つの特定のプログラム内でのさらに大きな問題です。カーネルコードを書く人は、すべての中で最も厳しい仕事をしています...

### Summary
ご覧のように、メモリを乱用する方法はたくさんあります。メモリのエラーが頻繁に発生するため、コード内でこのような問題を見つけるのに役立つエコスペースのツールが開発されました。 [HJ92]とvalgrind [SN05]の両方を調べてください。どちらもメモリ関連の問題の原因を突き止めるのに役立ちます。これらの強力なツールの使用に慣れたら、そのツールを使わずに生き残った方法が不思議に思うでしょう。

## 14.5 Underlying OS Support
`malloc()`と`free()`について議論するときにシステムコールについて話していないことに気づいたかもしれません。その理由はシンプルです。システムコールではなく、ライブラリ呼び出しです。したがって、mallocライブラリは仮想アドレス空間内の空間を管理しますが、それ自体はOSに呼び出されるいくつかのシステムコールの上に構築され、より多くのメモリを要求したり、システムに戻っていくことがあります。

そのようなシステムコールの1つはbrkと呼ばれ、プログラムのブレークの位置を変更するために使用されます。ヒープの終わりの位置です。1つの引数(新しいブレークのアドレス)が必要なので、新しいブレークが現在のブレークより大きいか小さいかに基づいてヒープのサイズを増減します。追加の呼び出しsbrkはインクリメントされますが、そうでなければ同様の目的を果たします。

brkまたはsbrkのいずれかを直接呼び出すことはできません。それらはメモリ割り当てライブラリによって使用されます。あなたがそれらを使用しようとすると、おそらく何かが(ひどく)間違ってしまうことになります。代わりに`malloc()`と`free()`を使用してください。

最後に、`mmap()`呼び出しによってオペレーティングシステムからメモリを取得することもできます。正しい引数を渡すことで、`mmap()`はプログラム内で匿名のメモリ領域を作成することができます。この領域は特定のファイルに関連付けられず、スワップ空間に関連付けられます。このメモリは、ヒープのように扱うことができ、ヒープのように管理することもできます。詳細は、`mmap()`のマニュアルページを参照してください。

## 14.6 Other Calls
メモリ割り当てライブラリがサポートするその他の呼び出しがいくつかあります。たとえば、`calloc()`はメモリを割り当て、返す前にメモリをゼロにします。これは、メモリがゼロになっていると仮定して、それを自分で初期化することを忘れるところで、いくつかのエラーを防ぎます(上記の「初期化されていない読み取り」の段落を参照してください)。ルーチン`realloc()`は、何か(配列など)にスペースを割り当ててから、何かを追加する必要があるときにも便利です。`realloc()`は新しいメモリ領域を作り、古い領域をポインタを新しい領域に返します。

## 14.7 Summary
メモリ割り当てを扱うAPIのいくつかを紹介しました。いつものように、私たちは基本的な話をしました。詳細は他の場所でも入手可能です。詳細は、Cブック[KR88]とスティーブンス[SR05](第7章)を参照してください。これらの問題の多くを自動的に検出して修正する方法に関する現代的な最新の論文については、Novark et al。 [N+07]というものがあります。このペーパーには、一般的な問題の素敵な要約と、それらを見つけて修正するためのいくつかのすばらしいアイデアも含まれています。

# 参考文献

[HJ92] Purify: Fast Detection of Memory Leaks and Access Errors  
R. Hastings and B. Joyce  
USENIX Winter ’92  
The paper behind the cool Purify tool, now a commercial product.  

[KR88] “The C Programming Language”  
Brian Kernighan and Dennis Ritchie  
Prentice-Hall 1988  
The C book, by the developers of C. Read it once, do some programming, then read it again, and then keep it near your desk or wherever you program.

[N+07] “Exterminator: Automatically Correcting Memory Errors with High Probability”  
Gene Novark, Emery D. Berger, and Benjamin G. Zorn  
PLDI 2007  
A cool paper on finding and correcting memory errors automatically, and a great overview of many common errors in C and C++ programs.

[SN05] “Using Valgrind to Detect Undefined Value Errors with Bit-precision”  
J. Seward and N. Nethercote  
USENIX ’05  
How to use valgrind to find certain types of errors.

[SR05] “Advanced Programming in the UNIX Environment”  
W. Richard Stevens and Stephen A. Rago  
Addison-Wesley, 2005  
We’ve said it before, we’ll say it again: read this book many times and use it as a reference whenever you are in doubt. The authors are always surprised at how each time they read something in this book, they learn something new, even after many years of C programming.

[W06] “Survey on Buffer Overflow Attacks and Countermeasures”  
Tim Werthman  
Available: www.nds.rub.de/lehre/seminar/SS06/Werthmann BufferOverflow.pdf  
A nice survey of buffer overflows and some of the security problems they cause. Refers to many of the famous exploits.

\newpage

# 15 Mechanism: Address Translation
CPUの仮想化の開発では、限定直接実行(LDE)と呼ばれる一般的なメカニズムに焦点を当てました。LDEの背後にある考え方は単純です。ほとんどの場合、プログラムをハードウェア上で直接実行させたいです。しかし、特定のキーポイント(プロセスがシステムコールを発行するときやタイマー割り込みが発生したときなど)では、OSが関与し、「正しい」ことが起こるように調整します。したがって、OSは、少しハードウェアをサポートして、効率的な仮想化を実現するために、実行中のプログラムから抜け出すために最善を尽くします。しかし、これらの重要な時点で介在することにより、OSはハードウェアの制御を確実に維持します。効率性と制御性は、最新のオペレーティングシステムの主な目標の2つです。

メモリの仮想化では、同様の戦略を追求し、効率と制御の両面を達成しながら、望ましい仮想化を実現します。効率性は、最初は非常に初歩的である(例えば、ほんの数個のレジスタ)ハードウェアサポートを利用しますが、かなり複雑になっています(例えば、TLB、ページテーブルサポートなど)制御は、OSがアプリケーションがメモリにアクセスすることを許可されていないことを保証することを意味します。したがって、アプリケーションをお互いから保護し、アプリケーションからOSを保護するために、ここでもハードウェアの助けが必要です。最後に、柔軟性の点で、VMシステムからもう少し必要なものがあります。具体的には、プログラムがアドレス空間をどのような方法でも使用できるようにしたいので、システムのプログラミングを容易にしたいと考えています。それで、私たちはとある要点にたどり着きます。

>> THE CRUX:HOW TO EFFICIENTLY AND FLEXIBLY VIRTUALIZE MEMORY  
>> どのように効率的な仮想化を構築できますか？アプリケーションに必要な柔軟性はどのように提供しますか？アプリケーションがアクセスできるメモリの場所をどのように制御し、アプリケーションのメモリアクセスが適切に制限されているかを確実に管理するにはどうすればよいですか？どのようにしてこのすべてを効率的に行うのですか？

限定直接実行の一般的なアプローチに加えて、使用する一般的な手法は、ハードウェアベースのアドレス変換、または単にアドレス変換と呼ばれるものです。アドレス変換では、ハードウェアは、各メモリアクセス(命令フェッチ、ロード、またはストア)を変換し、命令によって提供される仮想アドレスを、必要な情報が実際に位置する物理アドレスに変更します。したがって、各メモリ参照ごとに、ハードウェアによってアドレス変換が実行され、アプリケーションメモリ参照がメモリ内の実際の位置にリダイレクトされます。

もちろん、ハードウェアだけでは、メモリを仮想化することはできません。効率的に行うための低レベルのメカニズムを提供するだけです。正しい変換が行われるように、ハードウェアをセットアップするためには、OSが重要なポイントに関わる必要があります。したがって、メモリを管理し、空き領域と使用中の領域を追跡し、メモリの使用方法を制御するために適切に介入する必要があります。

もう一度この仕事の目標は、美しい錯覚を作り出すことです。プログラムには、独自のコードとデータが存在する独自のプライベートメモリがあります。その仮想現実の背後には醜い物理的真理があります。つまり、CPU(またはCPU達)が1つのプログラムと次のプログラムを実行する間に、多くのプログラムが実際に同時にメモリを共有しています。仮想化を通じて、OSは(ハードウェアの助けを借りて)醜いマシンの現実を、有用で、強力で、使いやすい抽象であるものに変えます。

## 15.1 Assumptions
メモリを仮想化しようとする私たちの最初の試みは、非常に簡単です。TLB、マルチレベルのページテーブル、その他の技術的な不思議な点を理解しようとすると、すぐにOSがあなたを馬鹿にするでしょう。ここではOSがどのように動くかをお話しするだけです。

具体的には、ユーザーのアドレス空間を物理メモリに連続して配置する必要があると想定します。わかりやすくするために、アドレス空間のサイズがそれほど大きくないと仮定します。具体的には、物理メモリのサイズよりも小さいことを示します。最後に、各アドレス空間がまったく同じサイズであると仮定します。これらの前提が非現実的であるとは心配しないでください。そのようにして現実的な仮想化を実現します。

## 15.2 An Example
アドレス変換を実装するために必要なことと、そのようなメカニズムが必要な理由を理解するために、簡単な例を見てみましょう。アドレス空間が図15.1のようなプロセスがあるとします。ここで検討するのは、メモリから値をロードし、3ずつインクリメントした後、その値をメモリに戻す短いコードシーケンスです。このコードのC言語表現では次のようになるかもしれません。
```c
void func() {
int x = 3000; // thanks, Perry.
x = x + 3; // this is the line of code we are interested in
```

>> TIP: INTERPOSITION IS POWERFUL  
>> Interpositionは、コンピュータシステムにおいて大きな効果を発揮するためによく使用される一般的かつ強力な技術です。仮想化メモリでは、ハードウェアが各メモリアクセスに介入し、プロセスによって発行された各仮想アドレスを、必要な情報が実際に格納されている物理アドレスに変換します。しかしながら、Interpositionの一般的な技術は、より広範に適用可能です。実際には、ほとんどすべての明確なインタフェースをInterpositionさせたり、新しい機能を追加したり、システムの他の側面を改善したりすることができます。このようなアプローチの通常のメリットの1つは透明性です。インタフェースのクライアントを変更する必要がなくInterpositionが行われることが多いです。

コンパイラは、このコード行をアセンブリに変換します。これは、(x86アセンブリ内で)このように見えます。Linuxの場合はobjdump、Macの場合はotoolを使って逆アセンブルしてください：
```c
128: movl 0x0(%ebx), %eax ;load 0+ebx into eax
132: addl $0x03, %eax ;add 3 to eax register
135: movl %eax, 0x0(%ebx) ;store eax back to mem
```
このコードスニペットは比較的簡単です。xのアドレスがレジスタebxに置かれていると想定し、movl命令("ロングワード"移動の場合)を使用してそのアドレスの値を汎用レジスタeaxにロードします。次の命令はeaxに3を加え、最後の命令はその同じ位置のメモリにeaxの値を戻します。

![](../15/img/fig15_1.PNG)

図15.1(137ページ)では、コードとデータの両方がプロセスのアドレス空間にどのように配置されているかを見ることができます。3命令コードシーケンスは128番地(最上部付近のコードセクション)に配置され、変数xの値はアドレス15KBに配置されます(最下部にスタックされる)。図では、xの初期値はスタック上の位置に示されているように3000です。

これらの命令が実行されると、プロセスの観点から、次のメモリアクセスが行われます。
- 128番地の命令をフェッチする
- この命令を実行する(アドレス15 K​​Bからロードする)
- アドレス132で命令をフェッチする
- この命令を実行する(メモリ参照なし)
- 135番地に命令をフェッチする
- この命令を実行する(15KBのアドレスにストアする)

プログラムの観点からは、アドレス空間はアドレス0から始まり、最大16 KBまで増加します。それが生成するすべてのメモリ参照は、これらの範囲内になければなりません。しかし、メモリを仮想化するために、OSは必ずしもアドレス0でなくても、物理メモリのどこかにプロセスを配置する必要があります。したがって、プロセスに透過的な方法でこのプロセスをどのようにメモリ内に再配置できますか？実際にはアドレス空間が他の物理アドレスに位置しているときに、0から始まる仮想アドレス空間の錯覚をどのように提供することができますか？

![](../15/img/fig15_2.PNG)

このプロセスのアドレス空間がメモリに配置された後の物理メモリの例を図15.2に示します。この図では、物理メモリの最初のスロットを使用してOSを認識し、上の例のプロセスを物理メモリアドレス32 KBから始まるスロットに再配置しました。他の2つのスロットは空いています(16 KB-32 KBと48 KB-64 KB)

## 15.3 Dynamic (Hardware-based) Relocation
ハードウェアベースのアドレス変換の理解を得るために、まず最初のアプローチについて説明します。1950年代後半の初めてのタイムシェアリングマシンでは、基本(base)と境界(bounds)と呼ばれる簡単なアイデアが導入されました。この技術は動的再配置(dynamic relocation)とも呼ばれます。両方の用語を同じ意味で使用します[SS74]。

具体的には、各CPU内に2つのハードウェアレジスタが必要です。1つは基本レジスタ、もう1つは境界レジスタです(制限レジスタとも呼ばれます)。この基本と境界のペアは、私たちが物理メモリ内のどこにでもアドレス空間を置くことを可能にし、プロセスがそれ自身のアドレス空間にしかアクセスできないようにします。

この設定では、各プログラムはアドレスゼロでロードされているかのように記述され、コンパイルされます。しかし、プログラムが実行を開始すると、OSは物理メモリ内のどこにロードすべきかを決定し、基本レジスタをその値に設定する。上記の例では、OSは物理アドレス32 KBにプロセスをロードすることを決定し、基本レジスタをこの値に設定します。

プロセスが実行されているときに面白いことが起こります。ここで、プロセスによってメモリ参照が生成されると、プロセッサによって次のように変換されます。

physical address = virtual address + base

>> ASIDE: SOFTWARE-BASED RELOCATION  
>>ハードウェアサポートが始まる前の初期の段階では、純粋にソフトウェアメソッドを使用して粗い形式の再配置を実行したシステムもありました。基本的な手法は静的再配置(static relocation)と呼ばれ、ローダーと呼ばれるソフトウェアが実行しようとしている実行ファイルを取り込み、そのアドレスを物理メモリの望ましいオフセットに書き換えます。  
>>例えば、命令がアドレス1000からレジスタ(例えば、movl 1000、％eax)へのロードであり、プログラムのアドレス空間が3000番地から読み込まれた場合(プログラムが考えるように0ではなく)ローダは、各アドレスを3000でオフセットするように命令を書き換えます(たとえば、movl 4000、％eax)。このようにして、プロセスのアドレス空間の単純な静的再配置がされます。  
>>しかし、静的再配置には多くの問題があります。プロセスが不正なアドレスを生成し、他のプロセスやOSメモリに不正にアクセスする可能性があります。そして最も重要なのは、保護を提供しないことです。真の保護[WL+93]のためには、一般的にハードウェアのサポートが必要になります。別の否定的な点は、いったん配置されると、後でアドレス空間を別の場所に再配置することが難しいことです[M65]。

プロセスによって生成された各メモリ参照は仮想アドレスです。ハードウェアはベースレジスタの内容をこのアドレスに加算し、結果はメモリシステムに発行できる物理アドレスです。これをよりよく理解するために、1つの命令が実行されたときの動作をトレースしてみましょう。具体的には、前のシーケンスからの1つの命令を見てみましょう：
```
128: movl 0x0(%ebx), %eax
```
プログラムカウンタ(PC)は128に設定されています。ハードウェアがこの命令をフェッチする必要があるときには、最初に32KBの基本レジスタ値(32768)に値を加算して32896の物理アドレスを取得します。ハードウェアはその物理アドレスから命令をフェッチします。次に、プロセッサは命令の実行を開始します。ある時点で、プロセスは仮想アドレス15 K​​Bからプロセッサに命令を発行し、再び基本レジスタ(32 KB)に加算して、最終物理アドレス47 KBを取得し、その結果、必要な内容を取得します。

仮想アドレスを物理アドレスに変換することは、まさにアドレス変換と呼ばれる手法です。すなわち、ハードウェアは、プロセスが参照していると考える仮想アドレスを取り、データが実際に存在する物理アドレスに変換します。このアドレスの再配置は実行時に行われるため、プロセスの実行が開始された後でもアドレス空間を移動できるため、技術はよく動的再配置(dynamic relocation)[M65]と呼ばれます。

>> TIP: HARDWARE-BASED DYNAMIC RELOCATION  
>> 動的再配置では、少しハードウェアが大きくなります。具体的には、ベースレジスタは、(プログラムによって生成された)仮想アドレスを物理アドレスに変換するために使用されます。境界(または制限)レジスタは、そのようなアドレスがアドレス空間の範囲内にあることを保証します。これらは一緒になって、シンプルで効率的なメモリの仮想化を実現します。

今あなたは疑問に思っているかもしれません。その境界(限界)登録はどうなりましたか？ということです。結局のところ、これはベースと境界アプローチではありませんか？確かにそうです。あなたが推測したように、境界レジスタは保護を助けるためのものです。具体的には、プロセッサはまずメモリ参照が境界内にあることを確認して、メモリ参照が合法であることを確認する。上記の単純な例では、境界レジスタは常に16 KBに設定されます。プロセスが境界よりも大きい仮想アドレスまたは負のアドレスを生成した場合、CPUは例外を発生させ、プロセスは終了する可能性があります。境界のポイントは、プロセスによって生成されたすべてのアドレスが合法でプロセスの「境界」内にあることを確認することです。

基本レジスタと境界レジスタは、チップ上に保持されているハードウェア構造(CPUごとに1組)であることに注意してください。メモリ管理ユニット(MMU)のアドレス変換に役立つプロセッサの一部を人々が呼ぶこともあります。より洗練されたメモリ管理技術を開発するにつれて、MMUにさらに多くの回路を追加するはずです。

バインドされたレジスタについての小さなもので、2つの方法のいずれかで定義できます。上記のように、アドレス空間のサイズを保持するため、ハードウェアは、ベース値を加算する前に、仮想アドレスを最初にチェックします。2番目の方法では、アドレス空間の最後の物理アドレスが保持されます。したがって、ハードウェアはまずベース値を加算し、アドレスが境界内にあることを確認します。どちらの方法も論理的に同等です。簡単にするために、通常は前者の方法を仮定します。

### Example Translations
ベースとバウンドによるアドレス変換をより詳細に理解するために、例を見てみましょう。物理アドレス16 KBにサイズ4 KB(非現実的で小さいです)のアドレス空間がロードされたプロセスを想像してください。以下に、いくつかのアドレス変換の結果を示します。

![](../15/img/fig15_2_1.PNG)

この例からわかるように、仮想アドレス(アドレス空間へのオフセットと見なすことができます)に基本アドレスを加算するだけで簡単に物理アドレスを取得できます。仮想アドレスが「大きすぎる」または負の場合にのみ、結果がフォルトとなり、例外が発生します。

>> ASIDE: DATA STRUCTURE — THE FREE LIST  
>> OSは、空きメモリのどの部分が使用されていないかを追跡して、プロセスにメモリを割り当てることができるようにする必要があります。もちろん、このようなタスクには多くの異なるデータ構造を使用できます。最も単純なもの(ここではこれを仮定します)は、現在使用されていない物理メモリの範囲のリストであるフリーリストです。

![](../15/img/fig15_3.PNG)

## 15.4 Hardware Support: A Summary
ハードウェアから必要なサポートを要約しましょう(図15.3参照)まず、CPU仮想化の章で説明したように、2つの異なるCPUモードが必要です。OSは特権モード(またはカーネルモード)で実行され、マシン全体にアクセスできます。アプリケーションはユーザーモードで実行されます。ユーザーモードでは、ユーザーの操作が制限されています。ある種のプロセッサステータスワードに格納されている単一のビットは、CPUが現在どのモードで動作しているかを示します。特定の特別な機会(例えば、システムコールまたは他の種類の例外または割り込み)が発生すると、CPUはモードを切り替えます。

ハードウェアは、ベースレジスタと境界レジスタ自体も提供する必要があります。したがって、各CPUは、CPUのメモリ管理ユニット(MMU)の一部である一組のレジスタを追加します。ユーザープログラムが実行されると、ハードウェアは、ユーザープログラムによって生成された仮想アドレスにベース値を加算することによって、各アドレスを変換します。また、ハードウェアは、アドレスが有効であるかどうかを確認することができなければなりません。これは、境界レジスタとCPU内のいくつかの回路を使用して行います。

ハードウェアは、ベースレジスタと境界レジスタを変更するための特別な命令を提供し、異なるプロセスが実行されたときにOSがそれらを変更できるようにする必要があります。これらの命令には特権があります。カーネル(または特権)モードでのみ、レジスタを変更できます。実行中にベースレジスタを任意に変更することができれば、ユーザプロセスが壊れる可能性があると想像してください。

最後に、CPUは、ユーザプログラムが不法にメモリにアクセスしようとする状況(「範囲外」のアドレス)で例外を生成できなければないけません。この場合、CPUはユーザプログラムの実行を停止し、OSの「境界外」例外ハンドラを実行するように調整する必要があります。OSハンドラは、どのように反応するかを知ることができます。この場合、プロセスを終了する可能性があります。同様に、ユーザプログラムが(特権の)基本レジスタと境界レジスタの値を変更しようとすると、CPUは例外を発生させ、"ユーザモードで特権動作を実行しようとしました"というハンドラを実行する必要があります。CPUは、これらのハンドラの位置を通知する方法も提供する必要があります。このようにして特権命令をいくつか追加する必要があります。

![](../15/img/fig15_3.PNG)

## 15.5 Operating System Issues
ハードウェアが動的再配置をサポートするための新しい機能を提供するのと同じように、OSには新たな問題があります。ハードウェアサポートとOS管理の組み合わせにより、シンプルな仮想メモリの実装が可能になります。具体的には、仮想メモリのベース・バウンド・バージョンを実装するためにOSが関与しなければならないいくつかの重要な接合点があります。

まず、OSは、プロセスが作成されたときにアクションを実行し、メモリ内のアドレス空間のための領域を見つけなければなりません。幸いにも、各アドレス空間が(a)物理メモリのサイズより小さく、(b)同じサイズであるという前提を考えると、これはOSにとって非常に簡単です。物理メモリーをスロットの配列として表示し、各スロットがフリーであるか使用中であるかを追跡できます。新しいプロセスが作成されると、OSはデータ構造(フリーリストと呼ばれることが多い)を検索して、新しいアドレス空間のためのスペースを見つけて使用することをマークする必要があります。可変サイズのアドレス空間では、より複雑になりますが、今後の章ではその懸念を残していきます。

例を見てみましょう。図15.2(139ページ)では、OS自体が物理メモリの最初のスロットを使用していること、および上記の例のプロセスを物理メモリアドレス32 KBから始まるスロットに再配置したことがわかります。他の2つのスロットは空いています(16 KB-32 KBと48 KB-64 KB)。したがって、空きリストはこれらの2つのエントリで構成されます。

第2に、OSは、プロセスが終了したとき(すなわち、正常に終了したとき、または誤って実行されたために強制終了したとき)、他のプロセスまたはOSで使用するためにすべてのメモリを再利用するときに何らかの作業を行わなければいけません。プロセスが終了すると、OSはそのメモリを空きリストに戻し、必要に応じて関連するデータ構造をクリーンアップします。

第3に、コンテキストスイッチが発生した場合、OSはさらにいくつかのステップを実行する必要があります。実際には、各プログラムはメモリ内の異なる物理アドレスにロードされるため、実行中のプログラムごとにその値が異なります。したがって、OSは、プロセス間で切り替えるときに、ベースと境界のペアを保存して復元する必要があります。具体的には、OSがプロセスの実行を停止すると決定した場合、プロセスストラクチャやプロセス制御ブロック(PCB)などのプロセスごとの構造によって、ベースレジスタと境界レジスタの値をメモリに保存する必要があります。同様に、OSが実行中のプロセスを再開する(または最初に実行する)場合、CPU上のベースと境界の値をこのプロセスの正しい値に設定する必要があります。

![](../15/img/fig15_4.PNG)

プロセスが停止された(すなわち、実行されていない)ときに、OSはメモリ内のある場所から別の場所へアドレス空間を移動させることができます。プロセスのアドレス空間を移動するために、OSは最初にプロセスをディスケジューリングします。つまり、OSは現在の場所から新しい場所にアドレス空間をコピーします。最後にOSは、(プロセス構造内の)保存されたベースレジスタを更新して、新しい位置を指すようにします。プロセスが再開されると、その(新しい)ベースレジスタが復元され、命令とデータがメモリ内の全く新しい場所にあることを知らずに、再び実行を開始します。

第4に、OSは上記のように例外ハンドラまたは呼び出される関数を提供しなければいけません。OSはブート時に(特権命令によって)これらのハンドラをインストールします。たとえば、プロセスが境界外のメモリにアクセスしようとすると、CPUは例外を送出します。そのような例外が発生した場合、OSは対応する必要があります。OSの一般的な反応は敵意の1つとして、攻撃プロセスを終了させる可能性があります。OSは実行中のマシンを高度に保護する必要があるため、メモリアクセス違反をするプロセスは実行させない必要があります。

![](../15/img/fig15_5.PNG)

図15.5(145ページ)は、タイムライン内のハードウェア/OSの相互作用の大部分を示しています。この図は、起動時にOSを使用してマシンを使用できる状態にしてから、プロセス(プロセスA)の実行が開始されたときの状態を示しています。OSの介入なしにハードウェアによってメモリ変換がどのように処理されるかを注意してください。ある時点でタイマ割り込みが発生し、OSはプロセスBに切り替わり、プロセスBは(不正なメモリアドレスに対して)「不良ロード」を実行します。その時点で、OSは関与し、プロセスの終了とBのメモリを解放し、プロセステーブルからそのエントリを削除することによってクリーンアップする必要があります。この図からわかるように、我々は依然として制限付き直接実行という基本的なアプローチに従っています。ほとんどの場合、OSはハードウェアを適切に設定し、プロセスをCPU上で直接実行させます。プロセスが誤動作した場合にのみOSが関与しなければいけません。

## 15.6 Summary
この章では、アドレス変換と呼ばれる仮想メモリで使用される特定のメカニズムを使用した限定直接実行の概念を拡張しました。アドレス変換により、OSはプロセスからのすべてのメモリアクセスを制御し、アクセスがアドレス空間の範囲内に収まるようにします。この手法の効率性の鍵はハードウェアのサポートであり、仮想アドレス(プロセスのメモリビュー)を物理的なもの(実際のビュー)に変換するアクセスごとに迅速に変換を実行します。このすべては、再配置されたプロセスに対して透過的な方法で実行されます。そのプロセスはメモリ参照が変換されているということを知らず、素晴らしい錯覚を作り出します。

また、ベースと境界または動的再配置と呼ばれる1つの特定の仮想化形態も見てきました。仮想アドレスにベース・レジスタを加算し、プロセスによって生成されたアドレスが境界内にあるかどうかをチェックするには、少しだけハードウェア・ロジックを必要とするため、ベースと境界の仮想化は非常に効率的です。ベースと境界は保護も提供します。OSとハードウェアが組み合わさって、プロセスが独自のアドレス空間外でメモリ参照を生成できないようにします。確かに保護はOSの最も重要な目標の1つです。それがなければ、OSはマシンを制御することができません(プロセスがメモリを上書きすることができれば、トラップテーブルを上書きしてシステムを引き継ぐような厄介なことを簡単に行うことができます)

残念なことに、動的再配置のこの単純な手法は非効率的です。たとえば、図15.2(139ページ)に示されているように、再配置されたプロセスは32KBから48KBの物理メモリを使用しています。ただし、プロセススタックとヒープがあまりにも大きくないので、2つの間のスペースのすべてが単に無駄になります。このタイプの廃棄物は、通常、内部断片化と呼ばれ、割り当てられた単位内の空間がすべて使用されていない(すなわち、断片化されている)ので、空いている部分は無駄になります。現在のアプローチでは、より多くのプロセスに十分な物理メモリがあるかもしれませんが、現在は固定サイズのスロットにアドレス空間を配置することに制限されているため、内部断片化が発生する可能性があります。したがって、物理メモリをより有効に活用し、内部断片化を回避するために、より洗練されたメカニズムが必要になります。私たちの最初の試みは、セグメント化と呼ばれるベースと境界のわずかな一般化です。次にこれについて説明します。

# 参考文献

[M65] “On Dynamic Program Relocation”  
W.C. McGee  
IBM Systems Journal  
Volume 4, Number 3, 1965, pages 184–199  
This paper is a nice summary of early work on dynamic relocation, as well as some basics on static relocation.

[P90] “Relocating loader for MS-DOS .EXE executable files”  
Kenneth D. A. Pillay  
Microprocessors & Microsystems archive  
Volume 14, Issue 7 (September 1990)  
An example of a relocating loader for MS-DOS. Not the first one, but just a relatively modern example of how such a system works.

[SS74] “The Protection of Information in Computer Systems”  
J. Saltzer and M. Schroeder  
CACM, July 1974  
From this paper: “The concepts of base-and-bound register and hardware-interpreted descriptors appeared, apparently independently, between 1957 and 1959 on three projects with diverse goals. At M.I.T., McCarthy suggested the base-and-bound idea as part of the memory protection system necessary to make time-sharing feasible. IBM independently developed the base-and-bound register as a mechanism to permit reliable multiprogramming of the Stretch (7030) computer system. At Burroughs, R. Barton suggested that hardware-interpreted descriptors would provide direct support for the naming scope rules of higher level languages in the B5000 computer system.” We found this quote on Mark Smotherman’s cool history pages [S04]; see them for more information.

[S04] “System Call Support”  
Mark Smotherman, May 2004  
http://people.cs.clemson.edu/˜mark/syscall.html  
A neat history of system call support. Smotherman has also collected some early history on items like interrupts and other fun aspects of computing history. See his web pages for more details.

[WL+93] “Efficient Software-based Fault Isolation”  
Robert Wahbe, Steven Lucco, Thomas E. Anderson, Susan L. Graham  
SOSP ’93  
A terrific paper about how you can use compiler support to bound memory references from a program, without hardware support. The paper sparked renewed interest in software techniques for isolation of memory references.

[W17] Answer to footnote: “Is there anything other than havoc that can be wreaked?”  
Waciuma Wanjohi, October 2017  
Amazingly, this enterprising reader found the answer via google’s Ngram viewing tool (available at the following URL: http://books.google.com/ngrams). The answer, thanks to Mr. Wanjohi: “It’s only since about 1970 that ’wreak havoc’ has been more popular than ’wreak vengeance’. In the 1800s, the word wreak was almost always followed by ’his/their vengeance’.” Apparently, when you wreak, you are up to no good, but at least wreakers have some options now.  

\newpage

# 16 Segmentation
これまでは、各プロセスのアドレス空間全体をメモリに入れていました。ベースレジスタと境界レジスタを使用すると、OSは物理メモリの異なる部分にプロセスを簡単に再配置できます。しかし、あなたはこれらのアドレス空間について興味深いことに気づいたかもしれません。スタックとヒープの間であり中央に大きな空き領域があります。

![](../16/img/fig16_1.PNG)

図16.1から分かるように、スタックとヒープの間のスペースはプロセスによって使用されていませんが、物理メモリのどこかにアドレス空間全体を再配置すると、物理メモリを占有しています。したがって、メモリを仮想化するためにベースレジスタと境界レジスタのペアを使用する単純なアプローチは無駄です。また、アドレス空間全体がメモリに収まらないときは、プログラムを実行するのが非常に難しくなります。したがって、ベースと境界は、私たちが思っているほど柔軟ではありません。

>>THE CRUX: HOW TO SUPPORT A LARGE ADDRESS SPACE  
>>スタックとヒープの間に(潜在的に)空き領域が多い大きなアドレス空間をどのようにサポートしますか？我々の例では、小さな(ふりがな)アドレススペースで、無駄がそこまでないように見えることに注意してください。もし、32ビットのアドレス空間(4 GBのサイズ)であったらという状況を想像してみてください。一般的なプログラムはMBのメモリしか使用しませんが、アドレス空間全体がメモリに常駐することを要求するので大きな無駄になってしまいます。

# 16.1 Segmentation: Generalized Base/Bounds
この問題を解決するために、アイディアが生まれました。これをセグメンテーションと呼びます。少なくとも1960年代初頭と同じくらい古いものであることは、かなり古い考えです[H61、G62]。アイデアは簡単です。私たちのMMUにはベースと境界のペアが1つずつあるのではなく、アドレス空間の論理セグメントごとにベースと境界のペアがないのはなぜですか？セグメントは、特定の長さのアドレス空間のちょうど連続した部分であり、正規アドレス空間には、コード、スタック、およびヒープの3つの論理的に異なるセグメントがあります。セグメンテーションでは、物理メモリの異なる部分にこれらのセグメントを配置し、物理メモリに未使用の仮想アドレス空間を埋め込まないようにします。

![](../16/img/fig16_2.PNG)

例を見てみましょう。図16.1のアドレス空間を物理メモリに配置したいとします。セグメントごとにベースと境界のペアを使用して、各セグメントを物理メモリに独立して配置することができます。たとえば、図16.2を参照してください。そこには3つのセグメントがあります。64KBの物理メモリー(およびOS用に16KBが予約されています)が表示されます。

図でわかるように、使用されているメモリだけが物理メモリの領域を割り当てられているため、大量の未使用アドレス空間(疎なアドレス空間と呼ばれることもあります)を持つ大きなアドレス空間に対応できます。セグメンテーションをサポートするために必要な私たちのMMUのハードウェア構造は、すでに知っているものです。この場合は、3つのベースとバウンドのレジスタペアのセットです。下の図16.3に、上記の例のレジスタ値を示します。各境界レジスタはセグメントのサイズを保持します。

![](../16/img/fig16_3.PNG)

図から分かるように、コードセグメントは物理アドレス32KBに配置され、サイズは2KB、ヒープセグメントは34KBに配置され、サイズは2KBです。

図16.1のアドレス空間を使って変換例を見てみましょう。仮想アドレス100(コードセグメント内にある)への参照が行われたと仮定します。参照が行われると(命令フェッチなど)、ハードウェアはこのセグメント(この場合は100)のオフセットにベース値を加算して、100 + 32KBまたは32868の物理アドレスに到達します。アドレスが境界内にあることを確認し(100が2KB未満)、それが存在することを確認し、物理メモリアドレス32868への参照を発行します。

>>ASIDE: THE SEGMENTATION FAULT  
>>セグメンテーション違反または違反という用語は、セグメント化されたマシン上での不正なアドレスへのメモリアクセスによって発生します。ユーモラスな言い方をすれば、セグメンテーションをまったくサポートしていないマシンであっても、この言葉はそのままです。あなたのコードがフォールトを起こしている理由を理解できなければ、ユーモラスなこともありません。  
>>次に、ヒープ内のアドレス、仮想アドレス4200を見てみましょう(図16.1を参照)。最初に行うべきことは、ヒープへのオフセット、すなわちアドレスが参照するこのセグメントのどのバイトを抽出するかです。ヒープは仮想アドレス4KB(4096)から開始するので、4200のオフセットは実際に4200マイナス4096つまり104です。次に、このオフセット(104)を取得し、ベースレジスタの物理アドレス(34K)に追加して、望ましい結果を得ます：34920。  
>>ヒープの終わりを超えて7KBなどの違法アドレスを参照しようとするとどうなりますか？何が起こるか想像することができます。ハードウェアはアドレスが範囲外であることを検出し、OSにトラップし、問題のプロセスが終了する可能性があります。そして、すべてのCプログラマーが恐怖を覚える有名な用語の起源、つまりセグメンテーション違反を知りました。  

## 16.2 Which Segment Are We Referring To?
ハードウェアは、変換中にセグメントレジスタを使用します。セグメントへのオフセット、およびアドレスが参照するセグメントをどのように知っていますか？明示的アプローチと呼ばれることもある一般的なアプローチの1つは、アドレス空間を仮想アドレスの上位数ビットに基づいてセグメントに分割することです。この技法はVAX/VMSシステム[LL82]で使用されていました。上記の例では、3つのセグメントがあります。したがって、私たちの仕事を達成するには2ビットが必要です。14ビット仮想アドレスの上位2ビットを使用してセグメントを選択すると、仮想アドレスは次のようになります。

![](../16/img/fig16_2_1.PNG)

この例では、上位2ビットが00の場合、ハードウェアは仮想アドレスがコードセグメント内にあることを認識し、コードベースと境界のペアを使用してアドレスを正しい物理位置に再配置します。上位2ビットが01の場合、ハードウェアはアドレスがヒープにあることを認識し、ヒープのベースと境界を使用します。これを明確にするために、ヒープ仮想アドレスを上から(4200)取り出して変換してみましょう。バイナリ形式の仮想アドレス4200がここに表示されます。

![](../16/img/fig16_2_2.PNG)

画像からわかるように、上位2ビット(01)は、どのセグメントを参照しているかをハードウェアに伝えます。下位12ビットはセグメントへのオフセットです：0000 0110 1000、または16進数は0x068または104です。したがって、ハードウェアは、どのセグメントレジスタを使用するかを決定するために最初の2ビットを取り、次にセグメントへのオフセットとして次の12ビットを取ります。ベースレジスタをオフセットに加えることによって、ハードウェアは最終的な物理アドレスになります。オフセットは境界チェックを容易にします。オフセットが境界よりも小さいかどうかを簡単に確認できます。もしそうでなければ、アドレスは不正です。したがって、ベースと境界が配列(セグメントごとに1つのエントリ)であれば、ハードウェアは次のようにして目的の物理アドレスを取得します。

```c
// get top 2 bits of 14-bit VA
Segment = (VirtualAddress & SEG_MASK) >> SEG_SHIFT
// now get offset
Offset = VirtualAddress & OFFSET_MASK
if (Offset >= Bounds[Segment])
RaiseException(PROTECTION_FAULT)
else
PhysAddr = Base[Segment] + Offset
Register = AccessMemory(PhysAddr)
```
実行中の例では、上の定数の値を記入することができます。具体的には、SEG MASKは0x3000、SEG SHIFTは12、OFFSET MASKは0xFFFに設定されます。

また、上位2ビットを使用し、3つのセグメント(コード、ヒープ、スタック)しか持たない場合、アドレス空間の1つのセグメントが使用されなくなることに気付くかもしれません。したがって、一部のシステムでは、ヒープと同じセグメントにコードを配置し、したがって、使用するセグメントを選択するために1ビットのみを使用します[LL82]。

特定のアドレスがどのセグメントにあるかをハードウェアが判断する他の方法があります。暗黙的なアプローチでは、ハードウェアはアドレスがどのように形成されたかを知ることによってセグメントを決定します。例えば、アドレスがプログラムカウンタから生成された(すなわち命令フェッチであった)場合、アドレスはコードセグメント内にあります。アドレスがスタックポインタまたはベースポインタに基づいている場合は、スタックセグメント内になければなりません。他のアドレスはすべてヒープ内になければなりません。

## 16.3 What About The Stack?
ここまでは、アドレス空間の重要な要素の1つ、スタックを除外しました。スタックは上の図の物理アドレス28KBに再配置されていますが、重要な違いが1つあります。スタックは後退します。物理メモリでは、28KBから始まり、仮想アドレス16KBから14KBに対応する26KBにまで拡大します。変換は異なる方法で進めなければなりません。

![](../16/img/fig16_4.PNG)

まず必要なのは、ハードウェアの追加サポートです。ハードウェアは、ベースと境界の値の代わりに、セグメントがどのように成長するかを知る必要があります(たとえば、セグメントが正の方向に成長すると1に設定され、負の場合は0に設定されます)。ハードウェアが追跡している更新されたビューを図16.4に示します。

セグメントが負の方向に成長することをハードウェアが理解すると、ハードウェアはこのような仮想アドレスをわずかに異なる形で変換する必要があります。スタック仮想アドレスの例を取り上げ、それを翻訳してプロセスを理解してみましょう。

この例では、仮想アドレス15KBにアクセスすることを想定しています。物理アドレス27KBにマップする必要があります。私たちの仮想アドレスはバイナリ形式で、11 1100 0000 0000(hex 0x3C00)のようになります。ハードウェアは上位2ビット(11)を使用してセグメントを指定しますが、オフセットは3KBです。正しい負のオフセットを得るには、3KBから最大セグメントサイズを減算する必要があります。この例では、セグメントは4KBになりますので、正しい負のオフセットは3KBマイナス4KB -1KBです。正しい物理アドレス(27KB)に達するように、マイナスオフセット(-1KB)をベース(28KB)に追加するだけです。境界チェックは、負のオフセットの絶対値がセグメントのサイズより小さいことを確認することによって計算できます。

## 16.4 Support for Sharing
セグメンテーションのサポートが増えるにつれて、システム設計者はもう少しハードウェアをサポートして新しいタイプの効率を実現できることをすぐに認識しました。具体的には、メモリを節約するために、アドレス空間間で特定のメモリセグメントを共有すると便利なことがあります。特に、今日のシステムではコード共有が一般的であり、依然として使用されています。

共有をサポートするためには、保護ビットの形でハードウェアから少し余分なサポートが必要です。基本的なサポートは、セグメントごとに数ビットを追加し、プログラムがセグメントを読み書きできるかどうか、またはセグメント内にあるコードを実行するかどうかを示します。コードセグメントを読み取り専用に設定することにより、分離を害する心配することなく、同じコードを複数のプロセスにわたって共有することができます。各プロセスはそれ自身のプライベートメモリにアクセスしていると考えていますが、OSは秘密にプロセスによって変更できないメモリを共有しているので、錯覚は保持されます。

![](../16/img/fig16_5.PNG)

ハードウェア(およびOS)によって追跡される追加情報の例を図16.5に示します。ご覧のように、コードセグメントは読み込みと実行が設定されているため、メモリ内の同じ物理セグメントを複数の仮想アドレス空間にマップできます。

保護ビットを使用すると、前述のハードウェアアルゴリズムも変更する必要があります。仮想アドレスが境界内にあるかどうかを検査することに加えて、ハードウェアはまた、特定のアクセスが許可されるかどうかをチェックしなければいけません。ユーザープロセスが読み取り専用セグメントに書き込もうとするか、実行不可能なセグメントから実行しようとすると、ハードウェアは例外を発生させ、OSに問題のプロセスを処理させる必要があります。

## 16.5 Fine-grained vs. Coarse-grained Segmentation
これまでの例のほとんどは、ほんのわずかのセグメント(コード、スタック、ヒープ)を持つシステムに焦点を当てていました。このセグメンテーションは、アドレス空間を比較的大きく粗い塊に分割するので、粗い粒度と考えることができます。しかしながら、いくつかの初期のシステム(例えば、Multics [CV65、DD68])は、より柔軟であり、アドレス空間が細かいセグメント化と呼ばれる多数のより小さいセグメントから構成されることが可能でした。

多くのセグメントをサポートするには、ハードウェアのサポートをさらに必要とし、ある種のセグメントテーブルをメモリに格納します。このようなセグメントテーブルは、通常、非常に多数のセグメントの作成をサポートし、したがって、システムが、これまで説明したよりも柔軟性の高い方法でセグメントを使用できるようにします。たとえば、Burroughs B5000のような初期のマシンでは何千ものセグメントがサポートされていたため、コンパイラはコードとデータを別々のセグメントに分割してOSとハードウェアがサポートすることを期待していました[RK68]。当時の考えは、セグメントを細かく分割することによって、どのセグメントが使用されているか、どのセグメントが使用されていないかをより良く知ることができ、主メモリをより効果的に利用できるようになりました。

## 16.6 OS Support
セグメンテーションの仕組みについての基本的な考え方が必要です。システムが動作するにつれてアドレス空間の一部が物理メモリに再配置されるため、アドレス空間全体に対して単一のベース/境界ペアを使用した簡単なアプローチと比較して、物理メモリの大幅な節約が達成されます。具体的には、スタックとヒープの間の未使用領域をすべて物理メモリに割り当てる必要はなく、より多くのアドレス空間を物理メモリに収めることができます。

しかし、セグメンテーションはいくつかの新しい問題を引き起こします。最初に、対処しなければならない新しいOSの問題について説明します。今まで述べてきたものは古いものです。コンテキストスイッチでOSは何をすべきですか？それは、セグメントレジスタを保存して復元する必要があります。明らかに、各プロセスには独自の仮想アドレス空間があり、OSはプロセスを再実行する前にこれらのレジスタを正しく設定する必要があります。

第2の重要な問題は、物理メモリの空き領域を管理することです。新しいアドレス空間が作成されると、OSはそのセグメントの物理メモリ内の領域を見つけることができなければなりません。以前は、各アドレス空間が同じサイズであると仮定していたため、物理メモリはプロセスが収まるスロットの束と考えることができました。プロセスごとに多数のセグメントがあり、各セグメントは異なるサイズになっています。

一般的な問題は、物理メモリがすぐに空き領域の小さな穴で満たされ、新しいセグメントを割り当てることや既存のセグメントを拡張することが困難になることです。この問題を外部断片化と呼んでいます[R69]。図16.6を参照してください。

![](../16/img/fig16_6.PNG)

この例では、プロセスが20 KBのセグメントを割り当てようとしています。この例では、24KBの空きがありますが、1つの連続したセグメントではありません(むしろ3つの連続していないチャンク)。したがって、OSは20KB要求を満たすことができません。

この問題に対する1つの解決策は、既存のセグメントを再配置することによって物理メモリをコンパクト化することです。例えば、OSは実行中のプロセスを停止し、それらのデータを1つの連続したメモリ領域にコピーし、それらのセグメントレジスタ値を新しい物理位置を指すように変更することができ、したがって、これにより、OSは新しい割り当て要求を成功させることができます。しかし、コピー・セグメントはメモリーを消費し、一般的にかなりの量のプロセッサー時間を使用するため、圧縮はコストがかかります。圧縮された物理メモリの図については、図16.6(右)を参照してください。

よりシンプルな方法は、割り当てのために利用可能な大量のメモリを保持しようとするフリーリスト管理アルゴリズムを使用することです。ベストフィット(空きスペースのリストを保持し、リクエスタへの望ましい割り当てを満たすサイズに最も近いものを返す)、ワーストフィット、ファーストフィットなどの古典的なアルゴリズムを含む、人々が取ったアプローチは文字通り何百もあります。バディアルゴリズム[K68]のようなより複雑なスキームもあります。

Wilsonらによる優れた調査によると、このようなアルゴリズム[W+95]の詳細を知りたい場合や、後の章で基本のいくつかをカバーするまでお待ちください。残念なことに、たとえどんなにスマートなアルゴリズムであっても、外部の断片化は依然として存在します。従って、良いアルゴリズムはそれを最小化しようとするだけです。

>> TIP: IF 1000 SOLUTIONS EXIST, NO GREAT ONE DOES  
>> 外部の断片化を最小限に抑えるために非常に多くの異なるアルゴリズムが存在するという事実は、より根底にある真実を示唆しています。問題を解決する最良の方法はありません。したがって、我々は合理的な何かのために解決し、十分に良くなることを願っています。唯一の実際の解決策(次の章で説明します)は、可変サイズのチャンクにメモリを決して割り当てないことによって、問題を完全に回避することです。

## 16.7 Summary
セグメンテーションは多くの問題を解決し、メモリのより効果的な仮想化を構築するのに役立ちます。ダイナミックリロケーションだけでなく、セグメント化は、アドレス空間の論理セグメント間の巨大な潜在的なメモリの浪費を避けることによって、無駄にあいているアドレス空間をよりよくサポートすることができます。

算術セグメント化が容易で、ハードウェアにも適しているため、高速です。変換のオーバーヘッドは最小限に抑えられます。フリンジの利益も発生します。それはコード共有です。コードが別のセグメント内に配置されている場合、そのセグメントは実行中の複数のプログラム間で共有される可能性があります。

しかし、私たちが学んだように、可変サイズのセグメントをメモリに割り当てることは、克服したいいくつかの問題につながります。最初に述べたように、外部の断片化です。セグメントは変数化されているため、空きメモリが奇数の部分に細分化されるため、メモリ割り当て要求を満たすことは困難です。スマートアルゴリズム[W+95]を使用することも、周期的にコンパクトなメモリを使用することもできますが、この問題は基本的で避けがたいものです。

第2の、そしておそらくより重要な問題は、セグメンテーションが、完全に一般化された無駄にあいているアドレス空間をサポートするのに十分柔軟でないことです。たとえば、1つの論理セグメント内に大規模だが、まばらに使用されるヒープがある場合、ヒープ全体がアクセスされるためにはまだメモリに常駐する必要があります。言い換えれば、アドレス空間がどのように使用されているかのモデルが、基礎となるセグメンテーションの設計方法と正確に一致しない場合、セグメンテーションはうまく機能しません。したがって、新しいソリューションを見つける必要があります。それらを見つける準備ができていますか？

# 参考文献

[CV65] “Introduction and Overview of the Multics System”  
F. J. Corbato and V. A. Vyssotsky  
Fall Joint Computer Conference, 1965  
One of five papers presented on Multics at the Fall Joint Computer Conference; oh to be a fly on the wall in that room that day!

[DD68] “Virtual Memory, Processes, and Sharing in Multics”  
Robert C. Daley and Jack B. Dennis  
Communications of the ACM, Volume 11, Issue 5, May 1968  
An early paper on how to perform dynamic linking in Multics, which was way ahead of its time. Dynamic linking finally found its way back into systems about 20 years later, as the large X-windows libraries demanded it. Some say that these large X11 libraries were MIT’s revenge for removing support for dynamic linking in early versions of UNIX!

[G62] “Fact Segmentation”  
M. N. Greenfield  
Proceedings of the SJCC, Volume 21, May 1962  
Another early paper on segmentation; so early that it has no references to other work.

[H61] “Program Organization and Record Keeping for Dynamic Storage”  
A. W. Holt  
Communications of the ACM, Volume 4, Issue 10, October 1961  
An incredibly early and difficult to read paper about segmentation and some of its uses.

[I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals”  
Intel, 2009  
Available: http://www.intel.com/products/processor/manuals  
Try reading about segmentation in here (Chapter 3 in Volume 3a); it’ll hurt your head, at least a little bit.

[K68] “The Art of Computer Programming: Volume I”  
Donald Knuth  
Addison-Wesley, 1968  
Knuth is famous not only for his early books on the Art of Computer Programming but for his typesetting system TeX which is still a powerhouse typesetting tool used by professionals today, and indeed to typeset this very book. His tomes on algorithms are a great early reference to many of the algorithms that underly computing systems today.

[L83] “Hints for Computer Systems Design”  
Butler Lampson  
ACM Operating Systems Review, 15:5, October 1983  
A treasure-trove of sage advice on how to build systems. Hard to read in one sitting; take it in a little at a time, like a fine wine, or a reference manual.

[LL82] “Virtual Memory Management in the VAX/VMS Operating System”  
Henry M. Levy and Peter H. Lipman  
IEEE Computer, Volume 15, Number 3 (March 1982)  
A classic memory management system, with lots of common sense in its design. We’ll study it in more detail in a later chapter.

[RK68] “Dynamic Storage Allocation Systems”  
B. Randell and C.J. Kuehner  
Communications of the ACM  
Volume 11(5), pages 297-306, May 1968  
A nice overview of the differences between paging and segmentation, with some historical discussion of various machines.

[R69] “A note on storage fragmentation and program segmentation”  
Brian Randell  
Communications of the ACM  
Volume 12(7), pages 365-372, July 1969  
One of the earliest papers to discuss fragmentation.

[W+95] “Dynamic Storage Allocation: A Survey and Critical Review”  
Paul R. Wilson, Mark S. Johnstone, Michael Neely, and David Boles  
In International Workshop on Memory Management  
Scotland, United Kingdom, September 1995  
A great survey paper on memory allocators.

\newpage

## 17 Free-Space Management
この章では、mallocライブラリ(プロセスのヒープのページを管理する)であろうと、OS自体(アドレスの一部を管理するものであろうと)を問わず、メモリ管理システムの基本的な側面について議論するために、プロセスのスペース)。具体的には、free space managementを取り巻く課題について議論していきます。

問題をより具体的にしましょう。空き領域を管理することは確実に簡単です。ページングの概念について議論するときにわかります。管理しているスペースが固定サイズのユニットに分割されている場合は簡単です。このような場合、これらの固定サイズのユニットのリストを保持するだけです。クライアントがそのうちの1つを要求すると、最初のエントリを返します。

free space managementがより難しくなる(興味深い)のは、管理している空き領域が可変サイズのユニットで構成されている場合です。これは、ユーザレベルのメモリ割り当てライブラリ(`malloc()`および`free()`)や、セグメンテーションを使用して仮想メモリを実装する際に物理メモリを管理するOSで発生します。どちらの場合でも、存在する問題は外部断片化として知られています。空き領域はサイズの小さな断片に細断され、断片化されます。空き領域の合計量が要求のサイズを超えていても、要求を満たすことができる連続した1つの領域がないため、後続の要求が失敗する可能性があります。

![](../17/img/fig17_1_1.PNG)

図はこの問題の例を示しています。この場合、使用可能な空き領域の合計は20バイトです。残念ながら、サイズ10の2つのチャンクに分割されています。その結果、20バイトの空きがあっても15バイトの要求は失敗します。そこで、この章で取り上げる問題に到達します。

>>CRUX: HOW TO MANAGE FREE SPACE  
>>可変サイズの要求を満たす場合、空き領域をどのように管理するべきか？断片化を最小限に抑えるためにどのような戦略を使用できますか？代替アプローチの時間と空間のオーバーヘッドはどのくらいですか？

## 17.1 Assumptions
この議論の大部分は、ユーザレベルのメモリ割り当てライブラリにあるアロケータの偉大な歴史に焦点を当てています。私たちはウィルソンの優れた調査[W+95]を参考にしていますが、興味を持った読者がソースドキュメント自体に行くことを勧めます。

`malloc()`と`free()`が提供するような基本的なインタフェースを想定しています。具体的には、void * malloc(size t size)は、アプリケーションによって要求されたバイト数であるsizeという単一のパラメータをとります。そのサイズの領域(またはそれ以上の大きさ)にポインタ(特定の型のないポインタ、またはC言語のvoidポインタ)を返します。void free(void * ptr)はポインタをとり、対応するチャンクを解放します。インターフェイスの意味に注意してください。ユーザーはスペースを解放するときにライブラリにサイズを通知しません。したがって、ライブラリは、メモリへのポインタが渡されたときに、メモリのチャンクがどれだけ大きいかを把握できる必要があります。この章の後半でこれを行う方法について説明します。

このライブラリが管理する領域は歴史的にヒープとして知られており、ヒープの空き領域を管理するために使用される汎用データ構造は何らかのフリーリストです。この構造体には、メモリの管理対象領域内の空きチャンクのすべてへの参照が含まれます。もちろん、このデータ構造はリストそのものでなくても、空き領域を追跡するためのデータ構造の一種である必要はありません。

さらに、主に、前述のように外部断片化に関心があると仮定します。アロケータは、もちろん、内部断片化の問題を抱えている可能性もあります。割り振り者が要求された量よりも多くのメモリを渡すと、そのようなチャンク内の未使用の(したがって未使用の)スペースは内部断片化とみなされます(割り当てられたユニットの内部で発生するため)。しかし、単純化のため、主に外部フラグメンテーションに焦点を当てます。

また、クライアントにメモリを渡すと、メモリ内の別の場所に移動することはできません。たとえば、プログラムが`malloc()`を呼び出し、ヒープ内のいくつかの領域へのポインタが与えられた場合、そのメモリ領域は、プログラムが対応するメモリ領域を介して返すまで、プログラムによって"所有"されます(ライブラリによっては移動できません)`free()`を呼び出します。したがって、断片化に対抗するのに有用な空き領域の圧縮は不可能です。しかし、セグメンテーションを実装する際に、断片化に対処するために、空き領域の圧縮をOSで使用することができます(セグメンテーションの章で説明したように)。

最後に、アロケータが連続したバイトの領域を管理していると仮定します。場合によっては、アロケータがその領域の拡張を要求することもできます。例えば、ユーザレベルのメモリ割り当てライブラリは、スペースを使い果たしたときにカーネルを呼び出して(sbrkなどのシステムコールを介して)ヒープを拡張することができます。しかし、わかりやすくするために、領域は一生を通して固定された単一のサイズであると仮定します。

## 17.2 Low-level Mechanisms
いくつかのポリシーの詳細を調べる前に、まず大部分のアロケータで使用されるいくつかの一般的なメカニズムについて説明します。まず、ほとんどのアロケータで一般的な手法である分割と融合の基本について説明します。次に、割り当てられた領域のサイズをどのように素早く簡単に追跡できるかを示します。最後に、空き領域内に単純なリストを作成し、空き領域と空でない領域を追跡する方法について説明します。

###Splitting and Coalescing 
空きリストには、まだヒープに残っている空き領域を記述する要素のセットが含まれています。したがって、次の30バイトのヒープを想定します。

![](../17/img/fig17_1_2.PNG)

このヒープの空きリストには2つの要素があります。1つのエントリは最初の10バイトの空きセグメント(バイト0〜9)を記述し、1つのエントリは他の空きセグメント(バイト20〜29)を記述します。

![](../17/img/fig17_1_3.PNG)

上記のように、10バイトを超えるリクエストは失敗します(NULLを返します)。利用可能なサイズのメモリが連続して1つだけはありません。いずれかのフリーチャンクによって、そのサイズ(10バイト)を正確に満たすことができます。しかし、要求が10バイトより小さいものの場合はどうなりますか？

ただ1バイトのメモリが要求されているとします。この場合、アロケータは分割と呼ばれるアクションを実行します。要求を満たすことができ、2つに分割する空きメモリを見つけます。

最初のチャンクは呼び出し元に戻ります。2番目のチャンクはリストに残ります。したがって、上記の例では、1バイトの要求が行われ、アロケータが2つの要素のうち2番目の要素を使用して要求を満たすことにした場合、`malloc()`の呼び出しは20を返します1バイトの割り当て領域)、リストは次のようになります。

![](../17/img/fig17_1_4.PNG)

上記のように、10バイトを超えるリクエストは失敗します(NULLを返します)。利用可能なサイズのメモリが連続して1つだけはありません。いずれかのフリーチャンクによって、そのサイズ(10バイト)を正確に満たすことができます。しかし、要求が10バイトより小さいものの場合はどうなりますか？

ただ1バイトのメモリが要求されているとします。この場合、アロケータは分割と呼ばれるアクションを実行します。要求を満たすことができ、2つに分割する空きメモリを見つけます。

最初のチャンクは呼び出し元に戻ります。2番目のチャンクはリストに残ります。したがって、上記の例では、1バイトの要求が行われ、アロケータが2つの要素のうち2番目の要素を使用して要求を満たすことにした場合、`malloc()`の呼び出しは20を返します。1バイトの割り当て領域)、リストは次のようになります。

![](../17/img/fig17_1_5.PNG)

この問題に注意してください。ヒープ全体が空いている間は、見かけ上10バイトの3つのチャンクに分割されています。したがって、ユーザーが20バイトを要求した場合、単純なリストの探索ではそのような空きチャンクを見つけず、失敗を返します。

この問題を回避するためにアロケータが行うことは、メモリのチャンクが解放されたときの空き領域の合体です。アイデアは簡単です：メモリに空きチャンクを返すときは、空き領域の近くのチャンクだけでなく、返すチャンクのアドレスも注意深く見てください。新しく作成されたスペースが既存の空きチャンクの1つ(またはこの例では2つ)のすぐ隣にある場合は、それらを1つの大きな空きチャンクにマージします。したがって、合体によって、最終的なリストは次のようになります。

![](../17/img/fig17_1_6.PNG)

確かに、これは割り当てが行われる前のヒープリストの最初のようなものです。合体により、アロケータは、アプリケーションに大きな空き領域があることを確認できます。

### Tracking The Size Of Allocated Regions
free(void * ptr)のインタフェースがsizeパラメータを取っていないことに気づいたかもしれません。mallocライブラリは解放されたメモリ領域のサイズを素早く決定し、空きリストにスペースを組み込むことができると仮定されています。

[](../17/img/fig17_1.PNG)

このタスクを達成するために、大部分のアロケータは、メモリ内に保持されたヘッダブロックに、通常はメモリのチャンクの直前に少しの余分な情報を格納します。もう一度例を見てみましょう(図17.1)。この例では、ptrが指す、20バイトの割り当てブロックを調べています。ユーザが`malloc()`と呼ばれ、その結果をptrに格納すると想像してください。たとえば、ptr = malloc(20);となります。

ヘッダーには、割り当てられた領域のサイズが最小限含まれます。(この場合は20)それは、割り当て解除を高速化するための追加ポインタ、追加の完全性チェックを提供するためのマジックナンバー、および他の情報を含むこともできます。領域のサイズとマジックナンバーを含む単純なヘッダを仮定しましょう。
```c
typedef struct __header_t {
int size;
int magic;
} header_t;
```

![](../17/img/fig17_2.PNG)

上記の例は、図17.2のようになります。ユーザーがfree(ptr)を呼び出すと、ライブラリは単純なポインタ演算を使用して、ヘッダーの開始位置を特定します。

```c
void free(void *ptr) {
header_t *hptr = (void *)ptr - sizeof(header_t);
...
```
このようなヘッダへのポインタを取得した後、ライブラリはマジックナンバーがサニティチェック(assert(hptr-> magic == 1234567))として期待値と一致するかどうかを簡単に判断し、新たに解放された領域の合計サイズを単純な数学(すなわち、領域のサイズにヘッダのサイズを加える)。最後の文では小さいながらも重要なことに注意してください。空き領域のサイズは、ヘッダーのサイズにユーザーに割り当てられたスペースのサイズを加えたものです。したがって、ユーザーがNバイトのメモリーを要求すると、ライブラリーはサイズNの空きチャンクを検索しません。むしろ、サイズNのフリーチャンクとヘッダーのサイズを検索します。

### Embedding A Free List
これまでのところ、私たちは単純なフリーリストを概念実体として扱ってきました。それはヒープ内のメモリの空きチャンクを記述する単なるリストです。しかし、空き領域の中にこのようなリストをどうやって作りますか？

より典型的なリストでは、新しいノードを割り当てるときに、ノードのためのスペースが必要なときに`malloc()`を呼び出すだけです。残念ながら、メモリ割り当てライブラリ内では、これを行うことはできません。代わりに、空き領域内にリストを構築する必要があります。

4096バイトのメモリを管理する(つまり、ヒープは4KB)と仮定します。これをフリーリストとして管理するには、最初に前記リストを初期化する必要があります。最初は、リストにはサイズ4096(ヘッダサイズを引いたもの)のエントリが1つあります。リストのノードの説明は次のとおりです。

```c
typedef struct __node_t {
int size;
struct __node_t *next;
} node_t;
```
次に、ヒープを初期化し、空きリストの最初の要素をそのスペース内に配置するコードを見てみましょう。ヒープは、システムコールmmapの呼び出しによって取得された空き領域内に構築されていると仮定しています。このようなヒープを作成する唯一の方法ではありませんが、この例ではうまく機能します。
```c
// mmap() returns a pointer to a chunk of free space
node_t *head = mmap(NULL, 4096, PROT_READ|PROT_WRITE,
MAP_ANON|MAP_PRIVATE, -1, 0);
head->size = 4096 - sizeof(node_t);
head->next = NULL;
```
このコードを実行すると、リストのステータスは、サイズが4088の単一のエントリを持つことになります。はい、これは小さなヒープですが、ここではわかりやすい例です。ヘッドポインタはこの範囲の先頭アドレスを含みます。それが16KBであると仮定しましょう(仮想アドレスは問題ありません)。視覚的には、ヒープは図17.3のようになります。

![](../17/img/fig17_3.PNG)

さて、メモリのチャンクが要求されたとしましょう。たとえば、100バイトのサイズです。この要求を処理するために、ライブラリは最初に要求を収容するのに十分な大きさを見つけます。空きチャンク(サイズ：4088)が1つしかないため、このチャンクが選択されます。次に、チャンクは2つに分割されます。1つのチャンクは要求(および前述のヘッダー)を処理するのに十分な大きさで、空きチャンクは残りのチャンクです。8バイトのヘッダ(整数サイズと整数のマジックナンバー)を仮定すると、ヒープ内のスペースは図17.4のようになります。

![](../17/img/fig17_4.PNG)

したがって、100バイトの要求時に、ライブラリーは既存の1つの空きチャンクから108バイトを割り当て、(上の図でptrとマークされた)ポインターを戻し、割り当てられたスペースの直前のヘッダー情報を後で使用できるように`free()`を呼び出し、リスト内の1つの空きノードを3980バイト(4088から108)に縮小します。

次に、割り当てられた3つの領域がある場合のヒープを見てみましょう。各領域は100バイト(またはヘッダーを含む108)です。このヒープの可視化を図17.5に示します。

![](../17/img/fig17_5.PNG)

そこから見ることができるように、最初のヒープの324バイトが割り当てられているので、その領域に3つのヘッダーがあり、呼び出し元のプログラムで3つの100バイトの領域が使用されています。フリーリストは興味深いもので、ただ1つのノード(頭が指している)ですが、今は3つの分割後のサイズが3764バイトです。しかし、呼び出し元のプログラムが`free()`でメモリを返すとどうなりますか？

この例では、アプリケーションはfree(16500)を呼び出すことによって割り当てられたメモリの中央のチャンクを返します(値16500は、メモリ領域の先頭16384を前のチャンクの108に追加することによって到着します。このチャンクのヘッダー)この値は前の図にポインタsptrで示されています。

![](../17/img/fig17_6.PNG)

ライブラリは空き領域のサイズをすぐに把握し、空きチャンクを空きリストに戻します。空きリストの先頭に挿入すると、スペースは次のようになります(図17.6)。そして今、私たちは小さなフリーチャンク(100バイト、リストの先頭を指す)と大きな空きチャンク(3764バイト)で始まるリストを持っています。

私たちのリストには、最終的に1つ以上の要素があります！そして、はい、free spaceは断片化していますが、残念ながら一般的なことです。

![](../17/img/fig17_7.PNG)

1つの最後の例：最後の2つの使用中チャンクが解放されたと仮定しましょう。合体しないと、断片化しているフリーリストになる可能性があります(図17.7参照)

図からわかるように、今は大きな混乱があります！どうしてでしょう？シンプルなので、リストをまとめるのを忘れてしまいました。メモリはすべてfreeされていますが、断片化したメモリとして見えます。解決策は簡単です：リストを通って隣接チャンクをマージします。終了すると、ヒープは再び全体になります。

### Growing The Heap
私たちは、多くの割り当てライブラリ内で見つかった1つの最後のメカニズムについて議論すべきです。具体的には、ヒープにスペースがなくなったらどうしたらいいですか？最も簡単なアプローチは失敗することです。場合によってはこれが唯一のオプションであるため、NULLを返すことは名誉あるアプローチです。

従来のほとんどのアロケータは、小さなヒープから始めて、ヒープがなくなったときに、OSからより多くのメモリを要求します。通常、これはヒープを成長させてそこから新しいチャンクを割り当てるために何らかの種類のシステムコール(例えば、ほとんどのUNIXシステムではsbrk)を行うことを意味します。sbrk要求を処理するために、OSは空きの物理ページを見つけ、要求元のプロセスのアドレス空間にマップしてから、新しいヒープの終わりの値を返します。その時点で、より大きいヒープが利用可能であり、要求を正常に処理することができます。

## 17.3 Basic Strategies
今ではいくつかの機械を手に入れましたので、空き領域を管理するための基本的な戦略について説明しましょう。これらのアプローチは、ほとんどあなたが自分自身を考えることができる非常にシンプルなポリシーに基づいています。

理想的なアロケータは高速であり、断片化を最小限に抑えます。残念なことに、割り振りと空き要求の流れは任意です(結局、プログラマーによって決定されます)。間違った入力を考えると、どのような戦略もかなり悪いことがあります。したがって、私たちは「最良の」アプローチについては説明しませんが、むしろいくつかの基本について話し、賛否両論について議論します。

### Best Fit
ベストフィット戦略は非常に単純です。まず、フリーリストを検索し、要求されたサイズよりも大きいか大きいサイズの空きメモリを探します。そのグループの中で最も小さいグループを返します。これはいわゆるベストフィットチャンクです(最小適合とも呼ぶことができます)。空きリストを1回通過するだけで正しいブロックが返されます。

ベストフィットの背後にある直感は簡単です。ユーザーが求めるものに近いブロックを返すことで、ベストフィットは無駄なスペースを減らそうとします。しかし、コストがあります。素朴な実装では、正しい空きブロックの網羅的な検索を実行すると、パフォーマンスが大幅に低下します。

### Worst Fit
Worst Fitアプローチはベストフィットの逆です。最大のチャンクを見つけて要求された量を返します。残りの(大)チャンクを空きリストに残しておきます。Worst Fitは、ベストフィット手法から生じる可能性のある小さなチャンクの代わりに、大きなチャンクを自由に残そうとします。しかし、もう一度、空き領域を完全に検索する必要があり、この方法はコストがかかる可能性があります。さらに悪いことに、ほとんどの調査ではパフォーマンスが悪く、余分な断片化が発生していますが、依然として高いオーバーヘッドが発生しています。

### First Fit
First Fit方法は、単に十分に大きい最初のブロックを見つけ、要求された量をユーザーに返します。前と同様に、残りの空き領域は、後続の要求で空きにされます。

First Fitはスピードの長所を持っています。空き領域を網羅的に検索する必要はありませんが、小さなオブジェクトで空きリストの先頭を汚染することがあります。したがって、アロケータが空きリストの順序をどのように管理するかが問題になります。1つのアプローチは、アドレスベースの順序付けを使用することです。リストを空き領域のアドレス順に並べることにより、合体が容易になり、断片化が減少する傾向があります。

### Next Fit
リストの始めに最初に適合する検索を常に開始するのではなく、次の適合アルゴリズムは、リスト内で最後に探していた場所への余分なポインタを保持します。このアイデアは、リスト全体に空きスペースの検索をより均一に広げることで、リストの先頭が崩れないようにすることです。このようなアプローチのパフォーマンスは、完全な検索が再び回避されるため、first fitと非常に似ています。

### Examples
上記の戦略のいくつかの例を示します。サイズが10,30、および20の3つの要素を持つフリーリストを想像してください。(ヘッダーやその他の詳細は無視されます)

![](../17/img/fig17_7_1.PNG)

ベストフィット手法はリスト全体を検索し、要求に対応できる最小の空き領域であるため、20が最適であることがわかります。

![](../17/img/fig17_7_2.PNG)

この例のように、最もよく合ったアプローチでよく起こりますが、小さなfreeチャンクが残ります。worst fitアプローチも同様ですが、この例では最大のチャンクが見つかります。

![](../17/img/fig17_7_3.PNG)

ファーストフィット戦略は、この例では、worst fitと同じことを行い、要求を満たすことができる最初のフリーブロックを見つけます。違いは検索コストです。リスト全体の見方がベストフィットとワーストフィットは違います。ファーストフィットは、適合するものが見つかるまでフリーチャンクを調べるだけで検索コストを削減します。

これらの例は、割り当て方針の表面を触っているだけです。より深い理解のためには、実際の仕事量とより複雑なアロケータの振る舞い(例えば、合体)によるより詳細な分析が必要です。

## 17.4 Other Approaches
上記の基本的なアプローチ以外にも、何らかの方法でメモリ割り当てを改善するための多くの提案されたテクニックとアルゴリズムがあります。

### Segregated Lists
1つの興味深いアプローチは、分離されたリストの使用です。基本的な考え方は単純です。特定のアプリケーションが一般的なサイズの要求を1つ(またはいくつか)持っている場合は、そのサイズのオブジェクトを管理するためだけに別のリストを保持します。他のすべての要求は、一般的なメモリアロケータに転送されます。

そのようなアプローチの利点は明らかです。1つの特定のサイズの要求専用のメモリチャンクを持つことで、断片化の懸念が大幅に減ります。さらに、リストの複雑な検索が不要であるため、割り振り要求および空き要求は、適切なサイズのものであれば、すばやく処理できます。

ちょうどいいアイデアのように、このアプローチはシステムにも新しい複雑さをもたらします。たとえば、一般プールとは異なり、特定のサイズの特別なリクエストを処理するメモリプールにどれだけのメモリを割り当てる必要がありますか？1つの特定のアロケータである、Uber engineerのJeff Bonwick(Solarisカーネル用に設計された)のスラブアロケータは、この問題をかなりうまく処理します[B94]。

具体的には、カーネルが起動すると、頻繁に要求される可能性が高いカーネルオブジェクト(ロック、ファイルシステムのinodeなど)に多数のオブジェクトキャッシュを割り当てます。このようにして、オブジェクトキャッシュは、与えるのサイズの空きリストをそれぞれ分離し、メモリ割り当ておよび空き要求を迅速に提供します。与えられたキャッシュが空き容量が少なくなったときには、より一般的なメモリアロケータから要求された量のスラブ(要求された合計量がページサイズの倍数と問題のオブジェクトです)を要求します。逆に、与えられたのスラブ内のオブジェクトの参照カウントがすべてゼロになると、汎用アロケータは、VMシステムがより多くのメモリを必要とするときによく行われる特別なアロケータからメモリを再要求することができます。

>> ASIDE: GREAT ENGINEERS ARE REALLY GREAT  
>> Jeff Bonwick(ここで言及したスラブアロケータを書いただけでなく、すばらしいファイルシステムの先駆けでもあったZFS)のようなエンジニアは、シリコンバレーの中心です。ほぼすべての偉大な製品や技術の背後には、才能、能力、献身が平均以上の人間(または少人数のグループ)があります。Mark Zuckerberg(Facebookの)は次のように述べています。「自分の役割において例外的な人は、少しだけ良い人ではありません。かなり良い人です。彼らは100倍も優れています。」これは、今日でも、世界の顔を永遠に変える会社(Google、Apple、Facebook)を1〜2人が立ち上げることができる理由です。懸命に働いて、あなたはそのような"100x"人になるかもしれません。それに失敗するかもしれません。しかし、そのような人と働くとあなたは1ヶ月以内に多くを学ぶよりも、1日で多くを学ぶでしょう。

スラブアロケータはまた、リスト上の空きオブジェクトを事前初期化状態に保つことによって、ほとんどの分離リスト手法を超えています。Bonwickは、データ構造の初期化と破壊はコストがかかることを示しています[B94]。しかし、スラブアロケータは、解放されたオブジェクトを初期化された状態の特定のリストに保持することにより、オブジェクトごとの頻繁な初期化および破棄サイクルを回避し、オーバーヘッドを顕著に低減することができます。

### Buddy Allocation
合体はアロケータにとって重要なので、いくつかのアプローチは合体を簡単にすることを目的として設計されています。1つの良い例がバイナリバディアロケータ[K65]にあります。

このようなシステムでは、空きメモリは、概念的には、サイズ2Nの1つの大きな空間と考えます。メモリの要求が行われると、空き領域の探索は、要求を収容するのに十分な大きさのブロックが見つかるまで、空き領域を2つに再帰的に分割します(さらに2つに分割すると、スペースが小さくなります)。この時点で、要求されたブロックがユーザーに返されます。ここでは、7KBブロックの検索で64KBの空き領域を分けた例を示します。

![](../17/img/fig17_7_4.PNG)

この例では、左端の8KBブロックが割り当てられ(暗い影の灰色で示されるように)、ユーザーに返されます。このスキームでは、2つのサイズのブロックを出力することだけが許可されているため、内部の断片化に悩まされる可能性があることに注意してください。

バディ割り当ての美しさは、そのブロックが解放されたときに何が起こるかで見られます。8KBのブロックを空きリストに戻すとき、アロケータは「バディ」8KBが空いているかどうかをチェックします。そうであれば、2つのブロックを合併して16KBのブロックにする。次に、アロケータは、16KBブロックのバディがまだフリーであるかどうかをチェックします。そうであれば、それらの2つのブロックを結合する。この再帰的な結合プロセスは、ツリー全体を再構築し、空き領域全体を復元するか、バディが使用中であることが判明したときに停止します。

バディ割り当てがうまく機能する理由は、特定のブロックのバディを決定するのが簡単だからです。どうやって調べるでしょうか？上記の空き領域にあるブロックのアドレスについて考えてみましょう。よく注意深く考えてみると、各バディーペアのアドレスは1ビットだけ異なることがわかります。このビットはバディツリーのレベルによって決まります。したがって、バイナリバディ割り当て方式がどのように機能するかという基本的な考え方があります。詳細については、いつものように、Wilson調査[W+95]を参照してください。

### Other Ideas
上記の多くのアプローチの1つの大きな問題は、スケーリングの欠如です。具体的には、リストの検索は非常に遅くなる可能性があります。したがって、高度なアロケータは、これらのコストに対処するために、より複雑なデータ構造を使用し、パフォーマンスを単純化します。例としては、バランスのとれたバイナリツリー、スプレイツリー、または部分的に順序付けられたツリー[W+95]があります。

現代のシステムでは複数のプロセッサがあり、マルチスレッドの仕事量を実行することが多いため(詳細については、同時実行に関する本のセクションで詳しく説明します)、マルチプロセッサベースのシステムでアロケータをうまく動作させるために多くの努力が費やされたことは驚くことではありません。2つの素晴らしい例がBerger et al[B+00]とエバンス[E06]。詳細を確認してください。

これらは、メモリアロケータについて人々が時間をかけて持っている何千ものアイデアのうちの2つに過ぎません。好奇心が強い場合は、あなた自身で読んでください。それに失敗した場合、glibcアロケータがどのように動作するかを読んで[S15]、実際の世界がどのようなものかを理解してください。

## 17.5 Summary
この章では、最も基本的な形のメモリアロケータについて説明しました。そのようなアロケータはどこにでも存在し、あなたが書いたすべてのCプログラムにリンクされているだけでなく、独自のデータ構造のためにメモリを管理している基礎となるOSにも存在します。多くのシステムと同様に、そのようなシステムを構築するには多くのトレードオフがあります。アロケータに与えられる正確な仕事量について知るほど、その仕事量に対してよりうまく動作するよう調整することができます。広範囲の仕事量でうまく動作する、空間効率の良い高速でスケーラブルなアロケータを作成することは、現代のコンピュータシステムにおいて進行中の課題です。

# 参考文献

[B+00] “Hoard: A Scalable Memory Allocator for Multithreaded Applications”  
Emery D. Berger, Kathryn S. McKinley, Robert D. Blumofe, and Paul R. Wilson  
ASPLOS-IX, November 2000  
Berger and company’s excellent allocator for multiprocessor systems. Beyond just being a fun paper, also used in practice!

[B94] “The Slab Allocator: An Object-Caching Kernel Memory Allocator”  
Jeff Bonwick  
USENIX ’94  
A cool paper about how to build an allocator for an operating system kernel, and a great example of how to specialize for particular common object sizes.

[E06] “A Scalable Concurrent malloc(3) Implementation for FreeBSD”  
Jason Evans  
http://people.freebsd.org/˜jasone/jemalloc/bsdcan2006/jemalloc.pdf  
April 2006  
A detailed look at how to build a real modern allocator for use in multiprocessors. The “jemalloc” allocator is in widespread use today, within FreeBSD, NetBSD, Mozilla Firefox, and within Facebook.

[K65] “A Fast Storage Allocator”  
Kenneth C. Knowlton  
Communications of the ACM, Volume 8, Number 10, October 1965  
The common reference for buddy allocation. Random strange fact: Knuth gives credit for the idea not to Knowlton but to Harry Markowitz, a Nobel-prize winning economist. Another strange fact: Knuth communicates all of his emails via a secretary; he doesn’t send email himself, rather he tells his secretary what email to send and then the secretary does the work of emailing. Last Knuth fact: he created TeX, the tool used to typeset this book. It is an amazing piece of software.

[S15] “Understanding glibc malloc”  
Sploitfun  
February, 2015  
https://sploitfun.wordpress.com/2015/02/10/understanding-glibc-malloc/  
A deep dive into how glibc malloc works. Amazingly detailed and a very cool read.  

[W+95] “Dynamic Storage Allocation: A Survey and Critical Review”  
Paul R. Wilson, Mark S. Johnstone, Michael Neely, David Boles  
International Workshop on Memory Management  
Kinross, Scotland, September 1995  
An excellent and far-reaching survey of many facets of memory allocation. Far too much detail to go into in this tiny chapter!

\newpage

# 18 Paging: Introduction
スペース管理問題を解決するには、オペレーティングシステムが2つのアプローチのうちの1つをとると言われることがあります。第1のアプローチは、仮想メモリのセグメンテーションで見たように、可変サイズの断片に分割することです。残念なことに、この解決策には固有の難点があります。特に、スペースを異なるサイズのチャンクに分割すると、スペース自体が断片化する可能性があり、時間の経過とともに割り当てがより困難になります。

したがって、第2のアプローチを検討する価値があるかもしれません。スペースを固定サイズの断片に切り詰めることです。仮想メモリでは、このアイディアをページングと呼びますが、これは初期の重要なシステムであるAtlas [KE+62、L78]に戻ります。プロセスのアドレス空間をいくつかの可変長論理セグメント(例えば、コード、ヒープ、スタック)に分割するのではなく、固定サイズの単位に分割します。それぞれはページと呼ばれます。これに対応して、物理メモリは、ページ・フレームと呼ばれる固定サイズのスロットの配列と見なします。これらの各フレームには単一の仮想メモリページを含めることができます。

>>THE CRUX:HOW TO VIRTUALIZE MEMORY WITH PAGES  
>>セグメント化の問題を避けるために、どのようにしてページを使ってメモリを仮想化できますか？基本的なテクニックは何ですか？最小限のスペースと時間のオーバーヘッドで、これらのテクニックをどのようにうまく機能させるにはどうすればよいでしょう？

## 18.1 A Simple Example And Overview

![](../18/img/fig18_1.PNG)

このアプローチをより明確にするために、簡単な例をあげて説明しましょう。図18.1は、4つの16バイトページ(仮想ページ0,1,2、および3)を持つ64バイトのサイズの小さなアドレス空間の例を示しています。実際のアドレス空間は、もちろん32ビットであり、したがって4GBのアドレス空間、さらには64ビットも同様です。この本では、小さな例を使ってダイジェストを簡単にすることがよくあります。

![](../18/img/fig18_2.PNG)

図18.2に示すように、物理メモリも固定サイズのスロット(この場合は128バイトの物理メモリを作成しますが、限りなく小さくします)が8ページのフレームで構成されています。図でわかるように、仮想アドレス空間のページは物理メモリ全体の異なる場所に配置されています。このダイアグラムは、物理メモリの一部を使用しているOSも示しています。

ページングには、これまでのアプローチよりも多くの利点があります。おそらく、最も重要な改善は柔軟性です。完全に開発されたページング手法では、システムがアドレス空間をどのように使用するかにかかわらず、アドレス空間の抽象化を効果的にサポートできます。たとえば、ヒープとスタックの成長方向と使用方法については想定しません。

もう1つの利点は、ページングが提供するfree space managementの単純さです。たとえば、OSが8ページの物理メモリに小さな64バイトのアドレス空間を配置したい場合、OSは単に4つの空きページを見つけます。おそらくOSはこのためのすべての空きページのフリーリストを保持しており、このリストから最初の4つの空きページだけを取得します。この例では、OSは、物理フレーム3のアドレス空間(AS)の仮想ページ0、物理フレーム7のASの仮想ページ1、フレーム5のページ2、フレーム2のページ3を配置しています。ページフレーム1、4、および6は現在フリーです。

アドレス空間の各仮想ページが物理メモリに配置される場所を記録するために、オペレーティングシステムは通常、ページテーブルと呼ばれるプロセス単位のデータ構造を保持します。ページテーブルの主な役割は、アドレス空間の各仮想ページのアドレス変換を格納することで、各ページが物理メモリのどこにあるのかを知ることができます。(仮想ページ0→物理フレーム3)、(VP1→PF7)、(VP2→PF5)、(VP3→PF2)の4つのエントリがページテーブルにあります。

このページテーブルはプロセスごとのデータ構造であることを覚えておくことが重要です(ほとんどのページテーブル構造についてはプロセスごとの構造ですが、ここで触れる例外は逆ページテーブルです)。上記の例で別のプロセスを実行する場合、仮想ページは明らかに別の物理ページにマップされるため(別のページテーブルを管理する必要があります)

さて、アドレス変換の例を実行するのに十分なことは分かっています。その小さなアドレススペース(64バイト)を持つプロセスがメモリアクセスを実行しているとします。
```
movl <virtual address>, %eax
```
具体的には、アドレス\<virtual address\>からレジスタeaxへのデータの明示的なロードに注意を払いましょう(したがって、先に起こっていなければならない命令フェッチを無視します)。

プロセスが生成したこの仮想アドレスを変換するには、仮想ページ番号(VPN)とページ内のオフセットの2つのコンポーネントに分割する必要があります。この例では、プロセスの仮想アドレス空間が64バイトであるため、仮想アドレス(2^6 = 64)に合計6ビット必要です。したがって、仮想アドレスは次のように概念化できます。

![](../18/img/fig18_2_1.PNG)

この図において、Va5は仮想アドレスの最上位ビットであり、Va0は最下位ビットです。ページサイズ(16バイト)を知っているので、仮想アドレスをさらに次のように分割することができます。

![](../18/img/fig18_2_2.PNG)

ページサイズは、64バイトのアドレス空間では16バイトごとです。したがって、4ページを選択できる必要があり、アドレスの上位2ビットはそれを選択するためにあります。したがって、2ビットの仮想ページ番号(VPN)があります。残りのビットは、ページのどのバイトにいるのかを示します。この場合は4ビットです。これをオフセットと呼びます。

プロセスが仮想アドレスを生成する場合、OSとハードウェアを結合して意味のある物理アドレスに変換する必要があります。たとえば、上記の負荷が仮想アドレス21であると仮定します。
```
movl 21, %eax
```
「21」をバイナリ形式にすると、「010101」が得られます。したがって、この仮想アドレスを調べて、仮想ページ番号(VPN)とオフセットに分解する方法を確認できます。

![](../18/img/fig18_2_3.PNG)

したがって、仮想アドレス「21」は、仮想ページ「01」(または「1」)の5番目(「0101」番目)のバイト上にある。仮想ページ番号を使用して、ページテーブルをインデックス化し、仮想ページ1がどの物理フレーム内に存在するかを見つけることができます。上記のページテーブルでは、物理フレーム番号(PFN)(物理ページ番号またはPPNとも呼ばれます)は7(バイナリ111)です。したがって、VPNをPFNに置き換えてこの仮想アドレスを変換し、物理メモリにロードします。(図18.3)

![](../18/img/fig18_3.PNG)

オフセットは、ページ内のどのバイトを必要としているかを示すだけなので、オフセットは同じである(つまり、変換されない)ことに注意してください。私たちの最終的な物理アドレスは1110101(10進数では117)であり、正確に必要なデータをロードして取得します。(図18.2)

この基本的な概要を念頭に置いて、ページングに関するいくつかの基本的な質問をすることができます。たとえば、これらのページテーブルはどこに格納されていますか？ページテーブルの典型的な内容とそのテーブルの大きさは何ですか？ページングはシステムをかなり遅くしますか？これらの疑問やその他の疑問に答える質問は、少なくとも部分的には以下の本文で回答しています。さぁ、読んでいきましょう！

## 18.2 Where Are Page Tables Stored?
ページ・テーブルは、以前に議論した小さなセグメント・テーブルまたはベース/境界ペアよりもはるかに大きくなることがあります。たとえば、典型的な32ビットのアドレス空間で、4KBのページがあるとします。この仮想アドレスは、20ビットのVPNと12ビットのオフセットに分割されます(1KBのページサイズには10ビットが必要で、4KBになるにはさらに2つ追加する必要があります)。

20ビットのVPNは、各プロセスでOSが管理しなければならない2^20の変換があることを示しています(これはおよそ100万です)。実際の変換に加えてページテーブルエントリ(PTE)あたり4バイトが必要であれば、各ページテーブルに必要なメモリは4MBになります。それはかなり大きいです。100個のプロセスが実行されているとしましょう。つまり、これらのアドレス変換だけで400 MBのメモリが必要です。機械のギガバイトの記憶がある現代でさえ、変換のためだけに大きな塊を使用するのは少しおかしいですね。そのようなページテーブルが64ビットのアドレス空間にどれだけ大きなものになるかについても考えたくもないですね。

ページテーブルは非常に大きいので、現在実行中のプロセスのページテーブルを格納するための特別なオンチップハードウェアをMMUに保持しません。代わりに、各プロセスのページテーブルをどこかのメモリに格納します。ページテーブルがOSが管理する物理メモリに存在すると仮定してみましょう。後でわかるように、OSのメモリそのものを仮想化することができるので、ページテーブルをOSの仮想メモリに格納することができます(そしてディスクにスワップすることもできます)。しかし、これは今のところ混乱するでしょう。図18.4に、OSメモリ内のページテーブルの写真を示します。そこには小さな変換がありますか？

![](../18/img/fig18_4.PNG)

## 18.3 What’s Actually In The Page Table?
ページテーブルの構成について少し話をしましょう。ページテーブルは、仮想アドレス(または実際には仮想ページ番号)を物理アドレス(物理フレーム番号)にマッピングするために使用される単なるデータ構造です。したがって、どのようなデータ構造でも機能します。最も簡単な形式は線形ページテーブルと呼ばれ、単なる配列です。OSは配列を仮想ページ番号(VPN)で索引付けし、その索引でページ表エントリ(PTE)を検索して、必要な物理フレーム番号(PFN)を見つけます。今のところ、この単純な線形構造を仮定します。後の章では、より高度なデータ構造を使用して、ページングに関するいくつかの問題を解決していきます。

各PTEの内容については、いくつかのレベルで理解する価値のあるさまざまなビットがあります。有効なビットは、特定の変換が有効かどうかを示すために一般的です。たとえば、プログラムの実行が開始されると、コードとヒープ、そのアドレス空間の一方の端にスタック、その他があります。その間の未使用スペースはすべて無効とマークされ、プロセスがこのようなメモリにアクセスしようとすると、OSにトラップが生成され、プロセスが終了する可能性があります。従って、有効ビットは、未割り当てアドレス空間をサポートするために重要です。単にアドレス空間内の未使用ページをすべて無効にするだけで、それらのページに物理フレームを割り当てる必要がなくなり、メモリを大幅に節約することができます。

また、ページの読み込み、書き込み、実行が可能かどうかを示す保護ビットがあります。ここでも、これらのビットによって許可されていない方法でページにアクセスすると、OSにトラップが生成されます。

重要ないくつかのビットがありますが、今はあまり話しません。presentビットは、このページが物理メモリ内にあるかディスク上にあるか(すなわち、スワップアウトされたか)を示します。物理メモリよりも大きいアドレス空間をサポートするために、アドレス空間の一部をディスクにスワップする方法を調べるときに必要になります。スワップは、ほとんど使用されないページをディスクに移動することによって、OSが物理メモリを解放することができます。dirtyビットも一般的であり、ページがメモリに格納されてから変更されたかどうかを示します。

referenceビット(a.k.aアクセスビット)は、ページがアクセスされたかどうかを追跡するために使用されることがあり、どのページが頻繁にアクセスされて必要であり、メモリに保持されるべきかを決定するのに有用です。このような知識は、ページの置換時に重要です。トピックについては、以降の章で詳しく説明します。

![](../18/img/fig18_5.PNG)

図18.5に、x86アーキテクチャ[I09]のページテーブルエントリの例を示します。presentビット(P)、このページに書き込みが許可されているかどうかを判定するread/writeビット(R/W)、ユーザモードプロセスがページにアクセスできるかどうかを決定するuser/supervisorビット(U/S)、これらのページのハードウェアキャッシングがどのように機能するかを決定する数ビット(PWT、PCD、PAT、およびG)、accessedビット(A)、dirtyビット(D)、最後に、ページ・フレーム番号(PFN)を含みます。

x86ページングのサポートの詳細については、インテルアーキテクチャマニュアル[I09]を参照してください。しかし、あらかじめ注意してください。これらのマニュアルを読むことは、かなり有益ですが(もちろん、OSでそのようなページテーブルを使用するコードを書く人にとっては必要です)、最初は挑戦的かもしれません。ちょっとした忍耐と多くの好奇心が必要です。

## 18.4 Paging: Also Too Slow
メモリ内のページテーブルでは、大きすぎる可能性があることを既に認識しています。それが判明したので、落ち着いてページテーブルを考えることができます。例題として簡単なものを考えていきましょう：
```
movl 21, %eax
```
繰り返しますが、アドレス21への明示的な参照を調べて、命令フェッチについて心配することはありません。この例では、ハードウェアが変換を実行すると仮定します。必要なデータをフェッチするために、システムは最初に仮想アドレス(21)を正しい物理アドレス(117)に変換しなければいけません。従って、アドレス117からデータをフェッチする前に、システムはまず、プロセスのページテーブルから適切なページテーブルエントリをフェッチし、変換を実行し、次に物理メモリからデータをロードしなければいけません。

そのためには、ハードウェアはページテーブルが現在実行中のプロセスのどこにあるのかを知る必要があります。ここでは、単一のページテーブルベースレジスタに、ページテーブルの開始位置の物理アドレスが含まれていると仮定します。目的のページテーブルエントリー(PTE)の場所を見つけるために、ハードウェアは次の機能を実行します。

```c
VPN = (VirtualAddress & VPN_MASK) >> SHIFT
PTEAddr = PageTableBaseRegister + (VPN * sizeof(PTE))
```
この例では、VPN Maskは0x30(16進数、または110000)に設定され、フル仮想アドレスからVPNビットが選択されます。SHIFTは4(オフセットのビット数)に設定されているため、正しい整数仮想ページ番号を形成するためにVPNビットを下に移動します。たとえば、仮想アドレス21(010101)で、マスクするとこの値は010000になります。それをシフトさせると01になります。それが仮想ページ1になります。次にこの値を、ページテーブルベースレジスタが指し示すPTEの配列へのインデックスとして使用します。

この物理アドレスがわかれば、ハードウェアはメモリからPTEを取り出し、PFNを抽出し、仮想アドレスからのオフセットと連結して必要な物理アドレスを形成することができます。具体的には、SHIFTによって左シフトされたPFNを考えることができます。次に、オフセットとORを取って最終アドレスを次のようにします。
```c
offset = VirtualAddress & OFFSET_MASK
PhysAddr = (PFN << SHIFT) | offset
```

```c
 // Extract the VPN from the virtual address
 VPN = (VirtualAddress & VPN_MASK) >> SHIFT

 // Form the address of the page-table entry (PTE)
 PTEAddr = PTBR + (VPN * sizeof(PTE))

 // Fetch the PTE
 PTE = AccessMemory(PTEAddr)

 // Check if process can access the page
 if (PTE.Valid == False)
 RaiseException(SEGMENTATION_FAULT)
 else if (CanAccess(PTE.ProtectBits) == False)
 RaiseException(PROTECTION_FAULT)
 else
 // Access is OK: form physical address and fetch it
 offset = VirtualAddress & OFFSET_MASK
 PhysAddr = (PTE.PFN << PFN_SHIFT) | offset
 Register = AccessMemory(PhysAddr)

 // Figure 18.6: Accessing Memory With Paging
```
最後に、ハードウェアはメモリから必要なデータを取り出し、それをレジスタeaxに入れることができます。プログラムはメモリから値をロードするのに成功しました！

要約すると、各メモリ参照で何が起きるかについての初期プロトコルを説明します。図18.6に基本的なアプローチを示します。すべてのメモリ参照(命令フェッチまたは明示的なロードまたはストア)にかかわらず、ページングでは、最初にページテーブルから変換をフェッチするために1つの余分なメモリ参照を実行する必要があります。それはたくさんの仕事です！余分なメモリ参照はコストがかかり、この場合、プロセスが2倍以上遅くなる可能性があります。

そして今、解決しなければならない2つの本当の問題があります。ハードウェアとソフトウェアの両方を慎重に設計しなければ、ページテーブルはシステムの動作が遅くなり過ぎるだけでなく、あまりにも多くのメモリを占有します。メモリ仮想化ニーズのための一見すばらしい解決策ですが、これらの2つの重大な問題をまず解決する必要があります。

## 18.5 A Memory Trace
終了する前に、単純なメモリアクセスの例をトレースして、ページングを使用したときに発生するすべてのメモリアクセスを実証します。興味のあるコードスニペット(C言語のarray.cというファイル)は次のようになります。
```c
int array[1000];
...
for (i = 0; i < 1000; i++)
array[i] = 0;
```

>> ASIDE: DATA STRUCTURE — THE PAGE TABLE  
>> 現代のOSのメモリ管理サブシステムにおける最も重要なデータ構造の1つは、ページテーブルです。一般に、ページテーブルは仮想アドレスから物理アドレスへの変換を格納しているので、アドレス空間の各ページが実際に物理メモリ内に存在する場所をシステムに知らせることができます。各アドレス空間はそのような変換を必要とするため、一般に、システムにはプロセスごとに1つのページテーブルがあります。ページテーブルの正確な構造は、ハードウェア(古いシステム)によって決定されるか、またはOS(現代システム)によってより柔軟に管理されます。

We compile array.c and run it with the following commands:
```
prompt> gcc -o array array.c -Wall -O
prompt> ./array
```
もちろん、このコードスニペット(単純に配列を初期化する)にアクセスするメモリが本当に理解できるようにするには、さらにいくつかのことを知っておく必要があります。まず、結果のバイナリを(Linuxではobjdump、Macではotoolを使用して)逆アセンブルして、どのアセンブリ命令を使ってループ内の配列を初期化するかを調べる必要があります。結果のアセンブリコードは次のとおりです。

```
1024 movl $0x0,(%edi,%eax,4)
1028 incl %eax
1032 cmpl $0x03e8,%eax
1036 jne 0x1024
```
コードは、あなたが少しx86を知っていれば、実際にはかなり理解しやすいです。最初の命令は、値のゼロ(\$0x0として表示)を配列の場所の仮想メモリアドレスに移動します。このアドレスは、％ediに％eaxを加算して％eaxに4を掛け合わせて計算します。したがって、％ediは配列のベースアドレスを保持しますが、％eaxは配列index(i)を保持します。配列は4バイトの整数の配列であるため、4を掛けます。

2番目の命令は、％eaxに保持されている配列インデックスをインクリメントし、3番目の命令は、そのレジスタの内容を16進値0x03e8または1000に比較します。比較によって、2つの値が等しくないのであれば、4番目の命令はループの先頭にジャンプします。

この命令シーケンスがどのようなメモリアクセスを行うか(仮想レベルと物理レベルの両方で)を理解するためには、仮想メモリ内のどこにコードスニペットと配列があるのか​​とページテーブルの内容と場所を知る必要があります。

この例では、サイズが64KB(非現実的に小さい)の仮想アドレス空間を想定しています。また、1KBのページサイズを想定しています。

ここで知る必要があるのは、ページテーブルの内容とその物理メモリ内の場所です。線形(配列ベース)ページテーブルを持ち、それが物理アドレス1KB(1024)にあると仮定しましょう。その内容に関しては、この例のためにマップしたことを心配する必要のある仮想ページがほんの少しあります。まず、コードが存在する仮想ページがあります。ページサイズは1KBなので、仮想アドレス1024は仮想アドレス空間の2番目のページにあります(VPN = 1、最初のページはVPN = 1)。この仮想ページが物理フレーム4(VPN 1→PFN 4)にマップされているとします。

次に、配列自体があります。そのサイズは4000バイト(1000個の整数)で、仮想アドレス40000〜44000(最後のバイトは含まない)に存在すると仮定します。この小数点範囲の仮想ページは、VPN = 39 ... VPN = 42です。したがって、これらのページのマッピングが必要です。例(VPN 39→PFN 7)、(VPN 40→PFN 8)、(VPN 41→PFN 9)、(VPN 42→PFN 10)の仮想 - 物理マッピングを想定してみましょう。

これで、プログラムのメモリ参照をトレースする準備ができました。実行されると、各命令フェッチは2つのメモリ参照を生成します。1つはページテーブルに命令が存在する物理フレームを見つけるためのもの、もう1つは処理のためにCPUにフェッチする命令です。さらに、1つの明示的なメモリ参照がmov命令の形式で存在します。これは最初に別のページテーブルアクセスを追加し(配列の仮想アドレスを正しい物理アドレスに変換します)、次に配列アクセス自体を追加します。

![](../18/img/fig18_7.PNG)

最初の5回のループ反復のプロセス全体が、図18.7に示されています。一番下のグラフはy軸の命令メモリ参照を黒で示しています(仮想アドレスは左に、実際の物理アドレスは右にあります)。真ん中のグラフは、暗い灰色の配列アクセスを示しています(再び左上の仮想と右の物理を示します)最後に、一番上のグラフは、ページテーブルのメモリアクセスを明るい灰色で示しています(この例のページテーブルは物理メモリに存在します)トレース全体のx軸は、ループの最初の5回の反復でのメモリアクセスを示します。ループ当たり10回のメモリアクセスがあります。これには、4回の命令フェッチ、1回の明示的なメモリ更新、これらの4回のフェッチと1回の明示的な更新を変換する5回のページテーブルアクセスが含まれます。

この視覚化に現れるパターンを理解できるかどうかを確認してください。特に、ループが最初の5回の反復を超えて実行されると、何が変わるでしょうか？どの新しいメモリ位置にアクセスするのでしょうか？

これはちょうど例のほんの一例です(Cコードのほんの数行だけです)。しかし、実際のアプリケーションの実際のメモリ動作を理解することの複雑さをすでに感じ取っているかもしれません。これ以上はメカニズムを複雑にするだけなので、ここまで終わりです。ごめんなさい！

## 18.6 Summary
私たちは、メモリを仮想化するという課題に対する解決策として、ページングの概念を導入しました。ページングには、以前のアプローチ(セグメンテーションなど)に比べて多くの利点があります。第1に、ページング(設計による)はメモリを固定サイズの単位に分割するため、外部の断片化につながることはありません。第2に、これは非常に柔軟であり、仮想アドレス空間の未使用部分を使用可能、使用不可の両方に対応してくれます。  
ただし、ページングのサポートを気にせずに実装すると、マシンが遅くなり(ページテーブルにアクセスするために余分なメモリアクセスが増えます)、メモリが無駄になります(有用なアプリケーションデータではなくページテーブルでメモリがいっぱいになります)。私たちは、ページングシステムを思いつくのはちょっと難しいと思うでしょう。次の2つの章では、幸いにも、マシンを速くする方法、メモリの無駄をなくす方法を教えてくれるでしょう。

# 参考文献

[KE+62] “One-level Storage System”  
T. Kilburn, and D.B.G. Edwards and M.J. Lanigan and F.H. Sumner  
IRE Trans. EC-11, 2 (1962), pp. 223-235  
(Reprinted in Bell and Newell, “Computer Structures: Readings and Examples” McGraw-Hill, New York, 1971).
The Atlas pioneered the idea of dividing memory into fixed-sized pages and in many senses was an early form of the memory-management ideas we see in modern computer systems.

[I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals”  
Intel, 2009  
Available: http://www.intel.com/products/processor/manuals  
In particular, pay attention to “Volume 3A: System Programming Guide Part 1” and “Volume 3B: System Programming Guide Part 2”

[L78] “The Manchester Mark I and atlas: a historical perspective”  
S. H. Lavington  
Communications of the ACM archive  
Volume 21, Issue 1 (January 1978), pp. 4-12  
Special issue on computer architecture  
This paper is a great retrospective of some of the history of the development of some important computer systems. As we sometimes forget in the US, many of these new ideas came from overseas.

\newpage

# 19 Paging: Faster Translations (TLBs)
仮想メモリをサポートするためのコアメカニズムとしてページングを使用すると、パフォーマンスのオーバーヘッドが発生する可能性があります。アドレス空間を小さい固定サイズの単位(すなわち、ページ)にすることにより、ページングは大量のマッピング情報を必要とします。そのマッピング情報は一般的に物理メモリに格納されるため、ページングは論理的にはプログラムによって生成された各仮想アドレスに対して余分なメモリルックアップを必要とします。命令のフェッチや明示的なロードやストアの前に、変換情報のためのメモリへの移動は非常に遅いです。

>> THE CRUX:HOW TO SPEED UP ADDRESS TRANSLATION  
>> どのようにしてアドレス変換を高速化し、ページングに必要な余分なメモリ参照を避けることができますか？どんなハードウェアサポートが必要ですか？どのようなOSの関与が必要ですか？

物事を速くしたいとき、OSは通常何らかの助けを必要とします。また、ハードウェアから助けを得ていることがよくあります。アドレス変換を高速化するために、(歴史的な理由で[CP78]と呼ばれる)変換ルックアサイドバッファ、すなわちTLB [CG68、C95]を追加します。TLBは、チップのメモリ管理ユニット(MMU)の一部であり、一般的な仮想から物理へのアドレス変換のハードウェアキャッシュです。したがって、より良い名前はアドレス変換キャッシュになります。各仮想メモリ参照時に、ハードウェアは最初にTLBをチェックして、その中に所望の変換が保持されているかどうかを調べます。そうであれば、ページテーブル(すべての変換が含まれています)を参照することなく、変換が(迅速に)実行されます。パフォーマンスに大きな影響を与えるため、実際の意味でのTLBは仮想メモリを可能にします[C95]。

## 19.1 TLB Basic Algorithm

![](../19/img/fig19_1.PNG)

図19.1は、単純な線形ページテーブル(すなわち、ページテーブルが配列)とハードウェア管理TLBです。ハードウェアがページ変換の多くを処理すると仮定して、ハードウェアが仮想アドレス変換を処理する方法の概略を示しています。テーブルのアクセスについては、後で詳しく説明します。

ハードウェアが従うアルゴリズムは次のとおりです。まず、仮想アドレス(図19.1の1行目)から仮想ページ番号(VPN)を抽出し、TLBがこのVPN(2行目)の変換を保持しているかどうかを確認します。もしそうなら、私たちはTLBヒットを持っています。これは、TLBが変換を保持していることを意味します。ここで、関連するTLBエントリからページフレーム番号(PFN)を抽出し、元の仮想アドレスからのオフセットに連結し、望んだ物理アドレス(PA)を形成し、メモリを保護することができます(5-7行目)チェックは失敗しません(4行目)。

CPUがTLB(TLBミス)で変換を見つけられない場合は、さらに処理する必要があります。この例では、ハードウェアがページテーブルにアクセスして変換を検索し(11〜12行目)、プロセスによって生成された仮想メモリ参照が有効でアクセス可能であると仮定すると(13行目、15行目)TLBの更新(18行目)。これらの一連のアクションは、主にページテーブルにアクセスするために必要な余分なメモリ参照が原因でコストがかかります(12行目)。最後に、TLBが更新されると、ハードウェアは命令を再試行します。今回はTLBに変換があり、メモリ参照は素早く処理されます。

TLBは、すべてのキャッシュと同様に、一般的なケースでは、キャッシュ内に変換がある(すなわち、ヒットしている)という前提で構築されています。TLBが処理コアの近くにあり、非常に高速になるように設計されているため、オーバーヘッドはほとんどありません。ミスが発生すると、ページングのコストが高くなります。変換を見つけるためにページテーブルにアクセスしなければならず、余分なメモリ参照(またはより複雑なページテーブルを含む)が結果として生じます。これが頻繁に発生する場合、プログラムは著しく遅く実行される可能性があります。ほとんどのCPU命令と比較してメモリアクセスは非常にコストがかかり、TLBミスはより多くのメモリアクセスにつながります。したがって、可能な限りTLBミスを避けることが重要です。

## 19.2 Example: Accessing An Array
TLBの動作を明確にするために、単純な仮想アドレストレースを調べて、TLBがどのように性能を向上させるかを見てみましょう。この例では、仮想アドレス100から始まる、メモリ内に10個の4バイト整数の配列があるとしましょう。さらに、16バイトページを持つ小さな8ビット仮想アドレス空間があるとします。したがって、仮想アドレスは4ビットのVPN(16の仮想ページがあります)と4ビットのオフセット(各ページに16バイトあります)に分割されます。

![](../19/img/fig19_2.PNG)

図19.2に、16バイトページにレイアウトされた配列を示します。ご覧のとおり、配列の最初のエントリ(a [0])は(VPN = 06、offset = 04)から始まります。そのページには3つの4バイト整数しか収まりません。配列は次のページ(VPN = 07)に進み、次の4つのエントリ(a [3] ... a [6])が見つかります。最後に、10エントリ配列(a [7] ... a [9])の最後の3つのエントリは、アドレス空間の次のページ(VPN = 08)に配置されます。次に、各配列要素にアクセスする単純なループを考えてみましょう。C言語では次のようになります。

```c
int sum = 0;
  for (i = 0; i < 10; i++) {
    sum += a[i];
}
```
簡単にするために、私たちは、ループに対して生成される唯一のメモリアクセスが配列に対するものであると推測します(変数iとsum、および命令自体は無視します)。最初の配列要素(a [0])がアクセスされると、CPUは仮想アドレス100へのロードを認識します。ハードウェアはこの(VPN = 06)からVPNを抽出し、それを使用してTLBの有効な変換をチェックします。プログラムが配列に初めてアクセスすると仮定すると、結果はTLBミスになります。

次のアクセスは[1]へのアクセスです。TLBヒット！配列の2番目の要素は最初の要素の隣にいるため、同じページに存在します。配列の最初の要素にアクセスするときにすでにこのページにアクセスしているので、変換はすでにTLBにロードされています。[2]へのアクセスは、[0]と[1]と同じページにも存在するため、同様の成功(別のヒット)に遭遇します。

残念ながら、プログラムが[3]にアクセスすると、別のTLBミスが発生します。しかし、TLB内のすべてのエントリがメモリ内の同じページに存在するため、次のエントリ(a [4] ... a [6])が再びヒットします。

最後に[7]にアクセスすると、最後のTLBミスが1つ発生します。ハードウェアは、再び、物理メモリ内のこの仮想ページの位置を把握するためにページテーブルを参照し、それに応じてTLBを更新します。TLB内のすべてのエントリがメモリ内の同じページに存在するため、最後の2回のアクセス(a [8]とa [9])には、再びヒットします。

ヒット、ヒット、ミス、ヒット、ヒット、ヒット、ミス、ヒット、ヒット、配列への10回のアクセス中にTLBアクティビティを要約しましょう。したがって、ヒット数をアクセス総数で割ったTLBヒット率は70％です。これはあまり高くありませんが(確かに、ヒット率は100％に近づいています)、これは非ゼロです。これは驚くかもしれません。プログラムが配列にアクセスするのはこれが初めてですが、TLBは空間的局所性のためにパフォーマンスを向上させます。配列の要素は、ページに密接にパックされている(すなわち、それらは互いに空間的に近い)ので、ページ上の要素への最初のアクセスのみがTLBミスをします。

この例では、ページサイズが果たす役割にも注意してください。ページ・サイズが単に2倍の大きさ(16バイトではなく32バイト)であれば、配列アクセスにはより少ないミスしか生じません。一般的なページサイズは4KBに似ていますので、これらのタイプの密な配列ベースのアクセスは優れたTLBパフォーマンスを実現し、1ページのアクセス当たり1回のミスに遭遇します。

TLBのパフォーマンスに関する最後の1つは、このループが完了してすぐにプログラムが再び配列にアクセスすると、必要な変換をキャッシュするのに十分なTLBがあると仮定すると、ヒット、ヒット、ヒット、ヒット、ヒット、ヒット、ヒット、ヒットになります。この場合、時間的局所性、すなわち時間的にメモリ項目の迅速な再参照のためにTLBヒット率が高くなります。キャッシュと同様、TLBはプログラムのプロパティである成功のために空間的(キャッシュの大きさ)および時間的局所性(再びアクセスされる可能性)に依存しています。関心のあるプログラムがそのような局所性(および多くのプログラム)を示す場合、TLBのヒット率は高くなる可能性が高いです。

>> TIP: USE CACHING WHEN POSSIBLE  
>> キャッシングは、コンピュータシステムで最も基本的なパフォーマンス手法の1つです。それは、「共通の高速化」[HP06]を何度も繰り返すことです。ハードウェアキャッシュの背後にあるアイデアは、命令とデータ参照における局所性を利用することです。通常、時間的局所性と空間的局所性の2つのタイプがあります。時間的局所性を考慮すると、最近アクセスされた命令またはデータ項目は、将来すぐに再アクセスされる可能性が高いという考えがあります。ループ変数や命令をループ内で考えてみましょう。それらは時間の経過と共に繰り返しアクセスされます。空間的局所性では、プログラムがアドレスxのメモリにアクセスすると、xの近くのメモリにすぐにアクセスする可能性があるという考えがあります。1つの要素にアクセスしてから次の要素にアクセスする何らかの配列をストリーミングすることを想像してください。もちろん、これらの特性はプログラムの正確な性質に依存するため、厳しい法則ではなく、より大雑把なルールです。  
ハードウェアキャッシュは、命令、データ、アドレス変換(TLBのように)を問わず、メモリのコピーを小型で高速なオンチップメモリ​​に保存することでローカリティを利用します。要求を満たすために(遅い)メモリに移動する代わりに、プロセッサは、まず、近くのコピーがキャッシュ内に存在するかどうかをチェックすることができます。そうであれば、プロセッサは迅速に(すなわち、数CPUサイクルで)アクセスし、メモリにアクセスするのに費やす時間(多くのナノ秒)を費やすことを回避することができます。  
あなたは疑問に思うかもしれません。(TLBのような)キャッシュがすばらしいのであれば、もっと大きなキャッシュを作り、その中にすべてのデータを保存してみてください。残念ながら、これは物理学のようなより基本的な法則に踏み込んでいます。高速キャッシュが必要な場合は、光速や他の物理的な制約などの問題が関連するため、キャッシュを小さくする必要があります。定義どおりの大規模なキャッシュはすべて遅いため、目的が破綻してしまいます。したがって、私たちは小さくて高速なキャッシュが必要なのです。残っている問題は、パフォーマンスを向上させるためにそれらを最適に使用する方法です。

## 19.3 Who Handles The TLB Miss?
私たちは答えなければならない1つの質問があります。誰がTLBミスを処理するのですか？ハードウェアまたはソフトウェア(OS)の2つの答えが可能です。昔、ハードウェアには複雑な命令セット(複雑な命令セットのコンピュータではCISCと呼ばれることもあります)がありました。ハードウェアを構築した人々は、OSの人たちをあまり信頼しませんでした。したがって、ハードウェアはTLBミスを完全に処理します。これを行うには、ハードウェアはページテーブルがメモリ内のどこにあるのかを(図19.1の11行目で使用されているページテーブルベースレジスタを使用して)正確に知る必要があります。欠落していると、ハードウェアはページテーブルを「歩いて」、正しいページテーブルエントリを見つけて、必要な変換を抽出し、TLBを更新し、命令を再試行します。ハードウェア管理TLBを持つ「古い」アーキテクチャの例は、固定マルチレベルページテーブルを使用するIntel x86アーキテクチャです(詳細は次の章を参照してください)。現在のページテーブルはCR3レジスタ[I0​​9]によって指示されます。
```c
 VPN = (VirtualAddress & VPN_MASK) >> SHIFT
 (Success, TlbEntry) = TLB_Lookup(VPN)
 if (Success == True) // TLB Hit
 if (CanAccess(TlbEntry.ProtectBits) == True)
 Offset = VirtualAddress & OFFSET_MASK
 PhysAddr = (TlbEntry.PFN << SHIFT) | Offset
 Register = AccessMemory(PhysAddr)
 else
 RaiseException(PROTECTION_FAULT)
 else // TLB Miss
 RaiseException(TLB_MISS)
// Figure 19.3: TLB Control Flow Algorithm (OS Handled)
```
より近代的なアーキテクチャ(例えば、MIPS R10k [H93]またはSunのSPARC v9 [WG00]、RISCまたは縮小命令セットコンピュータ)は、ソフトウェア管理TLBとして知られているものを持っています。TLBミスでは、ハードウェアは単に現在の命令ストリームを一時停止し、特権レベルをカーネルモードに持ち上げ、トラップハンドラにジャンプする例外(図19.3の11行目)を生成するだけです。ご想像のとおり、このトラップハンドラは、TLBミスを処理する明白な目的で書かれたOS内のコードです。実行されると、コードはページテーブルの変換を検索し、特殊な"特権"命令を使用してTLBを更新し、トラップから戻ります。この時点で、ハードウェアは命令をリトライします(結果としてTLBヒットとなります)。

重要な詳細をいくつか議論しましょう。まず、return from trap命令は、システムコールを処理する前に見たトラップからの復帰とは少し異なる必要があります。後者のソフトウェアの場合、プロシージャコールからのリターンがプロシージャへのコールの直後の命令に戻るのと同様に、return from trapはOSへのトラップ後の命令で実行を再開する必要があります。  
前者のハードウェアの場合、TLBミスハンドリングトラップから復帰するとき、ハードウェアはトラップを引き起こした命令で実行を再開しなければいけません。この再試行により、命令が再び実行され、今回はTLBヒットとなります。したがって、トラップや例外がどのように発生したかによって、ハードウェアはOSにトラップするときに別のPC(プログラムカウンタ)を保存しなければならず、その時間が到来したときに適切に再開する必要があります。

第2に、TLBミス・ハンドリング・コードを実行するとき、OSは無限のTLBミス・チェインが発生しないように注意する必要があります。その問題に対しては多くの解決策が存在します。たとえば、TLBミスハンドラを物理メモリに保存しておくことができます(アドレス変換が行われていない場合)。または、ハンドラコード自体を永続的に有効な変換のTLBにエントリとして予約します。これらの有線変換は常にTLBでヒットします。

ソフトウェア管理アプローチの主な利点は柔軟性です。OSは、ハードウェアの変更を必要とせずに、ページテーブルを実装したい任意のデータ構造を使用できます。別の利点は単純さです。TLBの制御フロー(図19.3の11行目と、図19.1の11行目とは対照的に)で見られるように、ハードウェアはミスで多くを行う必要はありません。例外が発生し、OSのTLBミスハンドラが残りの処理を行います。

>> ASIDE: RISC VS. CISC  
>> 1980年代には、コンピュータアーキテクチャのコミュニティで大きな戦いが起こりました。一方は複雑な命令セットコンピューティングのためのCISCでした。もう一方の側では、RISCがReduced Instruction Set Computing [PS81]でした。RISC側はBerkeleyのDavid PattersonとStanfordのJohn Hennessy(有名な著書[HP06]の共著者でもある)が主導していましたが、後にJohn CockeはRISCに関する彼の初期の研究でTuring賞を受賞しました[CM00]。CISC命令セットは多くの命令を持つ傾向があり、各命令は比較的強力です。たとえば、2つのポインタと長さを取り、ソースからデスティネーションにバイトをコピーする文字列コピーが表示されます。CISCの背後にある考え方は、命令はハイレベルのプリミティブでなければならず、アセンブリ言語そのものを使いやすくし、コードをよりコンパクトにする必要があるということでした。  
RISC命令セットはまったく逆です。RISCの背後にある重要なことは、命令セットは実際にコンパイラターゲットであり、実際にはすべてのコンパイラが高性能コードを生成するために使用できる単純なプリミティブであることです。したがって、RISCの主張者は、可能な限りハードウェア(特にマイクロコード)から多くのものを取り除き、単純なもの、均一で速いものを残しておきたいと主張しました。  
初期段階では、RISCチップは著しく高速であったため、大きなインパクトを与えました[BC91]。多くの論文が書かれました。いくつかの企業が形成された(例えば、MIPSおよびSun)。しかし、インテルなどのCISCメーカは、複雑な命令をマイクロ命令に変換したパイプラインステージを早期に追加するなど、RISCのような処理が可能な、多くのRISC技術をプロセッサのコアに組み込んでいました。これらのイノベーションに加え、各チップ上のトランジスタ数の増加により、CISCは競争力を維持することができました。最終的な結果は、議論が崩れ落ちたことです。今日、両方のタイプのプロセッサを高速に動作させることができます。  

## 19.4 TLB Contents: What’s In There?
ハードウェアTLBの内容をより詳しく見てみましょう。一般的なTLBは、32,64、または128のエントリを持ち、fully associative(完全連想型)と呼ばれるものです。基本的には、TLB内の任意の変換が可能であり、TLB全体を並行して検索して目的の変換を見つけることを意味します。TLBエントリは次のようになります。

![](../19/img/fig19_3_1.PNG)

変換がこれらの場所のいずれかに終わる可能性があるため、VPNとPFNの両方が各エントリに存在することに注意してください(TLBは完全連想型キャッシュと呼ばれます)。ハードウェアはエントリを並行して検索し、一致するエントリがあるかどうかを確認します。

>> ASIDE: TLB VALID BIT 6= PAGE TABLE VALID BIT  
よくある間違いは、TLBで見つかった有効なビットをページテーブルで見つかったものと混同することです。ページテーブルでは、ページテーブルエントリ(PTE)が無効とマークされている場合、ページがプロセスによって割り当てられていないことを意味し、正常に動作するプログラムによってアクセスされるべきではありません。無効なページがアクセスされたときの通常の応答は、プロセスを強制終了することによって応答するOSにトラップすることです。  
>> TLB有効ビットは、対照的に、単にTLBエントリ内に有効な変換があるかどうかを示します。例えば、システムがブートするとき、アドレス変換がそこにキャッシュされていないので、各TLBエントリの共通初期状態は無効に設定されます。仮想メモリが有効になり、プログラムの実行が開始されて仮想アドレス空間にアクセスすると、TLBにはゆっくりとデータが読み込まれ、有効なエントリがすぐにTLBを満たします。  
>> TLBの有効ビットはコンテキストスイッチを実行するときにも非常に便利です。これについては後で詳しく説明します。すべてのTLBエントリを無効に設定することにより、システムは実行しようとしているプロセスが前のプロセスからの仮想から物理への変換を誤って使用しないようにすることができます。

もっと興味深いのは「その他のビット」です。たとえば、TLBには通常、エントリが有効な変換を持つかどうかを示す有効ビットがあります。また、ページにアクセスする方法を決定するプロテクションビットもあります(ページテーブルのように)。たとえば、コード・ページは読み取りと実行のマークが付けられ、ヒープ・ページは読み取りと書き込みのマークが付けられます。また、アドレス空間識別子、ダーティビットなど、いくつかの他のフィールドがあるかもしれません。詳細は以下を参照してください。

## 19.5 TLB Issue: Context Switches
TLBでは、プロセス(したがってアドレス空間)を切り替えるときにいくつかの新しい問題が発生します。具体的には、TLBには、現在実行中のプロセスに対してのみ有効な仮想から物理への変換が含まれています。これらの変換は、他のプロセスにとって意味がありません。結果として、あるプロセスから別のプロセスに切り替えるときに、ハードウェアまたはOS(またはその両方)は、実行しようとしているプロセスが以前に実行されたプロセスの変換を誤って使用しないように注意する必要があります。

この状況をよりよく理解するために、例を見てみましょう。1つのプロセス(P1)が実行されているときには、TLBが有効な変換、すなわちP1のページテーブルから来た変換をキャッシュしていると仮定します。この例では、P1の10番目の仮想ページが物理フレーム100にマッピングされていると仮定します。

この例では、別のプロセス(P2)が存在し、OSがすぐにコンテキストスイッチを実行して実行することを決定する場合があります。ここでは、P2の10番目の仮想ページが物理フレーム170にマッピングされているものとします。両方のプロセスのエントリがTLBにある場合、TLBの内容は次のようになります。

![](../19/img/fig19_3_2.PNG)

上記のTLBでは、VPN 10はPFN 100(P1)またはPFN 170(P2)のいずれかに変換されますが、ハードウェアはどのプロセスがどのエントリを意味するのかを区別できません。 したがって、TLBが複数のプロセス間で仮想化を正確かつ効率的にサポートするためには、さらにいくつかの作業を行う必要があります。なので、以下が問題になります。

>>THE CRUX:HOW TO MANAGE TLB CONTENTS ON A CONTEXT SWITCH  
>>プロセス間のコンテキスト切り替え時に、最後のプロセスのTLB内の変換は、実行しようとしているプロセスにとって意味がありません。この問題を解決するには、ハードウェアやOSは何をすべきですか？

この問題にはいくつかの解決策があります。1つのアプローチは、単にコンテキストスイッチ上でTLBをフラッシュし、次のプロセスを実行する前にTLBを空にすることです。ソフトウェアベースのシステムでは、これは明示的な(および特権の)ハードウェア命令で実現できます。ハードウェア管理のTLBでは、ページテーブルのベースレジスタが変更されたときにフラッシュすることができます(OSはコンテキストスイッチ上でPTBRを変更する必要があります)。いずれの場合も、フラッシュ動作はすべての有効ビットを単に0に設定し、本質的にTLBの内容をクリアします。

各コンテキストスイッチでTLBをフラッシュすることにより、TLB内の誤った変換にプロセスが偶然に遭遇することはないので、今や実用的な解決策があります。しかし、コストがかかります。プロセスが実行されるたびに、TLBミスがデータ・ページやコード・ページに接触すると、TLBミスが発生する必要があります。OSがプロセス間で頻繁に切り替わると、このコストが高くなる可能性があります。

このオーバーヘッドを減らすために、コンテキストスイッチ間でTLBを共有できるようにするハードウェアサポートが追加されているシステムもあります。特に、一部のハードウェアシステムでは、TLBにアドレス空間識別子(ASID)フィールドが用意されています。ASIDはプロセス識別子(PID)と考えることができますが、通常はビット数が少なくなります(たとえば、ASIDの場合は8ビット、PIDの場合は32ビット)。

上記の例のTLBを上から取り、ASIDを追加すると、プロセスがTLBを容易に共有できることがわかります。それ以外は同一の変換を区別するためには、ASIDフィールドだけが必要です。ここに、追加されたASIDフィールドを含むTLBの描写があります。

![](../19/img/fig19_3_3.PNG)

したがって、アドレス空間識別子を使用すると、TLBは混乱を起こさずに異なるプロセスからの変換を同時に保持できます。もちろん、ハードウェアは変換を実行するために現在どのプロセスが実行されているかを知る必要があるため、OSはコンテクストスイッチ上で現在のプロセスのASIDにいくつかの特権レジスタを設定する必要があります。別として、TLBの2つのエントリが非常に似ている別のケースも考えているかもしれません。この例では、2つの異なるプロセスに2つのエントリがあり、2つの異なるVPNが同じ物理ページを指しています。

![](../19/img/fig19_3_4.PNG)

この状況は、たとえば、2つのプロセスがページ(コードページなど)を共有する場合に発生する可能性があります。上記の例では、プロセス1が物理ページ101をプロセス2と共有しています。P1はこのページをアドレス空間の10番目のページにマッピングし、P2はアドレス空間の50番目のページにマッピングします。コードページ(バイナリまたは共有ライブラリ)の共有は、使用されている物理ページの数を減らし、メモリのオーバーヘッドを削減するので便利です。

## 19.6 Issue: Replacement Policy
他のキャッシュと同様に、TLBも同様に、考慮すべきもう一つの問題はキャッシュの置き換えです。具体的には、TLBに新しいエントリをインストールするときに、古いエントリを置き換える必要があります。

>> THE CRUX: HOW TO DESIGN TLB REPLACEMENT POLICY  
>> 新しいTLBエントリを追加する際に、どのTLBエントリを置き換える必要がありますか？もちろん、目標はミス率を最小限に抑える(またはヒット率を高める)ことで、パフォーマンスを向上させることです。

ページをディスクにスワップする問題に取り組む際に、このようなポリシーを詳細に検討します。ここでは、いくつかの典型的なポリシーを強調します。最も一般的な方法の1つは、最も最近に使用されたエントリ(LRUエントリ)を削除することです。LRUは、最近使用されていないエントリが追い出される可能性が高いと仮定して、メモリ参照ストリームの局所性を利用しようとします。別の典型的な手法は、無作為にTLBマッピングを退去させるランダムポリシーを使用することです。このようなポリシーは、その単純さとコーナーケースの振る舞いを回避する能力のために有用です。たとえば、LRUなどの「合理的な」ポリシーは、プログラムがサイズnのTLBを使用してn + 1ページにわたってループするとき、非常に不合理な振る舞いをします。この場合、LRUはすべてのアクセス時にミスします。一方、ランダムはn + 1ページあるうち、ランダムに1つを選ぶだけなので、はるかに優れています。

## 19.7 A Real TLB Entry

![](../19/img/fig19_4.PNG)

最後に、実際のTLBを簡単に見てみましょう。この例は、ソフトウェア管理のTLBを使用する最新のシステムであるMIPS R4000 [H93]の例です。わずかに単純化されたMIPSのTLBエントリを図19.4に示します。MIPS R4000は、4KBページの32ビットアドレス空間をサポートしています。したがって、一般的な仮想アドレスでは20ビットのVPNと12ビットのオフセットが予想されます。ただし、TLBに表示されているように、VPNには19ビットしかありません。結果として、ユーザーアドレスはアドレス空間の半分(カーネル用に予約されている残り)からしか得られないため、19ビットのVPNしか必要としません。VPNは最大24ビットの物理フレーム番号(PFN)に変換されるため、最大64GBの(物理的な)メインメモリ(2^24 4KBページ)を持つシステムをサポートできます。

MIPS TLBには他にも興味深いビットがいくつかあります。プロセス間でグローバルに共有されるページに使用されるグローバルビット(G)があります。従って、グローバルビットがセットされている場合、ASIDは無視されます。また、OSがアドレス空間を区別するために使用できる8ビットのASIDも表示されています(前述)。1つの質問：256(2^8)を超えるプロセスが同時に実行されている場合、OSはどうすべきですか？最後に、ページがハードウェアによってどのようにキャッシュされるかを決定する3つのCoherence(C)ビットがあります(これらの注釈の範囲を少し超えています)。ページが書き込まれたときにマークされるdirtyビット(これは後で使用します)。エントリに有効な変換が存在するかどうかをハードウェアに知らせるvaildビット。複数のページサイズをサポートするpage mask field(図示せず)もあります。なぜ大きなページを持つことが有用なのかを後で見ていきます。最後に、64ビットのうちのいくつかは未使用(図では網掛けのグレー)です。

MIPS TLBには通常32または64のエントリがあり、そのほとんどはユーザプロセスが実行されるときに使用されます。しかし、いくつかはOS用に予約されています。wired registerは、OSによって予約されるTLBのスロット数をハードウェアに伝えるためにOSによって設定できます。OSは、(例えば、TLBミスハンドラ内で)TLBミスが問題となる重要な時間にアクセスしたいコードおよびデータに対して、これらの予約されたマッピングを使用します。

MIPS TLBはソフトウェアで管理されているため、TLBを更新する手順が必要です。MIPSは、以下の4つの命令を提供しています。TLBP：TLBを調べて、特定の変換がそこにあるかどうかを調べます。TLBRエントリの内容をレジスタに読み込むTLBR。特定のTLBエントリを置き換えるTLBWI。ランダムなTLBエントリを置き換えるTLBWR。OSはこれらの命令を使用して、TLBの内容を管理します。もちろん、これらの命令は特権です。ユーザプロセスがTLBの内容を変更する可能性がある場合(ヒント：マシンの引き継ぎ、悪意のある「OS」の実行、またはSunの消滅を含む)について何ができるか想像してください。

>>TIP: RAM ISN’T ALWAYS RAM (CULLER’S LAW)  
>> ランダムアクセスメモリ(RAM)という用語は、言葉の通り、ランダムに別のRAMに速くアクセスできます。TLBなどのハードウェア/OS機能のために、RAMをこのように考えるのは一般的には良いですが、特にそのページが現在TLBによってマップされていない場合は、メモリの特定のページにアクセスするとコストがかかる可能性があります。したがって、実装のヒントを覚えておくとよいでしょう。RAMは常にRAMであるとは限りません。場合によっては、アクセスされたページ数がTLBカバレッジを超えた場合など、アドレス空間にランダムにアクセスすると、パフォーマンスが著しく低下する可能性があります。当社の顧問の一人、David Cullerは、多くの業績上の問題の原因として常にTLBを指していたため、この法律の名誉を「Culler's Law」と名付けました。

## 19.8 Summary
私たちは、ハードウェアがアドレス変換を高速化するためにどのように役立つかを見てきました。アドレス変換キャッシュとして小型の専用オンチップTLBを提供することにより、主メモリ内のページテーブルにアクセスすることなく、ほとんどのメモリ参照がうまく処理されます。したがって、一般的なケースでは、プログラムのパフォーマンスは、メモリがまったく仮想化されていないか、オペレーティングシステムにとって優れた成果であり、現代システムでのページングの使用に不可欠です。

しかし、TLBは存在するすべてのプログラムのために、その性能を提供できません。特に、プログラムが短期間にアクセスするページ数がTLBに収まるページ数を超える場合、プログラムは多数のTLBミスを生成し、したがってかなり遅く実行されます。この現象はTLBの適用範囲を超えていると言い、特定のプログラムではかなり問題になる可能性があります。次の章で説明するように、1つのソリューションは、より大きなページサイズのサポートを含めることです。キーデータ構造を、より大きなページによってマッピングされるプログラムのアドレス空間の領域にマッピングすることによって、TLBの有効範囲を拡大することができる。ラージ・ページのサポートは、大規模でランダムにアクセスされる特定のデータ構造を持つデータベース管理システム(DBMS)などのプログラムによって利用されることがよくあります。

言及に値する他のTLBの問題の1つは、TLBアクセスがCPUパイプラインのボトルネックになりやすいことです。特に、physical indexキャッシュと呼ばれます。このようなキャッシュでは、キャッシュにアクセスする前にアドレス変換が行われなければなりません。そのため、かなり遅くなる可能性があります。この潜在的な問題のために、人々は仮想アドレスを持つキャッシュにアクセスするためのあらゆる種類の賢明な方法を検討しているため、キャッシュヒットの場合には高価な変換手順を避けています。このようなvirtual indexキャッシュは、パフォーマンス上の問題を解決しますが、ハードウェア設計にも新たな問題をもたらします。詳細についてはWigginsの詳細な調査を参照してください[W03]。

# 参考文献

[BC91] “Performance from Architecture: Comparing a RISC and a CISC  
with Similar Hardware Organization”  
D. Bhandarkar and Douglas W. Clark  
Communications of the ACM, September 1991  
A great and fair comparison between RISC and CISC. The bottom line: on similar hardware, RISC was about a factor of three better in performance.

[CM00] “The evolution of RISC technology at IBM”  
John Cocke and V. Markstein  
IBM Journal of Research and Development, 44:1/2  
A summary of the ideas and work behind the IBM 801, which many consider the first true RISC microprocessor.

[C95] “The Core of the Black Canyon Computer Corporation”  
John Couleur  
IEEE Annals of History of Computing, 17:4, 1995  
In this fascinating historical note, Couleur talks about how he invented the TLB in 1964 while working for GE, and the fortuitous collaboration that thus ensued with the Project MAC folks at MIT.

[CG68] “Shared-access Data Processing System”  
John F. Couleur and Edward L. Glaser  
Patent 3412382, November 1968  
The patent that contains the idea for an associative memory to store address translations. The idea, according to Couleur, came in 1964.

[CP78] “The architecture of the IBM System/370”  
R.P. Case and A. Padegs  
Communications of the ACM. 21:1, 73-96, January 1978  
Perhaps the first paper to use the term translation lookaside buffer. The name arises from the historical name for a cache, which was a lookaside buffer as called by those developing the Atlas system at the University of Manchester; a cache of address translations thus became a translation lookaside buffer. Even though the term lookaside buffer fell out of favor, TLB seems to have stuck, for whatever reason.

[H93] “MIPS R4000 Microprocessor User’s Manual”.  
Joe Heinrich, Prentice-Hall, June 1993  
Available: http://cag.csail.mit.edu/raw/  
documents/R4400 Uman book Ed2.pdf  

[HP06] “Computer Architecture: A Quantitative Approach”  
John Hennessy and David Patterson  
Morgan-Kaufmann, 2006  
A great book about computer architecture. We have a particular attachment to the classic first edition.

[I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals”  
Intel, 2009  
Available: http://www.intel.com/products/processor/manuals  
In particular, pay attention to “Volume 3A: System Programming Guide Part 1” and “Volume 3B: System Programming Guide Part 2”

[PS81] “RISC-I: A Reduced Instruction Set VLSI Computer”  
D.A. Patterson and C.H. Sequin  
ISCA ’81, Minneapolis, May 1981  
The paper that introduced the term RISC, and started the avalanche of research into simplifying computer chips for performance.

[SB92] “CPU Performance Evaluation and Execution Time Prediction  
Using Narrow Spectrum Benchmarking”  
Rafael H. Saavedra-Barrera  
EECS Department, University of California, Berkeley  
Technical Report No. UCB/CSD-92-684, February 1992  
www.eecs.berkeley.edu/Pubs/TechRpts/1992/CSD-92-684.pdf  
A great dissertation about how to predict execution time of applications by breaking them down into constituent pieces and knowing the cost of each piece. Probably the most interesting part that comes out of this work is the tool to measure details of the cache hierarchy (described in Chapter 5). Make sure to check out the wonderful diagrams therein.

[W03] “A Survey on the Interaction Between Caching, Translation and Protection”  
Adam Wiggins  
University of New South Wales TR UNSW-CSE-TR-0321, August, 2003  
An excellent survey of how TLBs interact with other parts of the CPU pipeline, namely hardware caches.  

[WG00] “The SPARC Architecture Manual: Version 9”  
David L. Weaver and Tom Germond, September 2000  
SPARC International, San Jose, California  
Available: http://www.sparc.org/standards/SPARCV9.pdf

\newpage

# 20 Paging: Smaller Tables
ここでは、ページングが導入する第2の問題に取り組んでいきます。ページテーブルが大きすぎるため、メモリを多く消費してしまいます。まずは、線形ページテーブルから始めましょう。線形ページテーブルはメモリ規模がかなり大きくなります。ここでも、4KB(2 ^ 12バイト)ページと4バイトのページテーブルエントリを持つ32ビットアドレス空間(2 ^ 32バイト)を仮定します。したがって、アドレス空間には約100万の仮想ページがあります(2 ^ 32/2 ^ 12)。ページテーブルエントリサイズを掛け合わせると、ページテーブルのサイズが4MBであることがわかります。また、システム内のすべてのプロセスについて1ページテーブルがあります。100個のアクティブなプロセス(現代のシステムでは一般的ではない)で、ページテーブルのためだけに数百メガバイトのメモリを割り当てます。その結果、我々はこの重い負担を軽減するための技術を模索しています。

>> CRUX: HOW TO MAKE PAGE TABLES SMALLER?  
>> 単純な配列ベースのページテーブル(通常は線形ページテーブル)は大きすぎるため、一般的なシステムではあまりにも多くのメモリを占有します。どのようにしてページテーブルを小さくすることができますか？重要なアイデアは何ですか？これらの新しいデータ構造の結果、どのような非効率性が生じますか？

## 20.1 Simple Solution: Bigger Pages
ページテーブルのサイズを1つの簡単な方法で減らすことができます。32ビットアドレス空間でもう一度考えてみましょう、しかし今回は16KBのページを仮定します。したがって、18ビットのVPNと14ビットのオフセットがあります。各PTE(4バイト)に同じサイズを仮定すると、線形ページテーブルには2^18のエントリがあり、ページテーブルの合計サイズは1MBとなり、ページテーブルのサイズは4倍に縮小されます(この縮小はページサイズの4倍の増加と反比例です)

しかし、このアプローチの主な問題は、大きなページが各ページ内で無駄になるということです。内部断片化として知られている問題(割り当て単位内のゴミ)です。アプリケーションはこうしてページの割り当てを終わらせますが、それぞれの小さなビットまたはある部分だけを使用するため、これらの大きすぎるページせいで、メモリはがいっぱいになります。したがって、ほとんどのシステムでは、通常の場合、4KB(x86の場合)または8KB(SPARCv9の場合)の比較的小さいページサイズを使用します。しかし、ページテーブルの問題は単純には解決できません。

>> ASIDE: MULTIPLE PAGE SIZES  
>> さて、MIPS、SPARC、x86-64などの多くのアーキテクチャで複数のページサイズがサポートされていることに注意してください。通常、小さな(4KBまたは8KB)ページサイズが使用されます。しかし、スマートなアプリケーションがそれを要求すると、アドレス空間の特定の部分に1つの大きなページ(たとえばサイズ4MB)を使用することができ、頻繁に使用される(そして大きな)データ構造をそのようなスペースは単一のTLBエントリのみを消費します。このような大きなページの使用は、データベース管理システムやその他のハイエンドの商用アプリケーションでは一般的です。しかし、複数のページサイズを使用する主な理由は、ページテーブルのスペースを節約することではありません。TLBへの負担を軽減し、プログラムがTLBミスを多すぎることなく多くのアドレス空間にアクセスできるようにします。しかし、[N+02]のように複数のページサイズを使用すると、OSの仮想メモリマネージャがより複雑になるため、大規模なページを直接要求するアプリケーションに新しいインターフェイスをエクスポートするだけで大​​きなページが簡単に使用されることがあります。

## 20.2 Hybrid Approach: Paging and Segments
あなたが人生の中で何かに合理的だが異なるアプローチを2つ持っているときは、常に両方の世界の最高を得ることができるかどうかを確認するために2つの組み合わせを調べる必要があります。このような組み合わせをハイブリッドと呼んでいます[M28]。  
数年前、Multicsの作成者(特にJack Dennis)は、Multics仮想メモリシステム[M07]の構築においてこのような考えにチャレンジしました。具体的には、Dennisはページテーブルのメモリオーバーヘッドを減らすためにページングとセグメンテーションを組み合わせる考えを持っていました。典型的な線形ページテーブルをより詳細に調べることによって、これがなぜ機能するのかが分かります。ヒープとスタックの使用部分が小さいアドレス空間があるとします。この例では、1KBページの小さな16KBアドレス空間を使用します(図20.1)。このアドレス空間のページテーブルは図20.2にあります。

![](../20/img/fig20_1.PNG)

![](../20/img/fig20_2.PNG)

この例では、単一コードページ(VPN 0)が物理ページ10、単一ヒープページ(VPN 4)から物理ページ23、およびアドレススペースの他端の2つのスタックページ(VPN 14および15)にマップされ、物理ページ28および4にそれぞれマッピングされる。画像からわかるように、ページテーブルのほとんどは使用されず、無効なエントリがいっぱいです。これは、16KBの小さなアドレス空間用です。32ビットのアドレス空間のページテーブルとそこに潜在するすべての無駄なスペースを想像してみてください！

したがって、私たちのハイブリッドアプローチとしては、プロセスのアドレス空間全体に単一のページテーブルを持たせる代わりに、論理セグメントごとに1つのページテーブルを作成するのはどうですか？この例では、3つのページテーブルを用意しています。1つはアドレススペースのコード、ヒープ、スタック部分用です。

ここで、セグメンテーションを覚えていれば、各セグメントが物理メモリにどこにあるのかを教えてくれるベースレジスタと、そのセグメントのサイズを教えてくれる境界レジスタまたは限界レジスタがありました。ハイブリッドでは、MMUにはそれらの構造があります。ここでは、ベースを使用してセグメント自体を指すのではなく、そのセグメントのページテーブルの物理アドレスを保持します。境界レジスタは、ページテーブルの終わり(すなわち、有効ページ数)を示すために使用されます。

明確な例を挙げてみましょう。4KBページの32ビット仮想アドレス空間と、4つのセグメントに分割されたアドレス空間を想定します。この例では、コード用、ヒープ用、スタック用の3つのセグメントのみを使用します。

アドレスが参照するセグメントを特定するために、アドレス空間の上位2ビットを使用します。00が何も使われていないセグメント、コードの場合は10、ヒープの場合は10、スタックの場合は11が使用されていると仮定しましょう。したがって、仮想アドレスは次のようになります。

![](../20/img/fig20_2_1.PNG)

ハードウェアでは、コード、ヒープ、スタックごとに1つずつ、3つのベース/境界のペアがあるとします。プロセスが実行されているとき、これらの各セグメントのベース・レジスタには、そのセグメントの線形ページテーブルの物理アドレスが格納されます。したがって、システム内の各プロセスには、3つのページテーブルが関連付けられています。コンテキストスイッチでは、これらのレジスタは、新しく実行されているプロセスのページテーブルの位置を反映するように変更する必要があります。

TLBミス(ハードウェア管理TLB、すなわちハードウェアがTLBミスを処理することを前提とする)では、ハードウェアはセグメントビット(SN)を使用して、使用するベースおよび境界のペアを決定する。次に、ハードウェアは、物理アドレスを取り込み、VPNと組み合わせてページテーブルエントリ(PTE)のアドレスを形成します。
```
SN = (VirtualAddress & SEG_MASK) >> SN_SHIFT
VPN = (VirtualAddress & VPN_MASK) >> VPN_SHIFT
AddressOfPTE = Base[SN] + (VPN * sizeof(PTE))
```
このシーケンスはよく知られているはずです。従来の線形ページテーブルとほぼ同じです。もちろん、唯一の違いは、単一のページテーブルベースレジスタの代わりに3つのセグメントベースレジスタの1つを使用することです。

ハイブリッド方式の重要な違いは、セグメントごとに境界レジスタが存在することです。各境界レジスタはセグメント内の最大有効ページの値を保持する。たとえば、コードセグメントが最初の3つのページ(0,1,2)を使用している場合、コードセグメントページテーブルには3つのエントリが割り当てられ、境界レジスタは3に設定されます。セグメントの終わりを超えたメモリアクセスは例外を生成し、プロセスの終了につながる可能性があります。このように、我々のハイブリッド手法は、線形ページテーブルと比較して大幅なメモリ節約を実現する。スタックとヒープの間の未割り当てページは、ページテーブル内のスペースを占有しなくなりました(有効でないものとしてマークする)。

しかし、気付いているように、このアプローチは問題がないわけではありません。まず、セグメント化を使用する必要があります。以前に議論したように、セグメンテーションは、アドレス空間の特定の使用パターンを想定しているので、我々が望むほどフレキシブルではありません。たとえば、大規模ではあるがまばらなヒープがあると、ページテーブルの無駄が多くなります。第二に、このハイブリッドは外部断片化を再び引き起こします。ほとんどのメモリはページサイズの単位で管理されますが、ページテーブルは任意のサイズ(PTEの倍数)になります。したがって、メモリ内の空き領域を見つけることはより複雑です。これらの理由から、人々は小さなページテーブルを実装するためのより良い方法を模索し続けました。

>> TIP: USE HYBRIDS  
>> 2つの良いアイデアと反対のアイデアがある場合は、両方の世界のベストを達成するためにハイブリッドに組み込むことができるかどうかを常に確認する必要があります。ハイブリッドトウモロコシ種は、例えば、任意の天然に存在する種よりも頑強であることが知られています。もちろん、すべてのハイブリッドが良い考えではありません。

## 20.3 Multi-level Page Tables
別のアプローチではセグメンテーションに頼るのではなく、同じ問題を攻撃します。つまり、ページテーブル内のすべての無効な領域をメモリ内に保存するのではなく、どのように取り除くのでしょうか？このアプローチは、線形ページテーブルをツリーのようなものに変えるため、マルチレベルのページテーブルと呼ばれています。このアプローチは、現代の多くのシステム(例えば、x86 [BOH10])を採用するほど効果的です。ここで、このアプローチについて詳しく説明します。

マルチレベルのページテーブルの背後にある基本的な考え方は簡単です。まず、ページ・テーブルをページ・サイズの単位に切ります。ページテーブルエントリ(PTE)のページ全体が無効である場合、ページテーブルのそのページに対してはまったく割り当てません。ページテーブルのページが有効かどうか(有効な場合はメモリ内のどこにあるか)を追跡するには、ページディレクトリと呼ばれる新しい構造を使用します。したがって、ページディレクトリは、ページテーブルのページがどこにあるか、またはページテーブルのページ全体に有効なページが含まれていないことを通知するために使用できます。

![](../20/img/fig20_3.PNG)

図20.3に例を示します。図の左側には古典的な線形ページテーブルがあります。アドレス空間の中間領域の大部分が有効ではないにもかかわらず、これらの領域に割り当てられたページテーブルスペース(すなわち、ページテーブルの中央の2ページ)が依然として必要です。右側には、複数レベルのページテーブルがあります。ページディレクトリは、ページテーブルの2ページだけを有効(最初と最後)としてマークします。したがって、ページテーブルの2つのページだけがメモリに存在します。したがって、マルチレベルテーブルが何をしているのかを視覚化する方法の1つを見ることができます。線形ページテーブルの一部を消して(他の用途ではそれらのフレームを解放して)、ページテーブルのどのページを割り当てるかを追跡します。

単純な2レベルの表のページディレクトリには、ページテーブルのページごとに1つのエントリが含まれています。これは、多数の page directory entries (PDE)で構成されています。PDE(最小限)は有効ビットとpage frame number(PFN)を持ち、PTEに似ています。しかし、上記で示したように、この有効ビットの意味はわずかに異なってます。PDEエントリが有効であれば、(PFNを介して)エントリがポイントするページテーブルの少なくとも1つのページが有効であり、このPDEが指し示すそのページ上の少なくとも1つのPTEにおいて、そのPTE内の有効ビットは1にセットされる。PDEエントリの有効ビットが0の場合、PDEは定義されません。

多レベルのページテーブルはこれまで見てきたアプローチに比べて明らかな利点がいくつかあります。最初に、おそらく最も明白なことに、マルチレベル・テーブルは、使用しているアドレス空間の量に比例したページ・テーブル・スペースのみを割り当てます。したがって、一般にコンパクトで、省メモリなアドレス空間をサポートします。

第2に、慎重に構築された場合、ページテーブルの各部分はページ内にきれいに収まるため、メモリの管理が容易になります。OSは、ページテーブルを割り当てたり、拡張したりする必要がある場合、次の空きページを簡単に取得できます。これを単純な(ページングされていない)線形ページテーブルと比較してください。これはVPNによってインデックスされたPTEの配列です。このような構造では、線形ページテーブル全体が物理メモリに連続して存在しなければいけません。大きなページテーブル(例えば4MB)では、未使用の連続した空き物理メモリのような大きなチャンクを見つけることは非常に困難です。マルチレベル構造では、ページテーブルの部分を指すページディレクトリを使用して間接レベルを追加します。そのため、物理メモリに必要な場所にページテーブルページを置くことを可能にします。

>> TIP: UNDERSTAND TIME-SPACE TRADE-OFFS  
>> データ構造を構築する際には、構築時に時間空間のトレードオフを常に考慮する必要があります。通常、特定のデータ構造へのアクセスを高速化したい場合は、その構造体に対してメモリを多く使用するペナルティを支払わなければなりません。  
マルチレベルテーブルには省メモリの代わりに高速化を犠牲にするコストがかかります。TLBミスでは、メモリからの2つのロードが、ページテーブル(ページディレクトリ用と、PTE用)から適切な変換情報を得るために必要となります。したがって、マルチレベルテーブルは、時間空間のトレードオフの小さな例です。私たちはもっと小さなテーブルを望みそれを手に入れましたが、トレードオフがあります。一般的なケース(TLBヒット)ではパフォーマンスは明らかに同一ですが、TLBミスはこの小さなテーブルではコストが高くなります。
別の問題点は、複雑さです。ページテーブル参照(TLBミス時)を処理するのがハードウェアであれ、OSであれ、間違いなく単純な線形ページテーブル参照よりも複雑です。多くの場合、パフォーマンスを向上させたり、メモリを削減したりするために、複雑さを増やすことになります。マルチレベルテーブルの場合、貴重なメモリを節約するために、ページテーブルのルックアップがより複雑になっています。

### A Detailed Multi-Level Example
マルチレベルのページテーブルの背後にある考え方をよりよく理解するために、例を挙げてみましょう。64KBのページで、サイズが16KBの小さなアドレス空間を想像してみてください。したがって、VPN用に8ビット、オフセット用に6ビットの14ビットの仮想アドレス空間があります。線形ページテーブルは、たとえアドレス空間のごく一部が使用されているとしても、28(256)のエントリを持ちます。図20.4にそのようなアドレス空間の一例を示します。

![](../20/img/fig20_4.PNG)

>>TIP: BE WARY OF COMPLEXITY  
>>システム設計者は、システムに複雑さを加えることに注意する必要があります。優れたシステム構築者は、手元にあるタスクを達成する最も簡単なシステムを実装しています。たとえば、ディスク容量が豊富な場合は、できるだけ数バイトで使用しにくいファイルシステムを設計しません。同様に、プロセッサが高速であれば、CPU内に最適化された手作業で作成された作業用のコードよりも、クリーンでわかりやすいモジュールをOSに書き込むほうがよいでしょう。早期に最適化されたコードや他の形式で、不必要な複雑さに注意してください。このようなアプローチにより、システムの理解、維持、デバッグが難しくなります。Antoine de Saint-Exuperyは次のように書いています。「完璧は、もはや何も追加する必要がなくなったときではなく、もはや取り去るものがなくなったときでもありません」彼は「実際に目的を達成するよりも完璧と言うのは簡単である。」と書いていませんでした。

この例では、仮想ページ0と1はコード用、仮想ページ4と5はヒープ用、仮想ページ254と255はスタック用です。アドレス空間の残りのページは使用されません。

このアドレス空間の2レベルのページテーブルを作成するには、完全な線形ページテーブルから始め、ページサイズの単位に分割します。私たちの全テーブル(この例では)には256個のエントリがあります。各PTEが4バイトのサイズであると仮定します。したがって、私たちのページテーブルは1KB(256×4バイト)のサイズです。64バイトのページがある場合、1KBのページテーブルは16の64バイトページに分割できます。各ページは16個のPTEを保持できます。

ここで理解する必要があるのは、VPNを使って、まずページディレクトリに、次にページテーブルのページにインデックスを付ける方法です。それぞれがエントリの配列であることを忘れないでください。したがって、私たちが把握する必要があるのは、それぞれのVPNからインデックスを作成する方法だけです。

最初にページディレクトリにインデックスを作成しましょう。この例のページテーブルは小さく、16ページにわたる256エントリです。ページディレクトリには、ページテーブルのページごとに1つのエントリが必要です。従って、それは16のエントリーを有する。その結果、ディレクトリにインデックスを作成するには、VPNの4ビットが必要です。次のように、VPNの上位4ビットを使用します。

![](../20/img/fig20_4_1.PNG)

page directory index(略してPDIndex)は、VPNからpage directory entry (PDE)を取り出すために使われます。使われる計算式は、PDEAddr = PageDirBase +(PDIndex * sizeof(PDE))となります。これにより、ページディレクトリが作成されます。ここでは、変換の進捗状況を確認します。

ページディレクトリのエントリが無効とマークされている場合、アクセスが無効であることがわかり、例外が発生します。しかし、PDEが有効な場合は、さらに多くの作業が必要です。具体的には、このページエントリエントリが指すページテーブルのページからページテーブルエントリ(PTE)をフェッチする必要があります。このPTEを見つけるには、VPNの残りのビットを使用してページテーブルの部分にインデックスを設定する必要があります。

![](../20/img/fig20_4_2.PNG)

このページテーブルインデックス(略してPTIndex)を使用して、ページテーブル自体にインデックスを付けて、PTEのアドレスを指定できます。
```
PTEAddr = (PDE.PFN << SHIFT) + (PTIndex * sizeof(PTE))
```
page directory entry (PDE)から取得したpage frame number(PFN)は、PTEのアドレスを形成するためにページ・テーブル索引と組み合わせる前に、左シフトして配置する必要があることに注意してください。

これがすべて意味をなさないかどうかを確認するために、いくつかの実際の値を持つ複数レベルのページテーブルを記入し、1つの仮想アドレスを変換します。この例のページ・ディレクトリから始めましょう(図20.5の左側)。この図では、各page directory entry (PDE)がアドレス・スペースのページ・テーブルのページについて何かを記述していることが分かります。 この例では、アドレス空間に(開始時と終了時に)2つの有効な領域と、その間にいくつかの無効なマッピングがあります。

物理ページ100(ページテーブルの0番目のページの物理フレーム番号)には、アドレス空間内の最初の16個のVPN用の16個のページテーブルエントリの最初のページがあります。ページテーブルのこの部分の内容については、図20.5(中央部)を参照してください。

![](../20/img/fig20_5.PNG)

ページテーブルのこのページには、最初の16個のVPNのマッピングが含まれています。この例では、VPN 0と1が有効であり(コードセグメント)、4と5(ヒープ)です。したがって、テーブルは、これらのページのそれぞれについてのマッピング情報を持っています。残りのエントリは無効とマークされます。ページテーブルの他の有効なページは、PFN 101内にある。このページは、アドレス空間の最後の16個のVPNのマッピングを含みます。詳細は図20.5(右)を参照してください。

この例では、VPN 254と255(スタック)は有効なマッピングを持っています。うまくいけば、この例からわかるように、マルチレベルの索引構造でどれくらいのスペースを節約できるかということです。この例では、線形ページテーブルに16ページすべてを割り当てるのではなく、ページディレクトリに1つ、ページテーブルのチャンクに有効なマッピングが2つずつ割り当てます。大きな(32ビットまたは64ビット)アドレス空間の節約は大きく働くでしょう。

最後に、この情報を使用して変換を実行してみましょう。ここには、VPN 254の0番目のバイト：0x3F80、または11 11000 1000 0000のバイナリを参照するアドレスがあります。

VPNの上位4ビットを使用してページディレクトリにインデックスを作成することを思い出してください。したがって、1111は、上記のページディレクトリの最後のエントリ(0番目から15番目のエントリ)を選択します。これは、アドレス101に位置するページテーブルの有効なページを示します。次に、VPNの次の4ビット(1110)を使用して、ページテーブルのそのページにインデックスを付け、欲しいPTEを見つます。1110は、ページ上の次の最後(14番目)のエントリであり、仮想アドレス空間のページ254が物理ページ55にマップされていることを示しています。PFN = 55(または16進数0x37)をoffset = 000000に連結すると、欲しい物理アドレスを形成し、その要求をメモリシステムに発行することができます。
```
PhysAddr = (PTE.PFN << SHIFT) + offset
= 00 1101 1100 0000 = 0x0DC0.
```
ページテーブルのページを指すページディレクトリを使用して、2レベルのページテーブルを構築する方法についていくつか考えてください。残念なことに、ここで説明するように、ページテーブルの2つのレベルでは不十分な場合があります。

### More Than Two Levels
これまでの例では、複数レベルのページテーブルはページディレクトリとページテーブルの2つのレベルしか持たないと仮定しています。場合によっては、より深いツリーが可能です(実際には必要です)。簡単な例を取り上げて、より深いマルチレベルテーブルが役立つ理由を説明しましょう。この例では、30ビットの仮想アドレス空間と小さな(512バイト)ページがあるとします。したがって、仮想アドレスには21ビットの仮想ページ番号コンポーネントと9ビットのオフセットがあります。

マルチレベルのページテーブルを構築することの目標を忘れないでください。ページテーブルの各部分を1ページに収めるようにします。これまでは、ページテーブル自体についてのみ検討してきました。ただし、ページディレクトリが大きすぎるとどうなりますか？

ページテーブルのすべての部分をページ内に収めるために、複数レベルのテーブルに必要なレベルの数を決定するには、ページ内にいくつのページテーブルエントリが収まるかを判断することから始めます。ページサイズが512バイトで、PTEサイズが4バイトであると仮定すると、1ページに128個のPTEを収めることができます。ページテーブルのページにインデックスを付けると、VPNの最下位7ビット(log2128)がインデックスとして必要であると判断できます。

![](../20/img/fig20_5_1.PNG)

上記の図から気づくかもしれないのは、(大きな)ページディレクトリに残っているビット数です。ページディレクトリのエントリが2^14であれば、ページは1ページではなく128ページになります。ページに収まる複数レベルのページテーブルが消えます。

この問題を解決するために、ページディレクトリ自体を複数のページに分割し、その上に別のページディレクトリを追加して、ページディレクトリのページを指すようにして、ツリーのレベルをさらに高めます。したがって、仮想アドレスを次のように分割することができます。

![](../20/img/fig20_5_2.PNG)

現在、上位レベルのページディレクトリのインデックスを作成するときには、仮想アドレスの最上位ビット(図のPDインデックス0)を使用します。この索引を使用して、top level page directoryから page directory entryをフェッチすることができます。有効な場合、トップレベルPDEからの物理フレーム番号とVPNの次の部分(PDインデックス1)を組み合わせて、ページディレクトリの第2レベルを調べます。最後に、有効であれば、PTEアドレスは、第2レベルのPDEからのアドレスと組み合わされたページテーブルインデックスを使用することによって形成することができます。

### The Translation Process: Remember the TLB
2レベルのページテーブルを使用してアドレス変換のプロセス全体を要約すると、アルゴリズムフロー形式で制御フローを示します(図20.6)。この図は、すべてのメモリ参照時にハードウェアで何が起こるかを示しています(ハードウェア管理TLBを前提としています。

図からわかるように、複雑なマルチレベル・ページ・テーブル・アクセスが発生する前に、ハードウェアはまずTLBをチェックします。ヒットすると、物理アドレスは、以前と同様にページテーブルに全くアクセスせずに直接形成される。TLBミス時にのみ、ハードウェアは完全なマルチレベルルックアップを実行する必要があります。このパスでは、従来の2レベルページテーブルのコストを確認できます。有効な変換を検索するための2つの追加メモリアクセスです。

![](../20/img/fig20_6.PNG)

## 20.4 Inverted Page Tables
反転したページテーブルを使用すると、ページテーブルの世界でさらに極端なスペース節減が見られます。ここでは、多くのページテーブル(システムのプロセスごとに1つ)を持つ代わりに、システムの物理ページごとに1つのページテーブルを保持しています。このエントリは、どのプロセスがこのページを使用しているか、およびそのプロセスのどの仮想ページがこの物理ページにマッピングされているかを示します。

正しいエントリを見つけることは、このデータ構造を通して検索することになります。線形スキャンは高価であり、検索を高速化するためにハッシュテーブルが基本構造上に構築されることが多いです。PowerPCはそのようなアーキテクチャ[JM98]の一例です。

より一般的には、逆ページテーブルは、最初から述べたことを示しています。ページテーブルは単なるデータ構造です。あなたは、データ構造を使ってたくさん面白いことをすることができます。小さくても大きくても、遅くても速くしても構いません。多段ページテーブルと逆ページテーブルは、できることの多くの2つの例に過ぎません。

## 20.5 Swapping the Page Tables to Disk
最後に、1つの最終的な仮定の緩和について議論する。ここまでは、カーネルが所有する物理メモリにページテーブルが存在すると仮定しています。しかし、ページテーブルのサイズを減らすための多くのトリックがあっても、メモリに入るには大きすぎる可能性があります。したがって、一部のシステムでは、このようなページテーブルをカーネル仮想メモリに配置するため、少しでもメモリが圧迫がされた場合に、システムがこれらのページテーブルの一部をディスクにスワップすることができます。これについては、今後の章(VAX/VMSのケーススタディ)で詳しく説明します。一度、ページをメモリ内外に移動する方法を理解したら、さらに詳しく説明します。

##20.6 Summary
実際のページテーブルの構築方法を見てきました。必ずしも線形配列ではなく、より複雑なデータ構造である必要があります。このようなテーブルは、時間と空間のトレードオフであり、テーブルが大きくなればなるほどTLBミスを高速に処理できるだけでなります。しかし、テーブルが小さくなればなるほど省メモリになりますがTLBミスが発生したら遅くなります。したがって、正しい構造の選択は、指定された環境の制約に強く依存します。

メモリが制約されたシステム(多くの古いシステムのように)では、小さな構造が理にかなっています。妥当な量のメモリを持ち、多数のページを積極的に使用する仕事量では、TLBミスの速度を上げる大きなテーブルが適切な選択肢になる可能性があります。ソフトウェア管理されたTLBを使用すると、データ構造全体がオペレーティングシステム革新者の喜びにつながります。どのような新しい構造を思いつくことができますか？どのような問題を解決しますか？これらの質問を考えて、オペレーティングシステム開発者だけが夢を見ることができる大きな夢を描いてみてください。

# 参考文献

[BOH10] “Computer Systems: A Programmer’s Perspective”  
Randal E. Bryant and David R. O’Hallaron  
Addison-Wesley, 2010  
We have yet to find a good first reference to the multi-level page table. However, this great textbook by Bryant and O’Hallaron dives into the details of x86, which at least is an early system that used such structures. It’s also just a great book to have.

[JM98] “Virtual Memory: Issues of Implementation”  
Bruce Jacob and Trevor Mudge  
IEEE Computer, June 1998  
An excellent survey of a number of different systems and their approach to virtualizing memory. Plenty of details on x86, PowerPC, MIPS, and other architectures.

[LL82] “Virtual Memory Management in the VAX/VMS Operating System”  
Hank Levy and P. Lipman  
IEEE Computer, Vol. 15, No. 3, March 1982  
A terrific paper about a real virtual memory manager in a classic operating system, VMS. So terrific, in fact, that we’ll use it to review everything we’ve learned about virtual memory thus far a few chapters from now.

[M28] “Reese’s Peanut Butter Cups”  
Mars Candy Corporation.  
Apparently these fine confections were invented in 1928 by Harry Burnett Reese, a former dairy farmer and shipping foreman for one Milton S. Hershey. At least, that is what it says on Wikipedia. If true, Hershey and Reese probably hated each other’s guts, as any two chocolate barons should.

[N+02] “Practical, Transparent Operating System Support for Superpages”  
Juan Navarro, Sitaram Iyer, Peter Druschel, Alan Cox  
OSDI ’02, Boston, Massachusetts, October 2002  
A nice paper showing all the details you have to get right to incorporate large pages, or superpages, into a modern OS. Not as easy as you might think, alas.

[M07] “Multics: History”  
Available: http://www.multicians.org/history.html  
This amazing web site provides a huge amount of history on the Multics system, certainly one of the most influential systems in OS history. The quote from therein: “Jack Dennis of MIT contributed influential architectural ideas to the beginning of Multics, especially the idea of combining paging and segmentation.” (from Section 1.2.1)

\newpage

# 21 Beyond Physical Memory: Mechanisms
ここまでは、アドレス空間は非現実的に小さく、物理メモリに収まると仮定しました。実際には、実行中のすべてのプロセスのアドレス空間がすべてメモリに収まると仮定しています。我々は今、これらの大きな仮定を緩和し、多数の並行して動作する大規模なアドレス空間をサポートしたいと仮定します。

これを行うには、メモリ階層に追加のレベルが必要です。ここまでは、すべてのページが物理メモリに存在すると仮定しています。しかし、大規模なアドレス空間をサポートするために、OSは現在大きな需要がないものはアドレス空間の一部を移動する場所が必要です。一般に、そのような場所の特徴は、メモリよりも容量が大きいことです。その結果、一般的に速度が遅くなります(速度が速ければメモリとして使用します)。現代のシステムでは、この役割は通常、ハードディスクドライブによって提供されます。したがって、私たちのメモリ階層では、容量が大きくて速度が遅いハードドライブが一番上にあり、容量が小さくて速いメモリは一番下にあります。

>> THE CRUX: HOW TO GO BEYOND PHYSICAL MEMORY  
>> OSはどのようにして、より大きい、より遅いデバイスを使って、大きな仮想アドレス空間の錯覚を透過的に提供することができますか？

1つ疑問を持っているかもしれません。それは、プロセスのために単一の大きなアドレス空間をサポートしたいのはなぜ？ということです。その答えは利便性と使いやすさです。アドレス空間が大きいと、プログラムのデータ構造に十分な余裕があれば心配する必要はありません。むしろ、必要に応じてメモリを割り当て、自然にプログラムを書くだけです。OSが提供する強力な錯覚です。メモリオーバーレイを使用していた旧式のシステムでは、プログラマは必要に応じてコードやデータを手動でメモリ内外に移動する必要がありました[D97]。関数の呼び出しやデータへのアクセスの前に、まずコードやデータをメモリに配置する必要があります。

>> ASIDE: STORAGE TECHNOLOGIES  
>> I/Oデバイスが実際にどのように後で動作するかについてもっと深く掘り下げて調べます(I/Oデバイスの章を参照)。もちろん、低速デバイスはハードディスクである必要はありませんが、FlashベースのSSDなど、よりモダンなものになる可能性があります。それらについても話します。現時点では、物理メモリよりも大きな非常に大きな仮想メモリの錯覚を構築するのに役立つ大きなデバイスと比較的遅いデバイスがあると仮定します。

単一のプロセスだけではなく、スワップ空間を追加することで、複数の同時に実行されているプロセスに対して、大きな仮想メモリの錯覚をOSがサポートできるようになります。初期のマシンはすべてのプロセスが必要とするすべてのページを一度に保持することができないため、マルチプログラミング(複数のプログラムを「すぐに」実行して、マシンをより良く利用する)は、ほとんどのページを交換する能力を要求しました。したがって、マルチプログラミングと使いやすさの組み合わせにより、物理的に利用可能なメモリよりも多くのメモリを使用することがサポートされるようになります。これは、現代のすべてのVMシステムが行うことです。

## 21.1 Swap Space
最初に行う必要のある作業は、ページを前後に移動するためにディスク上にスペースを確保することです。オペレーティングシステムでは、スワップスペースと呼ばれるスペースは、メモリの外にページをスワップして、ページをメモリ内にスワップするためです。したがって、OSはページサイズ単位でスワップ領域の読み書きを行うことができます。これを行うには、OSは特定のページのディスクアドレスを覚えておく必要があります。

スワップスペースのサイズは重要です。最終的には、特定の時間にシステムが使用できるメモリページの最大数を決定します。わかりやすくするために、今のところそれは非常に大きいものとしましょう。

ちょっとした例(図21.1)では、4ページの物理メモリと8ページのスワップ領域の小さな例を見ることができます。この例では、3つのプロセス(Proc 0、Proc 1、およびProc 2)が物理メモリを積極的に共有しています。しかし、3つのそれぞれは有効なページの一部のみをメモリに持ち、残りの部分はディスクのスワップ領域に配置します。4番目のプロセス(Proc 3)は、すべてのページをディスクにスワップアウトしているため、現在実行中ではありません。スワップのブロックは空きのままです。この小さな例からも、スワップスペースを使用すると、システムが実際よりも大きなメモリをまとめることができるようになります。

![](../21/img/fig21_1.PNG)

スワップスペースは、トラフィックをスワップするためのディスク上の唯一の場所ではありません。たとえば、プログラムバイナリ(たとえば、ls、または独自のコンパイルされたメインプログラム)を実行しているとします。このバイナリのコードページは、ディスク上に最初に発見され、プログラムが実行されるとメモリにロードされます(プログラムの実行開始時に一度に、現代システムでは一度に1ページずつ必要に応じてロードされます)。ただし、システムが他のニーズに合わせて物理メモリに空き領域を確保する必要がある場合は、後からでも、ファイルシステム内のディスク上にあるバイナリを再度スワップすることができるため、これらのコードページのメモリ領域を安全に再利用できます。

## 21.2 The Present Bit
ディスク上にスペースを確保したので、ディスクとのスワップページをサポートするために、いくつかのマシンをシステムに追加する必要があります。わかりやすくするために、ハードウェア管理TLBを備えたシステムを想定してみましょう。

まず、メモリ参照で何が起こるかを思い出してください。実行中のプロセスは、仮想メモリ参照(命令フェッチまたはデータアクセス用)を生成します。この場合、ハードウェアはメモリから目的のデータをフェッチする前に、それらを物理アドレスに変換します。

ハードウェアは最初に仮想アドレスからVPNを抽出し、TLBに一致(TLBヒット)をチェックし、ヒットした場合に結果の物理アドレスを生成してメモリからフェッチします。これは速い(追加のメモリアクセスを必要としない)ので、これは一般的なケースです。

VPNがTLBに見つからない場合(TLBミス)、ハードウェアは(ページテーブルベースレジスタを使用して)ページテーブルをメモリに配置し、VPNを使用してインデックスを特定し、このページのページテーブルエントリ(PTE)を検索します。ページが有効で物理メモリに存在する場合、ハードウェアはPTEからPFNを抽出し、それをTLBにインストールし命令を再試行します。今回はTLBヒットを生成します。ここまでは順調ですね。

しかし、ページをディスクにスワップさせたい場合は、さらにメカニズムを追加する必要があります。具体的には、ハードウェアがPTEを調べると、ページが物理メモリに存在しないことがあります。ハードウェア(またはソフトウェア管理のTLBアプローチにおけるOS)がこれを判断する方法は、現在のビットと呼ばれる各ページテーブルエントリ内の新しい情報によって行われます。現在のビットが1に設定されている場合、ページが物理メモリに存在し、すべてが上記のように進行することを意味します。ゼロに設定されている場合、ページはメモリにではなく、ディスクのどこかにあります。物理メモリにないページにアクセスする行為は、通常、ページフォルトと呼ばれます。

>> ASIDE: SWAPPING TERMINOLOGY AND OTHER THINGS  
>> 仮想メモリシステムの用語は、マシンやオペレーティングシステムでは多少混乱し、変わる可能性があります。例えば、ページフォールトは、より一般的には、何らかの種類のフォールトを生成するページテーブルへの参照にアクセスすることができることです。これは、ここで議論しているフォールトのタイプ、すなわちページが存在しないフォールトを含むことができます。しかし、不正なメモリアクセスを参照した場合はどうでしょう。確かに、プロセスの仮想アドレス空間にマップされたページへの法的なアクセス(当然のことながら物理メモリではない)を「障害」と呼ぶことは間違いありません。より正確には、それをページミスと呼ばれるべきです。しかし、プログラムが「ページフォールト」であると言うと、OSがディスクにスワップアウトした仮想アドレス空間の一部にアクセスしていることをよく意味します。
>> 私たちは、この動作が「障害」として知られるようになった理由は、それを処理するオペレーティングシステムのメカニズムに関係していると考えています。めったに起きない何かが起こったとき、すなわち、ハードウェアが何らかの処理方法を知らないとき、ハードウェアは制御をOSに移し、処理してくれることを望むでしょう。この場合、プロセスがアクセスしたいページがメモリから欠落しています。ハードウェアが可能なのは例外を発生させ、そこからOSが引き継ぎます。これは、プロセスが何らかの違法行為を行った場合と同じであるため、アクティビティを「障害」と呼ぶのは驚くことではありません。

ページフォルトが発生すると、OSはページフォールトを処理するために呼び出されます。ここで説明するように、ページフォルトハンドラと呼ばれる特定のコードが実行され、ページフォルトを処理する必要があります。

## 21.3 The Page Fault
TLBミスでは、ハードウェア管理されたTLB(ハードウェアがページテーブルを参照して目的の変換を検索する)とソフトウェア管理TLB(OSが実行する場所)の2種類のシステムがあります。いずれのタイプのシステムにおいても、ページが存在しない場合、ページフォールトを処理するためにOSが担当します。適切に指定されたOSページフォルトハンドラが実行され、何をすべきかが決定されます。事実上、すべてのシステムがソフトウェアのページフォルトを処理します。ハードウェア管理のTLBを使用しても、ハードウェアはこの重要な義務を管理するためにOSを信頼します。

ページが存在せず、ディスクにスワップされている場合、OSはページフォールトを処理するためにページをメモリにスワップする必要があります。ここに疑問が生じます。OSは、どのようにして目的のページを見つけるかを知っていますか？多くのシステムでは、ページテーブルはそのような情報を格納するための自然な場所です。一般的なOSの場合は、ページのPFNなどのデータに通常使用されるPTEのビットをディスクアドレスとして使用できます。OSがページのページフォールトを受信すると、OSはPTEを調べてアドレスを見つけ、ディスクに要求を発行してページをメモリにフェッチします。

>> ASIDE: WHY HARDWARE DOESN’T HANDLE PAGE FAULTS  
>> TLBを学んだ経験から、ハードウェア設計者は、OSに任せるのを嫌っています。では、なぜ彼らはOSにページ違反を処理するのを信頼しているのでしょうか？主な理由はいくつかあります。まず、ディスクへのページフォルトは遅いです。OSが大量の命令を実行してフォールトを処理するのに時間がかかる場合でも、ディスク操作そのものは伝統的に遅すぎて、逆に実行中のソフトウェアの様々なオーバーヘッドを最小限に抑えられます。次に、ページフォルトを処理できるようにするために、ハードウェアはスワップ領域、I/Oをディスクに発行する方法、および現在はあまり知られていないその他多くの詳細を理解する必要があります。したがって、パフォーマンスとシンプルさの両方の理由から、OSはページフォールトを処理し、ハードウェアタイプも満足できるものになります。

ディスクI/Oが完了すると、OSはページテーブルを更新してページを現在のものとしてマークし、ページテーブルエントリ(PTE)のPFNフィールドを更新して、新たにフェッチされたページのメモリ内の位置を記録し、命令を再試行します。この次の試行は、TLBミスを生成し、サービスされ、TLB変換を更新します(このステップを回避するためにページフォールトを処理するときにTLBを交互に更新することができます)。最後に、最後の再起動はTLB内の変換を見つけ、変換された物理アドレスのメモリから望んだデータまたは命令をフェッチすることに進みます。

I/Oが実行されている間は、プロセスはブロックされた状態になります。従って、ページフォールトが処理されている間に、OSは他の準備完了プロセスを自由に実行することができる。I/Oは高価なので、あるプロセスのI/O(ページフォールト)と他のプロセスの実行とのオーバーラップは、マルチプログラムされたシステムがそのハードウェアを最も効果的に使用できるもう1つの方法です。

## 21.4 What If Memory Is Full?
上記のプロセスでは、スワップ領域からページをページインするための空きメモリが十分にあると想定していることがわかります。もちろん、これは当てはまらないかもしれません。つまり、メモリがいっぱい(またはそれに近い)かもしれません。したがって、OSは、最初に1つまたは複数のページをページアウトして、そのOSが投入しようとしている新しいページのためのスペースを作りたいと思うかもしれません。プロセスを選んでキックアウトまたは置換することをページ置換ポリシーといいます。

上記のように、間違ったページを見つけキックアウトや置換をすると、プログラムのパフォーマンスに大きなコストをかけることになります。そのため、良いページ置換ポリシーを作成することに多くの考えがあります。間違った決定をすると、プログラムはメモリのようなスピードではなく、ディスクのようなスピードで動作する可能性があります。現在の技術では、プログラムが1万分の1倍または10万分の1倍遅く実行される可能性があります。したがって、そのようなポリシーは我々がある程度詳細に研究すべきでしょう。それはまさに次の章でやることです。今のところ、ここに記載されているメカニズムの上に構築されたこのようなポリシーが存在することを理解するだけで十分です。

## 21.5 Page Fault Control Flow
この知識がすべて整ったら、メモリアクセスの完全な制御フローを概略的にスケッチすることができます。言い換えれば、誰かが「プログラムがメモリからデータを取り込むときにどうなるか」と尋ねるとき、あなたはすべての異なる可能性についてかなり良い考えを持っているはずです。詳細は、図21.2および図21.3の制御フローを参照してください。最初の図は、変換中にハードウェアが何をするかを示し、2番目はページフォルト時にOSが行うことを示しています。

![](../21/img/fig21_2.PNG)

図21.2のハードウェア制御のフロー図から、TLBミスが発生したときを理解する重要な3つのケースがあることに注意してください。まず、ページが存在し、有効であったこと(18-21行目)。この場合、TLBミスハンドラは、単にPTEからPFNを取り込み、命令を再試行し(今度はTLBヒットを生じる)、前に説明したように(何度も)続行することができます。2番目のケース(22-23行目)では、ページフォルトハンドラを実行する必要があります。これはアクセスするプロセスの正当なページでしたが(結局のところ有効です)、物理メモリには存在しません。3番目(最後に)、プログラムのバグなどの理由で、無効なページにアクセスする可能性があります(行13-14)。この場合、PTEの他のビットは実際には重要ではありません。ハードウェアがこの無効なアクセスをトラップし、OSトラップハンドラが実行され、問題のプロセスが終了する可能性があります。

![](../21/img/fig21_3.PNG)

図21.3のソフトウェア制御フローから、ページフォルトを処理するためにOSが大まかに何をしなければならないかを見ることができます。第1に、OSは、間もなくフォールトインされるページが存在する物理フレームを見つける必要があります。そのようなページがない場合は、置換アルゴリズムが実行され、メモリからいくつかのページが蹴られるまで、メモリを解放するまで待たなければなりません。物理的なフレームがあれば、ハンドラはスワップ領域からページを読み込むためのI/O要求を発行します。最後に、その低速動作が完了すると、OSはページテーブルを更新して命令を再試行します。再試行によりTLBミスが発生し、次に別の再試行時にTLBヒットが発生し、その時点でハードウェアは所望のアイテムにアクセスすることができる。

## 21.6 When Replacements Really Occur
これまでのところ、置き換えがどのように行われるかを記述したところでは、OSはメモリが完全にいっぱいになるまで待ってから、他のページのためのスペースを作るためにページを置き換える(追い出す)だけです。しかし、これは少し現実的ではありません。OSがメモリの一部をもっと積極的に解放する理由はたくさんあります。

少量のメモリを確保するために、ほとんどのオペレーティングシステムは、メモリからページを取り出す開始時期を決定するために、ある種の上限値(HW)と下限値(LW)を持っています。これがどのように機能するかは次のとおりです。利用可能なLWページ数が少ないことをOSが認識すると、メモリの解放を担当するバックグラウンドスレッドが実行されます。スレッドは、利用可能なHWページがあるところまでページを退去させます。つまり、あるHWで設定している上限値を超えているページをHWに移動させます。また、あるLWで設定している下限値を下回っているページをLWに移動させます。スワップデーモンやページデーモンと呼ばれることもあるバックグラウンドスレッドは、実行中のプロセスやOSが使用するメモリを解放してくれたことをうかがってスリープ状態になります。

一度に多数の置換を実行することにより、新しいパフォーマンスの最適化が可能になります。たとえば、多くのシステムでは、いくつかのページをクラスタ化またはグループ化し、スワップパーティションに一度に書き出し、ディスクの効率を高めます[LL82]。ディスクをより詳細に説明するときに後で説明するように、このようなクラスタリングはディスクのシークおよびローテーションオーバーヘッドを減らし、パフォーマンスを大幅に向上させます。

バックグラウンドページングスレッドを処理するには、図21.3の制御フローを少し変更する必要があります。直接置換を実行するのではなく、代わりに、使用可能な空きページがあるかどうかを単純にチェックします。そうでない場合は、空きページが必要であることをバックグラウンドページングスレッドに通知します。スレッドがいくつかのページを解放すると、元のスレッドが再び呼び起こされ、それが目的のページにページングされ、その作業に進むことができます。

>> TIP: DO WORK IN THE BACKGROUND  
>> いくつかの作業がある場合、効率を高め、操作のグループ化を可能にするために、バックグラウンドで実行することをお勧めします。オペレーティングシステムはよくバックグラウンドで動作します。たとえば、実際にデータをディスクに書き込む前に、多くのシステムがファイル書き込みをメモリにバッファリングします。そうすることで多くの利点が得られます。ディスク効率が向上したディスクは一度に多くの書き込みを受け取ることができるため、ディスクをスケジュールできるようになります。書き込みの待ち時間が改善されます。書込みが決してディスクに行く必要がない場合(すなわち、ファイルが削除された場合)、作業の削減ができる可能性があります。システムがアイドル状態のときにバックグラウンド作業が行われる可能性があるため、ハードウェア[G+95]をより有効に活用できるため、アイドル時間の有効活用が可能になります。

## 21.7 Summary
この短い章では、システム内に物理的に存在するより多くのメモリにアクセスするという概念を導入しました。これを行うには、ページがメモリー内に存在するかどうかを知らせるために、現在のビット(ある種のもの)を含める必要があるため、ページテーブル構造の複雑になります。そうでない場合は、オペレーティングシステムのページフォルトハンドラがページフォールトを処理するために実行され、ディスクからメモリへの望んだページの移動を行います。おそらくメモリ内のいくつかのページを先に交換してすぐに、スワップインするためのスペースを確保します。

重要なことに、これらのアクションはすべてプロセスに対して透過的に行われます。プロセスに関する限り、それは自身のプライベートで連続した仮想メモリにアクセスしているだけです。背後では、ページは物理メモリ内の任意の(連続していない)場所に配置され、時にはメモリにも存在せず、ディスクからのフェッチが必要になることがあります。一般的なケースでは、メモリアクセスが高速であることが重要ですが、場合によってはそれを処理するために複数のディスク操作が必要になることもあります。最悪の場合、1つの命令を実行するように簡単な作業ですが、最悪の場合、その作業が完了するまでに数ミリ秒かかることがあります。

# 参考文献

[CS94] “Take Our Word For It”  
F. Corbato and R. Steinberg  
Available: http://www.takeourword.com/TOW146/page4.html  
Richard Steinberg writes: “Someone has asked me the origin of the word daemon as it applies to computing. Best I can tell based on my research, the word was first used by people on your team at Project MAC using the IBM 7094 in 1963.” Professor Corbato replies: “Our use of the word daemon was inspired by the Maxwell’s daemon of physics and thermodynamics (my background is in physics). Maxwell’s daemon was an imaginary agent which helped sort molecules of different speeds and worked tirelessly in the background. We fancifully began to use the word daemon to describe background processes which worked tirelessly to perform system chores.”

[D97] “Before Memory Was Virtual”  
Peter Denning  
From In the Beginning: Recollections of Software Pioneers, Wiley, November 1997  
An excellent historical piece by one of the pioneers of virtual memory and working sets.  

[G+95] “Idleness is not sloth”  
Richard Golding, Peter Bosch, Carl Staelin, Tim Sullivan, John Wilkes  
USENIX ATC ’95, New Orleans, Louisiana  
A fun and easy-to-read discussion of how idle time can be better used in systems, with lots of good examples.

[LL82] “Virtual Memory Management in the VAX/VMS Operating System”  
Hank Levy and P. Lipman  
IEEE Computer, Vol. 15, No. 3, March 1982  
Not the first place where such clustering was used, but a clear and simple explanation of how such a mechanism works.

\newpage

## 22 Beyond Physical Memory: Policies
仮想メモリマネージャでは、空きメモリが大量にある場合は簡単です。ページフォルトが発生した場合、フリーページリストに空きページがあり、それをフォールトページに割り当てます。

ほとんどのメモリが解放されれば、少し面白いことになります。このような場合、このメモリ圧迫により、OSは強制的に使用されるページのためのスペースを作るためにページをページアウトすることを開始します。追放するページ(またはページ)を決定することは、OSの置換ポリシー内にカプセル化されています。歴史的には、古いシステムでは物理メモリがほとんどなかったため、初期の仮想メモリシステムで最も重要な決定の1つでした。最小限にとどめておくと、これはもう少し知っておく価値がある面白いポリシーです。

>> THE CRUX: HOW TO DECIDE WHICH PAGE TO EVICT  
>> OSは、どのページ(またはページ)をメモリから退去させるかをOSはどのように決定できますか？この決定はシステムの置き換え方針によって行われます。システムの置き換え方針は、一般的な原則(下記参照)に従いますが、偏った場合の振る舞いを避けるための調整も含まれています。

## 22.1 Cache Management
ポリシーに入る前に、我々がまず解決しようとしている問題についてより詳しく説明します。メインメモリには、システム内のすべてのページのサブセットが格納されているため、システム内の仮想メモリページのキャッシュと見なすことができます。従って、このキャッシュのための置換ポリシーを選ぶことにおける我々の目標は、キャッシュミスの数を最小限にすること、すなわち、ディスクからページをフェッチする回数を最小にすることです。あるいは、キャッシュヒットの数、すなわちアクセスされたページがメモリ内に見つかった回数を最大にするという目標になります。

キャッシュヒット数とミス数を知ることで、プログラムのaverage memory access time(AMAT)を計算できます(メトリックコンピュータアーキテクトはハードウェアキャッシュを計算します[HP06])具体的には、これらの値を考慮して、次のようにプログラムのAMATを計算することができます。

![](../22/img/fig22_1_1.PNG)

ここで、TMはメモリにアクセスするコスト、TDはディスクにアクセスするコスト、PMはキャッシュ内のデータを見つけられない確率(ミス)を示します。PMissは0.0から1.0まで変化し、確率(例えば、10％のミス率はPMiss = 0.10を意味する)ではなく、パーセントミス率を参照することもあります。常にメモリ内のデータにアクセスするコストを支払うことに注意してください。さらに、見逃したときには、ディスクからデータを取り出すためのコストをさらに支払わなければなりません。

たとえば、(小さな)アドレス空間を持つマシン(4KB、256バイトページ)を想像してみましょう。したがって、仮想アドレスには、4ビットのVPN(最上位ビット)と8ビットのオフセット(最下位ビット)の2つのコンポーネントがあります。したがって、この例のプロセスは、24または16の合計仮想ページにアクセスできます。この例では、プロセスは、0x000、0x100、0x200、0x300、0x400、0x500、0x600、0x700、0x800、0x900という、次のメモリ参照(すなわち、仮想アドレス)を生成します。これらの仮想アドレスは、アドレス空間の最初の10個のページのそれぞれの最初のバイトを指します(ページ番号は各仮想アドレスの最初の16進数です)

さらに、仮想ページ3を除くすべてのページが既にメモリ内にあると仮定します。したがって、メモリ参照のシーケンスは、ヒット、ヒット、ヒット、ミス、ヒット、ヒット、ヒット、ヒット、ヒット、ヒットになります。ヒット率(メモリ内の参照の割合)を計算することができます。このとき、90％の参照がメモリに格納されているため、90％です。従って、ミス率は10％(PMiss = 0.1)です。一般に、PHit + PMiss = 1.0;ヒット率+ミス率合計を100％とします。

AMATを計算するには、メモリにアクセスするコストとディスクにアクセスするコストを知る必要があります。メモリ(TM)にアクセスするコストが約100ナノ秒であり、ディスク(TD)にアクセスするコストが約10ミリ秒であると仮定すると、100ns + 1ms、すなわち1.0001msである100ns + 0.1・10ms、約1ミリ秒です。ヒット率が99.9％(Pmiss = 0.001)だった場合、AMATは10.1マイクロ秒、つまり約100倍高速です。ヒット率が100％に近づくと、AMATは100ナノ秒に近づきます。

残念ながら、この例で分かるように、ディスクアクセスのコストは現代のシステムでは非常に高く、わずかなミス率でも実行中のプログラムのAMAT全体をすぐに支配します。できるだけ多くのミスを避けるか、ディスクの速度でゆっくりと実行する必要があります。これを手助けする1つの方法は、現在行っているように、優れたポリシーを慎重に開発することです。

## 22.2 The Optimal Replacement Policy
特定の置換ポリシーがどのように機能するかをよりよく理解するには、可能な限り最良の置換ポリシーと比較することをお勧めします。結局のところ、このような最適なポリシーは、何年も前にBeladyによって開発されました[B66](彼はもともとMINと呼ばれていた)最適な置換ポリシーは、全体として最小のミス数につながります。Beladyは、将来最も遠くにアクセスされるページを置き換えるシンプルな(しかし、残念なことに実装が難しい)アプローチが、最適なポリシーであり、結果としてキャッシュミスが最小限に抑えられることを示しました。

>> TIP: COMPARING AGAINST OPTIMAL IS USEFUL  
>> 最適な方針はあまり実用的ではありませんが、シミュレーションや他の研究の比較点としては非常に有用です。あなたが何も比較せずに派手な新しいアルゴリズムが80％のヒット率を持っているとしても意味がないということです。最適は82％のヒット率を達成すると言います(あなたの新しいアプローチは最適に非常に近いです)そのため、最適なものが何であるかを知ることで、より良い比較を実行し、改善の可能性がどれくらいあるのか、理想的なポリシーの改善に近づくことができます[AD03]。

うまくいけば、最適なポリシーの背後にあるものは理にかなっています。それについて考えてみましょう。あなたがいくつかのページを投げ捨てなければならない場合、最もアクセスする将来が遠いページを投げ捨てませんか？そうすることで、本質的に、キャッシュ内の他のすべてのページが、最も遠いページよりも重要であるということになります。これが当てはまる理由は簡単です。最も遠いものを参照する前に、他のページを参照します。

最適なポリシーがもたらす決定を理解するための簡単な例をたどってみましょう。プログラムは、0,1,2,0,1,3,0,3,1,2,1の仮想ページの次のストリームにアクセスすると仮定します。図22.1は、3ページに収まるキャッシュを想定した最適な動作を示しています。

![](../22/img/fig22_1.PNG)

この図では、次の操作を確認できます。最初の3回のアクセスは、キャッシュが空の状態で開始するため、ミスをしています。このようなミスはコールドスタートミス(または強制ミス)と呼ばれることがあります。次に、キャッシュ内でヒットしたページ0と1を再度参照します。最後に、別のミス(3ページ目)に達しますが、今回はキャッシュがいっぱいです。つまり、交換が行われなければいけません。どちらのページを置き換えなければならないのか疑問に思うでしょう。最適なポリシーでは、現在キャッシュ内にある各ページ(0,1,2)を調べ、0がほぼ即時にアクセスされ、1が少し後にアクセスされ、2が将来最も後にアクセスされることを確認します。つまり、ページ2を退かせて、キャッシュ内のページ0,1,3を生成します。次の3つの参考文献はヒットですが、退去したページ2のミスで苦しんでいます。ここで、最適なポリシーは、キャッシュ内の各ページ(0,1、および3)を調べ、ページ1(アクセスしようとしている)を追い出さない限り、問題ありません。この例では、ページ3が削除されていることが示されていますが、0でも問題ありません。最後に、1ページ目でヒットし、トレースが完了しました。

>> ASIDE: TYPES OF CACHE MISSES  
>> コンピュータアーキテクチャーの世界では、設計者はタイプごとにミスを3つのカテゴリの1つに特徴付けることが有用であることを示しています。compulsory、capacity、conflictのスリーC [H87]と呼ばれることがあります。一つ目の強制的なミス(またはコールドスタートミス[EF78])が発生するのは、キャッシュが最初から空であり、これがアイテムへの最初の参照であるためです。二つ目の容量不足は、キャッシュの容量が足りなくなり、新しいアイテムをキャッシュに持ち込むためにアイテムを追い出す必要があるために発生します。三つ目の競合ミスは、ハードウェアキャッシュ内にアイテムを置くことができる限界があるため、ハードウェアで発生します。そのようなキャッシュは常に完全に連想的です。つまり、メモリ内のどこにページを置くことができるかに制限がないので、OSページ・キャッシュ内では発生しません。詳細はH＆Pを参照してください[HP06]。

キャッシュのヒット率も計算できます。ヒット率は6ヒットと5ヒットで、ヒット率は(ヒット)/(ヒット+ミス)で、(6)/(6 + 5)または54.5％です。ヒット率を法とする強制ミスを計算することもできます(つまり、特定のページへの最初のミスを無視する)。その結果、ヒット率は85.7％になります。

残念ながら、以前にスケジューリング方針の策定において見た時と同じように、ページの将来は一般的にわかりません。汎用オペレーティングシステムの最適なポリシーを構築することはできません。したがって、実際の展開可能なポリシーを開発する際に、どのページを退去させるかを決める他の方法を見つけるアプローチに焦点を当てます。

## 22.3 A Simple Policy: FIFO
多くの初期のシステムでは、最適かつ雇用された非常に単純な代替ポリシーに近づくことの複雑さが回避されました。たとえば、一部のシステムでは、FIFO(先入れ先出し)置換が使用されました。ここでは、ページはシステムに入るときに単にキューに入れられます。置換が行われると、キューの末尾のページ(「ファーストイン」ページ)が追い出されます。FIFOには大きな強みがあります。実装が非常に簡単です。

![](../22/img/fig22_2.PNG)

サンプル参照ストリームでFIFOがどのように動作するかを調べてみましょう(図22.2)。ページ0、1、2への3回の強制ミスでトレースを開始し、0と1の両方でヒットします。次に、ページ3が参照され、ミスが発生します。置換の決定はFIFOで簡単です。「最初のもの」であったページを選択します(図のキャッシュ状態はFIFO順で、最初のページは左にあります)。これはページ0です。残念ながら、次のアクセスはページ0へのものであり、別のミスと置換(ページ1の)が発生します。その後、3ページ目にヒットしましたが、1と2でミスして、最終的に3になりました。

FIFOを最適値と比較すると、FIFOは著しく悪化します。つまり、ヒット率は36.4％(強制ミスを除くと57.1％)です。FIFOは単にブロックの重要性を判断することはできません。たとえページ0が何度もアクセスされたとしても、FIFOはメモリに取り込まれた最初のものだったからです。

>> ASIDE: BELADY’S ANOMALY  
>> Belady(最適政策の)と同僚たちは、予想外に行動した興味深い参照ストリームを見つけました[BNS69]。メモリ参照ストリームが1,2,3,4,1,2,5,1,2,3,4,5だったとしましょう。彼らが研究していたのは、キャッシュ・サイズが3から4ページに変更されたときに、キャッシュ・ヒット率がどのように変化したかです。一般に、キャッシュが大きくなると、キャッシュ・ヒット率が向上する(向上する)ことが期待されます。しかし、この場合、FIFOでは、悪化します！この行動は、一般的にBelady's Anomaly(彼の共著者の賛辞)と呼ばれています。  
LRUなどの他のポリシーは、この問題を抱えていません。なぜでしょうか？結論として、LRUにはスタックプロパティ[M+70]があります。このプロパティを持つアルゴリズムの場合、サイズN + 1のキャッシュには当然サイズNのキャッシュの内容が含まれます。したがって、キャッシュサイズを増やすと、ヒット率は変わらないか向上します。FIFOとランダム(とりわけ)は明らかにスタックのプロパティに従わず、したがって異常な動作の影響を受けやすいです。

![](../22/img/fig22_3.PNG)

## 22.4 Another Simple Policy: Random
もう1つの同様の置換ポリシーはRandomです。これはメモリ不足のもとで置換するランダムなページを選択するだけです。ランダムはFIFOに似た性質を持っています。実装するのは簡単ですが、どのブロックを取り除くことを考えると最適ではありません。私たちの有名な例のリファレンスストリームでRandomがどうなるかを見てみましょう(図22.3を参照)

もちろん、ランダムはどのように幸運な(または不運な)ランダムがその選択肢に入るかに完全に依存します。上記の例では、RandomはFIFOより少し良く、最適より少し劣っています。実際には、無作為実験を何千回も実行し、それがどのように一般的に行うかを決定することができます。図22.4は、無作為のシード値が異なる10,000件の試行に対して、無作為に達成したヒット数を示しています。あなたが見ることができるように、時には(時間のわずか40％を過ぎて)、ランダムは最適なほど良く、サンプルのトレースで6ヒットを達成します。時にはそれは2ヒット以下を達成するなど、さらに悪化する場合もあります。ランダムは抽選の運勢によって決まります。

![](../22/img/fig22_4.PNG)

## 22.5 Using History: LRU
残念なことに、FIFOやランダムなどの単純なポリシーは共通の問題を抱えている可能性があります。重要なページが再度参照される可能性があります。FIFOは、最初に持ち込まれたページをキックアウトします。これが重要なコードやデータ構造を持つページである場合、そのページはまもなくページングされますが、追い出されます。したがって、FIFO、ランダムなどのポリシーは最適に近づきそうにありません。よりスマートなものが必要です。

スケジューリング方針で行ったように、将来の推測を改善するために、履歴を見てみましょう。たとえば、プログラムが近い過去にページにアクセスした場合、近い将来にもう一度そのページにアクセスする可能性があります。

ページ置換ポリシーが使用できる履歴情報の1つのタイプは頻度です。ページが何度もアクセスされている場合は、明らかに何らかの価値があるので、置き換えてはいけません。もう一つは、アクセスの最新性です。より最近ページにアクセスした場合、おそらくそれが再びアクセスされる可能性が高くなります。

この一連のポリシーは、人々が地域の原則[D70]として言及しているものに基づいており、基本的にはプログラムとその行動についての単なる見解です。この原理は、プログラムがあるコードシーケンス(例えば、ループ内)およびデータ構造(例えば、ループによってアクセスされる配列)にかなり頻繁にアクセスする傾向があることを簡単に示しています。したがって、どのページが重要であるかを把握するために履歴を使用して、そのページを追い出し時にメモリに保存しておく必要があります。

![](../22/img/fig22_5.PNG)

そして、歴史的に単純なアルゴリズムのファミリーが生まれました。最小使用頻度の高い(LFU)ポリシーは、退去が発生しなければならないときに最も頻繁に使用されないページを置き換えます。同様に、LRU(Least Recently Used)ポリシーは、最も最近使用されたページを置き換えます。これらのアルゴリズムは名前を知ると、それが何をするのか正確に分かります。これは名前にとって優れた特性です。LRUをよりよく理解するために、LRUがサンプルの参照ストリームでどのように動作するかを調べてみましょう。図22.5に結果を示します。この図から、LRUがランダムまたはFIFOなどの履歴のような状態がないポリシーよりも優れた処理を行うために、履歴をどのように使用できるかがわかります。この例では、0と1が最近アクセスされたため、LRUは最初にページを置換する必要があるときにページ2を退去させます。1と3が最近アクセスされたため、ページ0が置き換えられます。どちらの場合も、履歴に基づくLRUの決定は正しいと判明し、次の参照はヒットします。したがって、単純な例では、LRUはパフォーマンスを最適にするためにできるだけ多くのことを行います。我々はまた、これらのアルゴリズムの対立するものが存在います。それは、Most Frequently Used(MFU)およびMost Recently Used(MRU)です。ほとんどの場合(すべてではありません)、これらのポリシーは、ほとんどのプログラムがそれを採用するのではなく局所性(キャッシュの状態)を無視するため、うまく機能しません。

>> ASIDE: TYPES OF LOCALITY  
>> プログラムが出現する傾向がある地域には2つのタイプがあります。一つは空間的局所性(spatial locality)として知られており、ページPがアクセスされた場合、その周辺のページ(例えば、P-1またはP + 1)もアクセスされる可能性が高いです。二つ目は、時間的局所性です。アクセスされたページは、近い将来再びアクセスされる可能性があります。このようなローカル性が存在すると仮定すると、ハードウェアシステムのキャッシュ階層で大きな役割を果たします。命令、データ、アドレス変換キャッシュのレベルをさまざまに配備して、局所性が存在する場合にプログラムを高速に実行できます。もちろん、局所性の原則は、すべてのプログラムが従わなければならない厳しい規則ではありません。実際、一部のプログラムは、メモリ(またはディスク)にむしろランダムにアクセスするため、アクセスストリームに少しも局所性がありません。したがって、あらゆる種類のキャッシュ(ハードウェアまたはソフトウェア)を設計する際に局所性を覚えておくことは良いことですが、成功を保証するものではありません。むしろ、それはコンピュータシステムの設計において有用であることがよく証明されるのがヒューリスティックです。

## 22.6 Workload Examples
これらのポリシーのいくつかの動作をよりよく理解するために、例を見てみましょう。ここでは、小さなトレースではなく、より複雑な仕事量を調べます。しかし、これらの仕事量さえも大幅に単純化されます。より良い研究にはアプリケーショントレースが含まれます。

私たちの最初の仕事量には局所性がありません。つまり、各参照はアクセスされたページのセット内のランダムなページになります。この単純な例では、仕事量は時間の経過とともに100のユニークなページにアクセスし、ランダムに参照する次のページを選択します。全体的に10,000ページがアクセスされます。実験では、各ポリシーがどのキャッシュサイズの範囲でどのように動作するかを確認するために、キャッシュサイズを非常に小さい(1ページ)からすべての固有ページ(100ページ)を保持するのに十分に変更しています。

![](../22/img/fig22_6.PNG)

図22.6は、最適、LRU、ランダム、およびFIFOの実験結果をプロットしたものです。図のy軸は、各ポリシーが達成するヒット率を示しています。x軸はキャッシュサイズを変更します。

グラフからいくつかの結論を導くことができます。まず、仕事量に局所性がない場合、どの現実的なポリシーを使用しているかは重要ではありません。LRU、FIFO、およびランダムはすべて、キャッシュのサイズによって正確に決定されるヒット率で同じ処理を行います。第2に、キャッシュが仕事量全体に適合するように十分な大きさであれば、どのポリシーを使用するかは重要ではありません。参照されたすべてのブロックがキャッシュに収まると、すべてのポリシー(ランダムでさえ)は100％のヒット率に収束します。最後に、最適化が現実的なポリシーよりも顕著に優れていることがわかります。可能であれば、将来を見て、より良い仕事を置き換えます。

![](../22/img/fig22_7.PNG)

次の仕事量は「80-20」仕事量と呼ばれ、局所性を示します。参照の80％はページの20％(「ホット」ページ)に作成されます。参照の20％はページの80％(「コールド」ページ)に作成されます。私たちの仕事量には、ユニークなページが合計100個あります。したがって、「ホット」ページはほとんどの時間に参照され、「コールド」ページは残りのページに参​​照されます。図22.7は、この仕事量でポリシーがどのように機能するかを示しています。図からわかるように、ランダムとFIFOの両方が合理的にうまくいく一方で、LRUはホットなページを保持する可能性が高いため、より良い結果を示します。それらのページは過去に頻繁に参照されているため、近い将来再び参照される可能性があります。LRUの履歴情報が完璧ではないことを示しています。

ここで疑問に思うかもしれません。ランダムとFIFOとLRUは本当にそれほど大きなトレードオフでしょうか？答えはいつものように「それは依存している」です。ミスが非常にコストがかかる場合、ヒット率のわずかな増加(ミス率の低下)でさえパフォーマンスに大きな違いをもたらす可能性があります。ミスがそれほどコストがかからない場合、もちろんLRUのメリットはそれほどありません。

最終的な仕事量を見てみましょう。これを「順序ループ」仕事量と呼びます。これは、50ページを順番に参照していきます。つまり、0から1、...、49ページまで順番に参照します。ループを繰り返して50ページへ合計10,000回のアクセスをします。図22.8の最後のグラフは、この仕事量でのポリシーの動作を示しています。

![](../22/img/fig22_8.PNG)

この仕事量は、多くのアプリケーション(データベース[CD85]などの重要な商用アプリケーションを含む)で一般的ですが、LRUとFIFOの両方で最悪のケースです。これらのアルゴリズムは、古いページを追い出します。そのため、仕事量がループする性質のため、これらの古いページは、将来使われるとしてもポリシーがキャッシュに保持しません。実際、サイズ49のキャッシュを使用しても、50ページのループ順の仕事量ではヒット率は0％になります。興味深いことに、ランダムなポリシーは著しく優れており、最適に近づいていませんが、少なくともゼロ以外のヒット率を達成しています。ランダムには素晴らしい性質があることがわかります。

## 22.7 Implementing Historical Algorithms
ご覧のように、LRUなどのアルゴリズムは、一般的に、FIFOやランダムなどの単純なポリシーよりも優れた処理を行いますが、重要なページを捨てる可能性があります。残念なことに、履歴のポリシーは私たちに新たな挑戦を提示します。

たとえば、LRUを取ってみましょう。完全に実装するには、多くの作業が必要です。具体的には、各ページアクセス(すなわち、各メモリアクセス、命令フェッチまたはロードまたはストア)に応じて、このページをリストの前部(すなわち、MRU側)に移動させるためにいくつかのデータ構造を更新しなければいけません。これをFIFOに対比すると、ページのFIFOリストは、ページが取り除かれたとき(最初のページを取り除くことによって)にアクセスされるとき、または新しいページがリストに追加されたとき(最後の側に)どのページが最小で最も最近に使用されたのかを把握するために、システムはすべてのメモリ参照に対していくつかのアカウンティング作業を行う必要があります。明らかに、細心の注意を払うことがありません。しかし、そのような一連の処理はパフォーマンスを大幅に低下させる可能があります。

これをスピードアップするのに役立つ方法の1つは、ハードウェアのサポートを少し追加することです。例えば、マシンは、各ページのアクセス時にメモリ内のtime fieldsを更新することができます(例えば、これはプロセス毎のページテーブル内にあってもよいし、メモリ内の別個の配列内にあってもよく、システムの物理ページ毎に1エントリ)。つまり、ページがアクセスされるとき、time fieldsはハードウェアによって現在の時間に設定されます。次に、ページを置換するとき、OSはシステム内のすべてのtime fieldsを単に走査して、最も最近に使用されたページを見つけることができます。

残念なことに、システム内のページ数が増えるにつれて、最も最近使用されていないページを見つけるために膨大な数のtime fieldsをスキャンするのは非常に高価です。4GBのメモリを搭載した最新のマシンを4KBのページに分けたと想像してください。このマシンには100万ページがあるため、最新のCPU速度であっても、LRUページの検索には長い時間がかかります。本当に交換する最も古いページを見つける必要があるのでしょうか？

>> CRUX: HOW TO IMPLEMENT AN LRU REPLACEMENT POLICY  
>>完璧なLRUを実装するのにはコストがかかることを考えるのであれば、何らかの方法で近似させることができますか？

## 22.8 Approximating LRU
結論としては、答えは「はい」です。計算上のオーバーヘッドの観点から、LRUを近似する方がより現実的であり、現代の多くのシステムではそうです。アイデアは、使用ビット(リファレンスビットと呼ばれることもあります)の形でハードウェアサポートを必要とします。最初のものは、ページング付きの最初のシステムで実装されたアトラスonelevel store [KE+62]です。システムの1ページあたり1ビットの使用ビットがあり、その使用ビットはどこかのメモリに存在します(たとえば、プロセスごとのページテーブル内、または配列のどこかにある可能性があります)。ページが参照される(すなわち、読み書きされる)ときはいつも、使用ビットはハードウェアによって1にセットされます。ハードウェアはビットを決してクリアしません(すなわち0にセットする行為)それはクリアを行うのはOSの責任です。

OSはLRUを近似するために使用ビットをどのように使用しますか？まあ、たくさんの方法があるかもしれませんが、clock algorithm[C69]では単純なpproachが1つ提案されました。システムのすべてのページが循環リストに配置されているとします。時計の針は、最初にいくつかの特定のページを指しています(本当に問題ありません)。置換が行われなければならない場合、OSは、現在指示されたページPに1または0の使用ビットがあるかどうかをチェックします。1ならば、これはページPが最近使用されたことを意味し、したがって置換のための良好な候補ではありません。したがって、Pの使用ビットは0(クリア)に設定され、クロック・ハンドは次のページ(P + 1)にインクリメントされます。アルゴリズムは、このページが最近使用されていないことを意味する0に設定された使用ビットが見つかるまで続きます(または、最悪の場合、すべてのページの検索を終了しすべてのクリアビットになっている)

このアプローチは、LRUを近似するために使用ビットを使用する唯一の方法ではないことに注意してください。実際には、使用ビットを定期的にクリアして、どちらのページが1と0の使用ビットを持っているかを区別して、どちらを置き換えるかを決めるアプローチは問題ありません。Corbatoのクロックアルゴリズムは、成功を収めた初期のアプローチの1つで、未使用のページを探しているすべてのメモリを繰り返しスキャンしないという素晴らしい特性を持っていました。

![](../22/img/fig22_9.PNG)

図22.9にクロックアルゴリズムの変形例の動作を示します。この変形は、置換を行うときにランダムにページをスキャンします。基準ビットが1にセットされたページに遭遇すると、ビットをクリアする(すなわち、それを0にセットする)。参照ビットが0に設定されたページが検出されると、参照ビットがその犠牲者として選択されます。ご覧のように、完璧なLRUとは言えませんが、履歴を全く考慮していないアプローチよりも優れています。

## 22.9 Considering Dirty Pages
一般的に行われているクロックアルゴリズム(Corbato [C69]が最初に提案したもの)を少し変更したのは、メモリ内でページが変更されたかどうかをさらに考慮することです。この理由は、ページが変更されて汚れている場合、ページを追い出すためにディスクに書き戻さなければならず、これは高価です。変更されていない(したがってクリーンな)場合は、削除は容易です。物理的なフレームは、追加のI/Oなしで他の目的のために単純に再利用することができます。したがって、一部のVMシステムでは、ダーティページでクリーンページを削除することができます。

この動作をサポートするために、ハードウェアは変更されたビット(a.k.a.ダーティビット)を含むべきである。このビットは、ページが書き込まれるたびに設定されるため、ページ置換アルゴリズムに組み込むことができます。たとえば、クロックアルゴリズムを変更して、使用されていないページとクリーンなページの両方をスキャンして最初に削除することができます。それらを見つけることができず、次いで、未使用のページが汚れているかどうか、等々です。

## 22.10 Other VM Policies
ページ置換は、VMサブシステムが採用している唯一のポリシーではありません(ただし、最も重要です)。例えば、OSはページをメモリにいつ持ち込むかを決定しなければいけません。このポリシーは(Denning [D70]によって呼び出されたように)ページ選択ポリシーと呼ばれることもありますが、OSにはいくつかのオプションがあります。

ほとんどのページでは、OSは単純にデマンドページングを使用します。つまり、OSはページがアクセスされたときにメモリを「オンデマンドで」オンにします。もちろん、OSはページが使用されようとしていることを推測することができます。この動作はプリフェッチと呼ばれ、合理的な成功の可能性がある場合にのみ実行する必要があります。例えば、あるシステムは、コードページPがメモリに持ち込まれると、そのコードページP +1がまもなくアクセスされる可能性が高いので、メモリに持ち込むべきであると仮定します。

別のポリシーは、OSがどのようにページをディスクに書き出すかを決定します。もちろん、それらは一度に1つずつ書き出すことができます。しかし、多くのシステムでは、多数のペンディング書き込みをメモリにまとめて1つの(より効率的な)書き込みでディスクに書き込みます。この動作は、通常、クラスタリングと呼ばれ、単純に書き込みのグループ化と呼ばれ、多数の小さなものよりも効率的に単一の大きな書き込みを実行するディスクドライブの性質のために有効です。

## 22.11 Thrashing
閉鎖する前に、最終的な質問に答えます。メモリが単純に過多になったときにOSが行うべきことは、実行中のプロセスのメモリ要求が単に利用可能な物理メモリを上回るだけですか？この場合、システムは絶えずページングを行い、時にはスラッシング[D70]と呼ばれる状態になります。

以前のオペレーティングシステムの中には、発生時にスラッシングを検出し対処するためのかなり洗練されたメカニズムがありました。例えば、一連のプロセスがある場合、システムは、プロセスの作業セット(積極的に使用しているページ)の縮小されたセットがメモリに収まり、改善されることを期待して、プロセスのサブセットを実行しないことを決定できます。このアプローチは、一般にアドミッションコントロールとして知られていますが、現実の生活だけでなく現代のコンピュータシステム(悲しいことに)で頻繁に遭遇する状況を、一度にすべてうまくやろうとするよりも、うまく動作しない方が良いと述べています。

現在のシステムの中には、メモリ過負荷に対するより厳しいアプローチをとっているものがあります。たとえば、Linuxのバージョンによっては、メモリが過剰登録されたときにメモリ不足のキラー(OOM killer)を実行するものがあります。このデーモンは大量のメモリを必要とするプロセスを選択して終了させるので、メモリーをあまりにも微妙な方法で減らすことができます。メモリ使用量を削減するのに成功しているが、このアプローチは、例えばXサーバを殺して、ディスプレイを必要とするアプリケーションを使用できなくすると、問題を引き起こす可能性があります。

## 22.12 Summary
我々は、すべての最新のオペレーティングシステムのVMサブシステムの一部であるいくつかのページ置換(およびその他の)ポリシーの導入を見てきました。現代のシステムでは、時計のような簡単なLRU近似にいくつかの微調整が追加されています。例えば、走査抵抗は、ARC [MM03]のような多くの最近のアルゴリズムの重要な部分です。スキャン耐性アルゴリズムは通常LRUのようなものですが、LRUのワーストケースの動作を回避しようとしています。LRUはループ順の仕事量で見ました。したがって、ページ置換アルゴリズムの進化が続いていきます。

しかし、多くの場合、メモリアクセスとディスクアクセス時間との間の相違が増大するにつれて、前記アルゴリズムの重要性が減少しています。ディスクへのページングは非常に高価なので、頻繁なページングのコストは非常に高くなります。したがって、過度のページングに対する最善の解決策は、よく単純(知的に不満な場合)です。

# 参考文献

[AD03] “Run-Time Adaptation in River”  
Remzi H. Arpaci-Dusseau  
ACM TOCS, 21:1, February 2003  
A summary of one of the authors’ dissertation work on a system named River. Certainly one place where he learned that comparison against the ideal is an important technique for system designers.

[B66] “A Study of Replacement Algorithms for Virtual-Storage Computer”  
Laszlo A. Belady  
IBM Systems Journal 5(2): 78-101, 1966  
The paper that introduces the simple way to compute the optimal behavior of a policy (the MIN algorithm).

[BNS69] “An Anomaly in Space-time Characteristics of Certain Programs Running in a Paging Machine”  
L. A. Belady and R. A. Nelson and G. S. Shedler  
Communications of the ACM, 12:6, June 1969  
Introduction of the little sequence of memory references known as Belady’s Anomaly. How do Nelson and Shedler feel about this name, we wonder?

[CD85] “An Evaluation of Buffer Management Strategies for Relational Database Systems”  
Hong-Tai Chou and David J. DeWitt  
VLDB ’85, Stockholm, Sweden, August 1985  
A famous database paper on the different buffering strategies you should use under a number of common database access patterns. The more general lesson: if you know something about a workload, you can tailor policies to do better than the general-purpose ones usually found in the OS.

[C69] “A Paging Experiment with the Multics System”  
F.J. Corbato  
Included in a Festschrift published in honor of Prof. P.M. Morse  
MIT Press, Cambridge, MA, 1969  
The original (and hard to find!) reference to the clock algorithm, though not the first usage of a use bit. Thanks to H. Balakrishnan of MIT for digging up this paper for us.

[D70] “Virtual Memory”  
Peter J. Denning  
Computing Surveys, Vol. 2, No. 3, September 1970  
Denning’s early and famous survey on virtual memory systems.

[EF78] “Cold-start vs. Warm-start Miss Ratios”  
Malcolm C. Easton and Ronald Fagin  
Communications of the ACM, 21:10, October 1978  
A good discussion of cold-start vs. warm-start misses.

[FP89] “Electrochemically Induced Nuclear Fusion of Deuterium”  
Martin Fleischmann and Stanley Pons  
Journal of Electroanalytical Chemistry, Volume 26, Number 2, Part 1, April, 1989  
The famous paper that would have revolutionized the world in providing an easy way to generate nearlyinfinite power from jars of water with a little metal in them. Unforuntately, the results published (and widely publicized) by Pons and Fleischmann turned out to be impossible to reproduce, and thus these two well-meaning scientists were discredited (and certainly, mocked). The only guy really happy about this result was Marvin Hawkins, whose name was left off this paper even though he participated in the work; he thus avoided having his name associated with one of the biggest scientific goofs of the 20th century.

[HP06] “Computer Architecture: A Quantitative Approach”  
John Hennessy and David Patterson  
Morgan-Kaufmann, 2006  
A great and marvelous book about computer architecture. Read it!  

[H87] “Aspects of Cache Memory and Instruction Buffer Performance”  
Mark D. Hill  
Ph.D. Dissertation, U.C. Berkeley, 1987  
Mark Hill, in his dissertation work, introduced the Three C’s, which later gained wide popularity with its inclusion in H&P [HP06]. The quote from therein: “I have found it useful to partition misses ... into three components intuitively based on the cause of the misses (page 49).”

[KE+62] “One-level Storage System”  
T. Kilburn, and D.B.G. Edwards and M.J. Lanigan and F.H. Sumner  
IRE Trans. EC-11:2, 1962  
Although Atlas had a use bit, it only had a very small number of pages, and thus the scanning of the use bits in large memories was not a problem the authors solved.

[M+70] “Evaluation Techniques for Storage Hierarchies”  
R. L. Mattson, J. Gecsei, D. R. Slutz, I. L. Traiger  
IBM Systems Journal, Volume 9:2, 1970  
A paper that is mostly about how to simulate cache hierarchies efficiently; certainly a classic in that regard, as well for its excellent discussion of some of the properties of various replacement algorithms. Can you figure out why the stack property might be useful for simulating a lot of different-sized caches at once?

[MM03] “ARC: A Self-Tuning, Low Overhead Replacement Cache”  
Nimrod Megiddo and Dharmendra S. Modha  
FAST 2003, February 2003, San Jose, California  
An excellent modern paper about replacement algorithms, which includes a new policy, ARC, that is now used in some systems. Recognized in 2014 as a “Test of Time” award winner by the storage systems community at the FAST ’14 conference.

\newpage

# 23 The VAX/VMS Virtual Memory System
仮想メモリの調査を終了する前に、VAX/VMSオペレーティング・システム[LL82]にある、きれいに整った仮想メモリ・マネージャの1つを詳しく見てみましょう。ここでは、以前の章でもたらされた概念のいくつかが完全なメモリマネージャにどのように集まっているかを示すためのシステムについて説明します。

## 23.1 Background
VAX-11ミニコンピュータのアーキテクチャは、Digital Equipment Corporation(DEC)によって1970年代後半に導入されました。DECは、ミニコンピュータの時代にコンピュータ業界で大規模な市場を持った企業でした。残念なことに一連の悪い決定とPCの出現はゆっくりと崩壊につながりました[C03]。このアーキテクチャは、VAX-11/780やそれほど強力ではないVAX-11/750など、いくつかの実装で実現されました。

このシステムのOSはVAX/VMS(または単純なVMS)として知られていましたが、主なアーキテクトはDave Cutlerでした。後でMicrosoftのWindows NT [C93]を開発しようと努力しました。非常に安価なVAXenを含む広範囲のマシンで、同じアーキテクチャファミリの非常にハイエンドでパワフルなマシンです。したがって、OSには、この巨大なシステム全体で機能し、うまく機能する仕組みとポリシーが必要でした。

>> THE CRUX: HOW TO AVOID THE CURSE OF GENERALITY
>> オペレーティングシステムはしばしば「一般性の呪い」として知られている問題を抱えています。これらは幅広い種類のアプリケーションやシステムの一般的なサポートが行われています。この呪いの基本的な結果は、OSがいずれかのインストールを非常にうまくサポートしない可能性があるということです。VMSの場合、VAX-11アーキテクチャーはさまざまな実装で実現されていたため、呪いは非常にリアルでした。したがって、幅広いシステムで効果的に動作するようにOSを構築するにはどうすればよいですか？

追加の問題として、VMSはアーキテクチャの固有の欠陥のいくつかを隠すために使用されるソフトウェア革新の優れた例です。OSは効率的な抽象化と錯覚を構築するためにハードウェアに依存していることがよくありますが、ハードウェア設計者はすべてのことを正しく行うことができません。VAXハードウェアにはいくつかの例があり、これらのハードウェアの欠陥にもかかわらず、VMSオペレーティングシステムが効果的で実用的なシステムを構築するために何をしているのかがわかります。

## 23.2 Memory Management Hardware
VAX-11は、1プロセス当たり32ビットの仮想アドレス空間を512バイトのページに分割して提供しました。したがって、仮想アドレスは23ビットのVPNと9ビットのオフセットで構成されています。さらに、VPNの上位2ビットを使用して、ページがどのセグメントに存在するかを区別しました。したがって、システムは以前に見たようにページングとセグメンテーションのハイブリッドでした。アドレス空間の下半分は「プロセス空間」と呼ばれ、各プロセスに固有のものでした。プロセス空間の前半(P0として知られている)では、ユーザープログラムが見つかっただけでなく、下向きに成長するヒープも検出されます。プロセス空間(P1)の後半では、上向きに成長するスタックを見つけます。アドレススペースの上半分はシステムスペース(S)として知られていますが、半分しか使用されていません。保護されたOSコードとデータはここにあり、OSはこのようにしてプロセス間で共有されます。

VMSデザイナの主な関心事の1つは、VAXハードウェア(512バイト)のページのサイズが非常に小さいことでした。歴史的な理由から選択されたこのサイズは、単純な線形ページテーブルを過度に大きくするという基本的な問題があります。したがって、VMSデザイナの最初の目標の1つは、VMSがページテーブルを使用してメモリを圧迫しないようにすることでした。

システムは、2つの方法でメモリ上の圧迫するページテーブルの場所を減らしました。まず、ユーザー・アドレス空間を2つに分割することにより、VAX-11はプロセスごとにこれらのリージョン(P0とP1)のそれぞれにページ・テーブルを提供します。従って、スタックとヒープとの間のアドレス空間の未使用部分にはページテーブルスペースは必要ありません。baseとboundsレジスタは期待どおりに使用されます。ベースレジスタはそのセグメントのページテーブルのアドレスを保持し、境界はそのサイズ(すなわちページテーブルエントリの数)を保持します。

第2に、OSは、カーネル仮想メモリにユーザページテーブル(P0とP1のために1プロセスあたり2つ)を配置することにより、さらにメモリ圧迫を減らします。このように、ページテーブルを割り振ったり、成長させたりすると、カーネルは、セグメントS内に、それ自身の仮想メモリから空間を割り当てます。メモリが厳しい状況になると、カーネルはこれらのページテーブルのページをディスクにスワップして、他の用途のために使用します。

ページテーブルをカーネル仮想メモリに置くと、アドレス変換がさらに複雑になります。たとえば、P0またはP1の仮想アドレスを変換するには、まずハードウェアがページテーブル(そのプロセスのP0またはP1ページテーブル)でそのページのページテーブルエントリを検索する必要があります。ただし、ハードウェアはまずシステムページテーブル(物理メモリに存在する)を参照する必要があります。その変換が完了すると、ハードウェアはページテーブルのページのアドレスを学習し、最後に望んだメモリアクセスのアドレスを知ることができます。幸いにも、これはVAXのハードウェア管理のTLBによって高速化されています。このTLBは通常、この厄介な検索を回避します(うまくいけばの話ですが…)

## 23.3 A Real Address Space
VMSを研究するうえでの真面目なところは、実際のアドレス空間がどのように構築されているかを見ることができます(図23.1)。ここまでは、ユーザコード、ユーザデータ、ユーザヒープだけの単純なアドレス空間を想定していましたが、実アドレス空間は明らかに複雑です。

![](../23/img/fig23_1.PNG)

>> ASIDE: WHY NULL POINTER ACCESSES CAUSE SEG FAULTS  
>> NULLポインタ逆参照で何が起こるかを正確に理解する必要があります。プロセスは、次のようにすることによって仮想アドレス0を生成します。
```
int *p = NULL; // set p = 0
*p = 10; // try to store value 10 to virtual address 0
```  
>> ハードウェアは、TLB内のVPN(ここでも0)をルックアップしようとし、TLBミスを起こします。ページテーブルが参照され、VPN 0のエントリが無効とマークされていることがわかります。したがって、私たちは無効なアクセス権だったため、OSに制御を移し、プロセスを終了させる可能性があります(UNIXシステムでは、プロセスにそのようなフォールトに反応するシグナルが送信されますが、キャッチされないとプロセスは強制終了されます)

例えば、コードセグメントはページ0から始まりません。代わりに、NULLポインタアクセスを検出するためのサポートを提供するために、このページはアクセス不能とマークされます。したがって、アドレス空間を設計する際の懸案事項の1つはデバッグのサポートであり、アクセスできないゼロページはここで何らかの形で提供されます。

おそらくもっと重要なことに、カーネル仮想アドレス空間(すなわち、そのデータ構造およびコード)は各ユーザアドレス空間の一部です。コンテキストスイッチでは、OSは、すぐに実行されるプロセスの適切なページテーブルを指すようにP0およびP1レジスタを変更します。しかし、Sのベースレジスタと境界レジスタは変更されず、その結果、「同じ」カーネル構造が各ユーザアドレス空間にマッピングされます。

カーネルは、いくつかの理由から各アドレス空間にマップされます。このような構成により、カーネルの作業が容易になります。例えば、OSにポインタを渡すと(例えば、`write()`システムコールで)、そのポインタからのデータをそれ自身の構造にコピーすることは容易です。OSは、アクセスしているデータがどこから来るのか心配することなく、自然に書かれコンパイルされます。これとは対照的に、カーネルが完全に物理メモリに置かれていた場合、ページテーブルのページをディスクにスワップするなどの作業は非常に難しくなります。カーネルに独自のアドレス空間が与えられていれば、ユーザーアプリケーションとカーネル間でデータを移動することは、かなり複雑になってしまいます。この構造(現在広く使われている)では、カーネルは、保護されているものの、アプリケーションのライブラリのように見えます。

このアドレス空間に関する最後の1つのポイントは、保護に関するものです。明らかに、OSはユーザアプリケーションがOSのデータやコードを読み書きすることを望みません。従って、ハードウェアは、これを可能にするためにページに対して異なる保護レベルをサポートしなければいけません。VAXは、ページテーブルの保護ビットに、特定のページにアクセスするためにCPUが必要とする特権レベルを指定することによってそうしました。したがって、システムデータおよびコードは、ユーザデータおよびコードよりも高い保護レベルに設定されます。そのような情報にユーザーコードからアクセスしようとすると、OSにトラップが生成され、問題のプロセスが終了する可能性があります。

## 23.4 Page Replacement
VAXのページテーブルエントリ(PTE)には、有効ビット、保護フィールド(4ビット)、変更(またはダーティ)ビット、OS使用のために予約されたフィールド(5ビット)、および最後に物理メモリにページの場所を格納するための物理フレーム番号(PFN)があります。しかし、参照ビットがありません。したがって、VMS置換アルゴリズムは、どのページがアクティブであるかを決定するためのハードウェアサポートなしで行う必要があります。

開発者はまた、メモリを多くのメモリを使用するプログラムがあり、他のプログラムを実行するのを困難にする場合について懸念していました。これまで見てきたポリシーのほとんどは、このような騒ぎに敏感です。たとえば、LRUはプロセス間でメモリを公平に共有しないグローバルポリシーです。

### Segmented FIFO
この2つの問題に対処するために、開発者はセグメント化されたFIFO置換ポリシー[RL81]を考え出しました。アイデアは単純です。各プロセスには、resident set size(RSS)と呼ばれる、メモリに保存できる最大ページ数があります。これらの各ページはFIFOリストに保持されます。プロセスがそのRSSを超えると、「先入れ先出し(first-in)」ページが追い出されます。FIFOはハードウェアからのサポートを必要とせず、実装が容易です。

当然のことながら、純粋なFIFOは、以前のように、特にうまく機能しません。FIFOのパフォーマンスを向上させるために、VMSはメモリから削除される前にページが配置される2つのセカンドチャンスリストを考えました。それはグローバルクリーンページフリーリストとダーティページリストです。プロセスPがそのRSSを超えると、ページはプロセスごとのFIFOから削除されます。クリーン(変更されていない)の場合はクリーンページリストの最後に配置されます。汚れている(変更されている)場合は、ダーティページリストの最後に配置されます。

別のプロセスQに空きページが必要な場合は、グローバルクリーンリストから最初の空きページが削除されます。ただし、元のプロセスPがページフォルトが発生した場合、元のプロセスPが再利用される前に、Pはフリー(またはダーティ)リストからそのページを再要求し、コストのかかるディスクアクセスを回避します。これらのセカンドチャンスリストが大きいほど、セグメント化されたFIFOアルゴリズムはLRU [RL81]に近づきます。

### Page Clustering
VMSで使用される別の最適化は、VMSの小さなページサイズを克服するのにも役立ちます。具体的には、このような小さいページでは、スワップ時のディスクI/Oは、ディスクが大規模な転送をもっているものでよりうまくいくので、非常に非効率的である可能性があります。VMSは、I/Oのスワッピングをより効率的にするために、いくつかの最適化を追加しますが、最も重要なのはクラスタリングです。クラスタリングでは、VMSは大規模なダーティー・リストから大量のバッチ・ページをグループ化し、それらを一気にディスクに書き込むことで、クリーンになります。スワップスペース内のどこにでもページを配置できるため、OSグループのページを作成したり、書き込み回数を減らしたりすることができ、パフォーマンスが向上するため、現代のシステムではクラスタリングが使用されています。

>> ASIDE: EMULATING REFERENCE BITS  
>> 分かっているように、システムでどのページが使用されているかという概念を得るために、ハードウェア参照ビットは必要ありません。実際、1980年代初めに、BabaogluとJoyは、VAXの保護ビットを使用して参照ビットをエミュレートすることができることを示しました[BJ81]。基本的な考え方として、システムでどのページが積極的に使用されているかを理解したい場合は、ページテーブルのすべてのページをアクセス不可能とマークします(しかし、プロセスによって実際にアクセス可能なページ、おそらくページテーブルエントリの「予約されたOSフィールド」部分にあるかもしれません)。  
プロセスがページにアクセスすると、OSにトラップが生成されます。OSはページが実際にアクセス可能であるかどうかをチェックし、そうであればページを通常の保護(例えば、読み取り専用または読み書き)に戻します。その置換時に、OSはどのページがアクセス不能とマークされているかを確認することができます。  
そのため、どのページが最近使用されていないのかを知ることができます。参照ビットのこの「エミュレーション」の鍵は、オーバーヘッドを削減しながら、ページ使用の良いアイデアを得ることです。OSは、アクセス不可能なページをマーキングするにはあまりにも攻撃的であるため、やってはいけません。また、オーバーヘッドが高すぎます。OSはまた、そのようなマーキングではあまりにも受動的であってはなりません。そうでないと、すべてのページが参照されます。また、OSは、どのページを退去させるべきかについては、まったく考えていません。

## 23.5 Other Neat VM Tricks
VMSには、デマンドゼロ(demand zeroing)とコピーオン・ライトの2つの標準的なトリックがありました。ここで、これらの遅延最適化について説明します。

VMS(およびほとんどの現代システム)における怠惰の1つの形態は、ページの要求のゼロ化(デマンドゼロ)です。これをよりよく理解するために、アドレス空間にヒープ内のページを追加する例を考えてみましょう。単純な実装では、OSはヒープにページを追加する要求に応答し、物理メモリ内のページを見つけてゼロにします(セキュリティが必要な場合は、他のプロセスが発生したときのページの内容を確認できます)それをあなたのアドレス空間にマッピングします(つまり、物理ページを望むようにページテーブルを設定する)。しかし、特にページがプロセスによって使用されない場合は、単純な実装にはコストがかかる可能性があります。

デマンドゼロは、アドレス空間にページが追加されてもOSはほとんど機能しません。ページ・テーブルには、そのページにアクセスできないとマークするエントリが挿入されます。プロセスがページを読み書きすると、OSへのトラップが発生します。トラップを処理するとき、OSは(実際にはページテーブルエントリの「OSのために予約された」部分に記されたビットを通して)これが実際にはデマンドゼロのページであることに気づきます。この時点で、OSは、物理ページを見つけ出し、ゼロにし、プロセスのアドレス空間にマッピングするために必要な作業を行います。プロセスがページにアクセスしない場合、この作業はすべて回避されます。

>> TIP: BE LAZY
>> 遅延であることは、オペレーティングシステムに利点をもたらします。遅延は後で作業を延期することができます。これは、いくつかの理由でOS内で有益です。まず、作業を中止すると、現在の操作の待ち時間が短縮され、応答性が向上します。たとえば、オペレーティングシステムでは、ファイルへの書き込みがすぐに成功したことを報告し、バックグラウンドで後でディスクに書き込むことがよくあります。第二に、さらに重要なことに、遅延は時にはその作業をやる必要性をなくします。例えば、ファイルが削除されるまで書き込みを遅延させると、書き込みを一切行う必要がなくなります。

VMSで見いだされたもうひとつのクールな最適化(やはり、現代のあらゆるOSで)は、コピーオンライト(略してCOW)です。アイデアとしてはTENEXオペレーティングシステム[BB+72]というものがあります。そのアイデアは簡単です。OSがコピーする代わりに、あるアドレス空間から別のアドレス空間にページをコピーする必要があるとき、それをターゲットアドレスにマップすることができます。このとき両方のアドレス空間で読み取り専用にマークします。両方のアドレス空間がページだけを読み込んだ場合、OSは実際にデータを移動することなく高速コピーを実現します。ただし、アドレススペースの1つが実際にページに書き込もうとすると、OSにトラップされます。OSはそのページがCOWページであることに気づき、新しいページを割り当て、データで埋めて、この新しいページをフォールトを起こしたプロセスのアドレス空間にマッピングします。その後、プロセスは続行され、そのプロセス専用のコピーが作成されます。

COWは多くの理由で有用です。確かにどんな種類の共有ライブラリも、多くのプロセスのアドレス空間にコピーオンライトでマッピングすることができ、貴重なメモリスペースを節約できます。UNIXシステムでは、COWは`fork()`と`exec()`の正確さのためにさらに重要です。呼び出すことができるように、`fork()`は呼び出し元のアドレス空間の正確なコピーを作成します。大きなアドレス空間では、そのようなコピーを作成するのが遅く、データ集約的です。さらに悪いことに、ほとんどのアドレス空間は`exec()`の後続の呼び出しによって直ちに上書きされ、呼び出し元プロセスのアドレス空間とすぐに実行されるプログラムのアドレス空間が上書きされます。代わりに、コピーオンライトの`fork()`を実行することにより、OSは不必要なコピーをほとんど回避し、したがってパフォーマンスを向上させながら正しいデータを保持します。

## 23.6 Summary
仮想メモリシステム全体のトップからボトムのレビューを見たことがあります。基本的な仕組みやポリシーの大部分をすでによく理解したので、細部のほとんどは簡単に理解できるはずです。詳細はLevy and Lipman [LL82]の優れた(そして短い)論文に掲載されています。これらの章の背後にある原資料がどのようなものかを見るための素晴らしい方法です。  
可能であれば、Linuxやその他の最新のシステムについて読むことで、最先端技術についてさらに学ぶ必要があります。そこにはいくつかの合理的な書籍[BC05]を含む多くのソース資料があります。VAX/VMS上の古い文書に見られる古典的なアイデアが、現代のオペレーティングシステムの構築方法にどのように影響しているのか、あなたを驚かせるでしょう。

# 参考文献

[BB+72] “TENEX, A Paged Time Sharing System for the PDP-10”  
Daniel G. Bobrow, Jerry D. Burchfiel, Daniel L. Murphy, Raymond S. Tomlinson  
Communications of the ACM, Volume 15, March 1972  
An early time-sharing OS where a number of good ideas came from. Copy-on-write was just one of those; inspiration for many other aspects of modern systems, including process management, virtual memory, and file systems are found herein.

[BJ81] “Converting a Swap-Based System to do Paging in an Architecture Lacking Page-Reference Bits”  
Ozalp Babaoglu and William N. Joy  
SOSP ’81, Pacific Grove, California, December 1981  
A clever idea paper on how to exploit existing protection machinery within a machine in order to emulate reference bits. The idea came from the group at Berkeley working on their own version of UNIX, known as the Berkeley Systems Distribution, or BSD. The group was heavily influential in the development of UNIX, in virtual memory, file systems, and networking.

[BC05] “Understanding the Linux Kernel (Third Edition)”  
Daniel P. Bovet and Marco Cesati  
O’Reilly Media, November 2005  
One of the many books you can find on Linux. They go out of date quickly, but many of the basics remain and are worth reading about.

[C03] “The Innovator’s Dilemma”  
Clayton M. Christenson  
Harper Paperbacks, January 2003  
A fantastic book about the disk-drive industry and how new innovations disrupt existing ones. A good read for business majors and computer scientists alike. Provides insight on how large and successful companies completely fail.

[C93] “Inside Windows NT”  
Helen Custer and David Solomon  
Microsoft Press, 1993  
The book about Windows NT that explains the system top to bottom, in more detail than you might like. But seriously, a pretty good book.

[LL82] “Virtual Memory Management in the VAX/VMS Operating System”  
Henry M. Levy, Peter H. Lipman  
IEEE Computer, Volume 15, Number 3 (March 1982) Read the original source of most of this material;  
it is a concise and easy read. Particularly important if you wish to go to graduate school, where all you do is read papers, work, read some more papers, work more, eventually write a paper, and then work some more. But it is fun!

[RL81] “Segmented FIFO Page Replacement”  
Rollins Turner and Henry Levy  
SIGMETRICS ’81, Las Vegas, Nevada, September 1981  
A short paper that shows for some workloads, segmented FIFO can approach the performance of LRU

\newpage

\part{Concurrency}
# 26 Concurrency: An Introduction
ここまでは、OSが実行する基本的な抽象概念の開発を見てきました。単一の物理CPUを複数の仮想CPUに変換する方法を見てきました。これにより、複数のプログラムが同時に実行されているように見えます。また、プロセスごとに大きな仮想プライベート仮想メモリを作成する方法を見てきました。このアドレス空間の抽象化によって、OSが実際に物理メモリ(および場合によってはディスク)上のアドレス空間を秘密に多重化しているときに、各プログラムが独自のメモリを持つかのように動作することができます。

ここでは、スレッドの1つの実行プロセスに対する新しい抽象概念を紹介します。プログラム内の単一の実行ポイント(すなわち、命令がフェッチされて実行される1つのPC(プログラムカウンタ))の古典的なビューの代わりに、マルチスレッドプログラムは複数の実行ポイントを持っています(つまり、複数のPC、それはフェッチされ実行されている)おそらく、これを考えるもう一つの方法は、各スレッドが1つの違いを除いて、別々のプロセスに非常に似ていることです。つまり、同じアドレス空間を共有し、同じデータにアクセスできます。

したがって、単一のスレッドの状態は、プロセスの状態と非常に似ています。それは、プログラムが命令をフェッチする場所を追跡するプログラムカウンタ(PC)を持っています。各スレッドには、計算に使用する専用のレジスタセットがあります。したがって、1つのプロセッサ上で実行されている2つのスレッドがある場合、1つを実行する(T1)からもう一方のスレッドを実行する(T2)に切り替えるときは、コンテキスト切り替えが行われなければなりません。スレッド間のコンテキスト切り替えは、プロセス間のコンテキスト切り替えと非常によく似ています。これは、T1のレジスタ状態を保存し、T2を実行する前にT2のレジスタ状態を復元する必要があるためです。プロセスでは、状態をプロセス制御ブロック(PCB)に保存しました。現在、プロセスの各スレッドの状態を格納するために、1つ以上のスレッド制御ブロック(TCB)が必要になります。ただし、プロセスと比較してスレッド間で実行するコンテキスト切り替えには大きな違いが1つあります。アドレス空間は同じままです(つまり、使用しているページテーブルを切り替える必要はありません)

スレッドとプロセスのもう1つの大きな違いは、スタックに関するものです。従来のプロセス(単一スレッドプロセスと呼ぶこともできる)のアドレス空間の単純なモデルでは、通常はアドレス空間の一番下に単一のスタックがあります(図26.1、左)

しかし、マルチスレッドプロセスでは、各スレッドは独立して実行され、もちろんどのような作業をしていてもさまざまなルーチンを呼び出すことができます。アドレス空間には1つのスタックの代わりにスレッドごとに1つのスタックが存在します。たとえば、2つのスレッドを持つマルチスレッドプロセスがあるとします。結果のアドレス空間は異なって見えます(図26.1、右)。

![](../26/img/fig26_1.PNG)

この図では、プロセスのアドレス空間全体に2つのスタックが広がっていることがわかります。したがって、スレッドに割り当てられた変数、パラメータ、戻り値、およびスタック上に置かれた他のものは、thread local storageと呼ばれることもあるスタックに格納されます。

これがどのように私たちの美しいアドレス空間のレイアウトを崩すのか気づくかもしれません。以前は、スタックとヒープが独立して成長する可能性があり、アドレス空間の空き領域を使い果たしたときにのみ問題が発生しました。ここでは、もはやこのような素晴らしい状況はありません。幸運なことに、スタックは一般に非常に大きくする必要はありません(例外は再帰を頻繁に使用するプログラムです)

## 26.1 Why Use Threads?
スレッドの詳細とマルチスレッドプログラムの作成に伴う問題のいくつかに入る前に、もっと簡単な質問にまず答えてみましょう。なぜスレッドを使うべきなのでしょうか？

スレッドを使用する主な理由は少なくとも2つあります。最初のものは単純です。それは並列性です。たとえば、2つの大きな配列を一緒に追加するか、配列内の各要素の値をある量だけインクリメントするなど、非常に大きな配列で操作を実行するプログラムを作成しているとします。単一のプロセッサで実行している場合、タスクは簡単です。それぞれの操作を実行するだけです。ただし、複数のプロセッサを搭載したシステム上でプログラムを実行している場合は、各プロセッサを使用して作業の一部を実行することで、このプロセスを大幅に高速化する可能性があります。標準のシングルスレッドプログラムを複数のCPUでこの種の作業を行うプログラムに変換する作業は並列化と呼ばれ、この作業を行うCPUごとのスレッドを使用するのは、現代のハードウェアでプログラムをより高速に実行できる自然で一般的な方法です。

2つ目の理由は、I/Oが遅いことによる、ブロッキングされているプログラムの進行を避けるためです。さまざまなタイプのI/Oを実行するプログラムを記述しているとします。メッセージの送信または受信の待機、明示的なディスクI/Oの完了、またはページ・フォールトの完了(暗黙的)です。待機する代わりに、プログラムを使用してCPUを使用して計算を実行します、さらにI/O要求を発行するなど、何か他の処理を実行することもできます。スレッドを使用するのは自然な方法です。プログラム内の1つのスレッドが待機している(つまり、I/Oを待ってブロックされている)場合、CPUスケジューラは実行準備が整っていて有用なことをする他のスレッドに切り替えることができます。スレッディングは、プログラム間のプロセスのマルチプログラミングのように、単一のプログラム内の他のアクティビティとのI/Oのオーバーラップを可能にします。その結果、多くの現代のサーバベースのアプリケーション(ウェブサーバ、データベース管理システムなど)は、その実装においてスレッドを利用します。

もちろん、上記のいずれの場合でも、スレッドの代わりに複数のプロセスを使用できます。しかし、スレッドはアドレス空間を共有するため、データの共有が容易になり、したがって、これらのタイプのプログラムを構築する際には自然な選択です。プロセスは、メモリ内のデータ構造をほとんど共有する必要のない、論理的に分離したタスクのためのより健全な選択です。

## 26.2 An Example: Thread Creation
詳細をいくつか取り上げましょう。私たちは2つのスレッドを作成するプログラムを実行したいとします。それぞれのスレッドは独立した作業を行います。この場合、「A」または「B」を印刷します。コードは図26.2(278ページ)に示されています。メインプログラムは2つのスレッドを作成し、それぞれは異なる引数(文字列AまたはB)を使用しても、関数`mythread()`を実行します。スレッドが作成されると、すぐに実行を開始することがあります(スケジューラーの気まぐれに応じて)。あるいは、「実行可能」状態であるが、まだ実行状態ではなく、実行されていないと置いてみましょう。もちろん、マルチプロセッサ上では、スレッドは同時に実行することもできますが、この可能性についてはまだ心配しないでください。

![](../26/img/fig26_2.PNG)

2つのスレッド(T1とT2と呼ぶ)を作成した後、メインスレッドは`pthread_join()`を呼び出し、特定のスレッドが完了するのを待ちます。このように2回実行すると、T1とT2が確実に実行され、完了してからメインスレッドが再び実行できるようになります。終了すると、「main：end」と表示され、終了します。全体的に、この実行中に3つのスレッド、すなわちメインスレッド、T1、およびT2が使用されました。

この小さなプログラムの実行順序を調べてみましょう。実行ダイアグラム(図26.3)では、時間が下方向に増加し、各列は異なるスレッド(メインスレッド、スレッド1、またはスレッド2)が実行されていることを示します。

![](../26/img/fig26_3.PNG)

ただし、この順序は唯一の可能な順序ではないことに注意してください。実際には、一連の命令があれば、スケジューラが特定のポイントで実行するスレッドに応じてかなりの数があります。たとえば、スレッドが作成されるとすぐにスレッドが実行され、図26.4に示す実行につながります。

![](../26/img/fig26_4.PNG)

スレッド1が先に作成されたとしても、スケジューラがスレッド2を最初に実行することに決めた場合、「A」の前に「B」が印刷されていることもあります。最初に作成されたスレッドが最初に実行されると仮定する理由はありません。図26.5にこの最終実行順序を示します。スレッド2はスレッド1より前にスレッドを処理します。

![](../26/img/fig26_5.PNG)

あなたが見ることができるように、スレッドの作成について考える方法の1つは、関数呼び出しのようなものです。しかし、関数を最初に実行してから呼び出し元に戻る代わりに、呼び出されているルーチンの新しい実行スレッドが作成され、呼び出し元から独立して実行されます(おそらく作成から戻る前に)次に実行されるのはOSスケジューラによって決定され、スケジューラはいくつかの実用的なアルゴリズムを実装する可能性がありますが、特定の瞬間に何が実行されるのかを知ることは困難です。

この例からも分かるように、スレッドを使用すると複雑な作業になります。いつ実行するのかはすでに分かりません！コンピュータは、並行性なしでは理解するのに十分なほど難しい。残念ながら、同時実行性では、単に悪化します。ずっと悪いです。

##26.3 Why It Gets Worse: Shared Data
上に示した単純なスレッドの例は、スケジューラーがそれらを実行する方法に応じて、スレッドがどのように作成され、どのように異なる順序で実行されるかを示すのに役立ちました。しかし、共有データにアクセスするときにスレッドがどのようにやりとりするかは、あなたには分かりません。2つのスレッドがグローバル共有変数を更新したいという単純な例を想像してみましょう。調査するコードは、図26.6(280ページ)にあります。

![](../26/img/fig26_6.PNG)

コードについてのいくつかの注意があります。まず、スティーブンスが[SR05]を提案しているように、スレッドの作成と結合ルーチンをラップして失敗時に終了するだけです。このようなシンプルなプログラムでは、少なくともエラーが発生したことに気づきたいですが、それについては非常に賢明なことはしません(exitなど)。したがって、`pthread_create()`は単に`pthread_create()`を呼び出し、戻りコードが0であることを確認します。そうでない場合、`pthread_create()`は単にメッセージを出力して終了します。

第2に、ワーカースレッドに2つの別々の関数本体を使用する代わりに、単一のコードを使用してスレッドに引数(この場合は文字列)を渡すだけで、各スレッドがメッセージの前に別の文字を表示できるようになります。

最後に、最も重要なのは、各作業者が何をしようとしているのかを見てみましょう。共有変数カウンタに数値を追加し、ループで1000万回(1e7)を実行します。従って、望む最終結果は、20,000,000です。

プログラムのコンパイルと実行を行い、プログラムの動作を確認します。時には、すべてが期待通りに働くこともあります。  
```
prompt> gcc -o main main.c -Wall -pthread
prompt> ./main
main: begin (counter = 0)
A: begin
B: begin
A: done
B: done
main: done with both (counter = 20000000)
```  
残念ながら、このコードを実行すると、単一のプロセッサであっても、必ずしも望ましい結果が得られるとは限りません。  
```
prompt> ./main
main: begin (counter = 0)
A: begin
B: begin
A: done
B: done
main: done with both (counter = 19345221)
```  
結局のところ、コンピュータは決定論的な結果を生み出すとは思われませんか？  
```
prompt> ./main
main: begin (counter = 0)
A: begin
B: begin
A: done
B: done
main: done with both (counter = 19221041)
```  
それぞれが正しく実行されるだけでなく、別の結果が得られます。大きな疑問が残っています。なぜこれが起こりますか？

>> TIP: KNOW AND USE YOUR TOOLS  
>> コンピュータシステムの作成、デバッグ、および理解に役立つ新しいツールを常に学習する必要があります。ここでは、ディスアセンブラと呼ばれる素敵なツールを使用します。実行ファイルに対して逆アセンブラを実行すると、どのアセンブリ命令がプログラムを構成しているかが表示されます。たとえば、カウンタを更新するための低レベルのコードを理解したい場合(この例のように)、objdump(Linux)を実行してアセンブリコードを表示します。
```
prompt> objdump -d main
```  
>> そうすることで、(特に-gフラグを付けてコンパイルした場合に)きれいに表示され、プログラム内のシンボル情報を含む、プログラム内のすべての命令の長いリストが生成されます。objdumpプログラムは、使い方を学ぶべき多くのツールの1つに過ぎません。gdbのようなデバッガ、valgrindやpurifyのようなメモリプロファイラ、もちろんコンパイラ自体はもっと学ぶために時間を費やすべきものです。ツールを使用している方が優れているほど、優れたシステムを構築できます。

## 26.4 The Heart Of The Problem: Uncontrolled Scheduling
なぜこのようなことが起こるかを理解するためには、コンパイラがカウンタの更新のために生成するコードシーケンスを理解する必要があります。この場合、カウンタに数値(1)を追加するだけです。したがって、そうするためのコードシーケンスは、(x86では)このように見えるかもしれません。
```
mov 0x8049a1c, %eax
add $0x1, %eax
mov %eax, 0x8049a1c
```  
この例では、変数カウンタがアドレス0x8049a1cにあると仮定しています。この3命令シーケンスでは、最初にx86 mov命令が使用され、アドレスのメモリ値を取得してレジスタeaxに格納します。次に、addが実行され、eaxレジスタの内容に1(0x1)を加え、最後にeaxの内容が同じアドレスのメモリに戻されます。

私たちの2つのスレッド(スレッド1)のうちの1つがこのコード領域に入り、カウンタを1つ増やすことを想像してみましょう。それは、カウンタの値を読み込みます(最初は50としましょう)。その値はレジスタeaxにロードされます。したがって、スレッド1の場合はeax = 50となり、レジスタに1が加算されます。従ってeax = 51になります。そして今、何か不幸なことが起こります。ここでタイマー割り込みがオフになります。したがって、OSは現在実行中のスレッド(そのPC、eaxを含むレジスタなど)の状態をスレッドのTCBに保存します。

スレッド2が実行されるように選択され、同じコードが入力されます。また、最初の命令を実行してカウンタの値を取得し、それをeaxに入れます(実行時にはスレッドごとに専用のレジスタがあり、レジスタはそれらを保存および復元するコンテキストスイッチコードによって仮想化されます)。この時点でcounterの値はまだ50であるため、スレッド2はeax = 50です。スレッド2が次の2つの命令を実行し、eaxを1つ増やして(つまりeax = 51)、eaxの内容をカウンタ(アドレス0x8049a1c)に保存すると仮定しましょう。したがって、グローバル変数カウンタは今や値51を持っています。

最後に、別のコンテキスト切り替えが発生し、スレッド1が実行を再開します。それがmovを実行して追加したことを思い出して、最終的なmov命令を実行しようとしています。eax = 51でした。したがって、最終mov命令が実行され、その値がメモリに保存されます。カウンタは再び51に設定されます。

簡単に言えば、インクリメントカウンタのコードは2回実行されていますが、50で開始されたカウンタは51にしかなりません。このプログラムの「正しい」バージョンでは、変数カウンタは52と等しくなるはずです。

問題をよりよく理解するための詳細な実行トレースを見てみましょう。この例では、上記のコードが次のシーケンスのようにメモリ上のアドレス100にロードされているものと仮定します(RISC風の命令セットに使用されていたものの、x86には可変長命令があります。このmovの命令は5バイトのメモリ、およびaddは3バイトのみ)
```
100 mov 0x8049a1c, %eax
105 add $0x1, %eax
108 mov %eax, 0x8049a1c
```  
これらの前提で、図26.7に何が起こるかを示します。カウンタが値50で始まると仮定し、この例をトレースして、何が起こっているかを理解してください。

![](../26/img/fig26_7.PNG)

ここで示したことは競合条件と呼ばれ、結果はコードのタイミング実行に依存します。いくつかの不運(実行中のタイミングの悪い時点で発生するコンテキストスイッチ)により、間違った結果が得られます。実際、私たちは毎回異なる結果を得るかもしれません。したがって、私たちはコンピュータから慣れ親しんだ素敵な決定論的な計算の代わりに、この結果を不確定と呼びます。ここでは、出力がどのようになるか分からず、実行中に実際に異なる可能性があります。

このコードを実行する複数のスレッドが競合状態になる可能性があるため、このコードをクリティカルセクションと呼びます。クリティカルセクションは、共有変数(より一般的には共有リソース)にアクセスするコードであり、複数のスレッドで同時に実行してはいけません。

このコードで本当に欲しいのは、私たちが相互排除と呼ぶものです。このプロパティーは、あるスレッドがクリティカルセクション内で実行している場合、他のスレッドがクリティカルセクション内で実行していないことを保証します。

ところで、これらの用語のほとんどは、この分野のパイオニアであったEdsger Dijkstraによって造られたもので、実際にこの作業や他の作業のためにTuring Awardを受賞しました。問題の驚くほど明確な説明については、1968年の「一連のプロセスの協力」[D68]の論文を参照してください。本書のこのセクションでは、Dijkstraについて詳しく聞いていきます。

## 26.5 The Wish For Atomicity
この問題を解決する1つの方法は、単一のステップで、必要な処理を正確に行い、不意に中断する可能性をなくした、より強力な命令を持つことです。たとえば、このようなスーパーインストラクションがあればどうでしょうか？
```
memory-add 0x8049a1c, $0x1
```  
この命令がメモリ位置に値を追加し、ハードウェアが原子的に実行することを保証すると仮定します。命令が実行されると、必要に応じて更新が実行されます。途中で命令を中断することはできません。これは、ハードウェアから受け取った保証のためです。割り込みが発生した場合、命令がまったく実行されていないか、完了まで実行されています。中間の状態は存在しません。

原子的には、この文脈では、「単位として」を意味し、時には「すべてかどうか」とみなします。私たちが望むものは、3つの命令シーケンスを原子的に実行することです。
```
mov 0x8049a1c, %eax
add $0x1, %eax
mov %eax, 0x8049a1c
```  
もしこれを行う一つの指示があれば、私たちはその指示を出すだけで済みます。しかし、一般的なケースでは、そのような指示はありません。私たちが並行してBツリーを構築していて、それを更新したいと思ったとします。ハードウェアが「Bツリーのアトミック更新」命令をサポートすることを本当に望んでいますか？おそらく、少なくとも純粋な命令セットではないでしょう。

したがって、代わりに、ハードウェアにいくつかの有用な命令を頼んで、同期プリミティブと呼ばれる一般的なセットを構築することができます。これらのハードウェア同期プリミティブを使用することで、オペレーティングシステムの助けを得て、クリティカルセクションに同期して制御された方法でアクセスするマルチスレッドコードを構築することができます。同時実行の困難な性質にもかかわらず、正確に正しい結果を生成します。同時実行、かなり素晴らしいですよね？

>> TIP: USE ATOMIC OPERATIONS
>>原子操作は、コンピュータ・アーキテクチャーからコンカレント・コード(ここで学んでいるもの)、ファイル・システム(すぐに研究する予定です)、データベース管理システム、さらには分散システム[L + 93]といったシステムに使われている強力な技術です。  
一連のアクションをアトミックにするという背後にあるアイデアは、「すべてかどうか」というフレーズで単純に表現されます。グループ化するすべてのアクションが発生したか、いずれも発生していない状態で、中間の状態が表示されていないかのように表示されます。時には、多くのアクションを1つのアトミックアクションにグループ化することをトランザクションと呼びます。これはデータベースとトランザクション処理の世界で非常に詳細に開発されたアイデアです[GR92]。  
並行処理のテーマでは、短いシーケンスの命令を実行のアトミックブロックに変換するために同期プリミティブを使用しますが、アトミック性のアイデアはそれよりもはるかに大きくなります。たとえば、ファイルシステムでは、ディスク障害時に正しく動作するために不可欠なディスク上の状態をアトミックに移行するために、ジャーナリングやコピーオンライトなどの技術を使用します。それが意味をなさないのであれば、心配しないでください。この後の章で学んでいきます。

これは本のこのセクションで研究する問題です。それは素晴らしい、難しい問題であり、あなたの心が傷つくはずです(少し)。それがなければ、あなたは理解できません！あなたの頭が痛むまで働き続けてください。あなたの頭が正しい方向に向かっていることを知っています。その時点で休憩をとります。私たちはあなたの頭があまりにも傷ついて欲しくない。

>> THE CRUX:HOW TO PROVIDE SUPPORT FOR SYNCHRONIZATION
>> 有用な同期プリミティブを構築するために、ハードウェアから何をサポートする必要がありますか？OSから何をサポートする必要がありますか？これらのプリミティブを正確かつ効率的に構築するにはどうすればよいですか？プログラムはどのようにして目的の結果を得ることができますか？

## 26.6 One More Problem: Waiting For Another
この章では、共有変数にアクセスするスレッドとクリティカルセクションのアトミック性をサポートする必要があるスレッド間で、1つのタイプの対話しか発生しないように、並行性の問題を設定しています。それが判明すると、別の一般的なやりとりが起こります。あるスレッドは、別のスレッドが続行する前に何らかのアクションを完了するのを待たなければなりません。この相互作用は、たとえば、プロセスがディスクI/Oを実行しスリープ状態になったときに発生します。I/Oが完了すると、プロセスを継続して実行できるように、プロセスをその眠りから呼び出す必要があります。

したがって、今後の章では、アトミック性をサポートするための同期プリミティブのサポートを構築する方法だけでなく、マルチスレッドプログラムでよく見られるこの種のスリープ/スリープ状態の相互作用をサポートするメカニズムについても検討します。これが今や意味がわからないのであれば、それは大丈夫です！その章を何回も読み返すことです。そうでなければ、うまくいきません。

## 26.7 Summary: Why in OS Class?
ラップアップする前に、あなたが持っているかもしれない1つの質問です。なぜ私たちはこれをOSクラスで勉強していますか？「歴史」は1語の答えです。OSは最初の並行プログラムであり、多くの技術がOS内で使用するために作成されました。その後、マルチスレッドプロセスでは、アプリケーションプログラマもそのようなことを考慮する必要がありました。

たとえば、2つのプロセスが実行されている場合を考えてみましょう。ファイルに書き込むために`write()`を呼び出し、両方ともファイルにデータを追加する(つまり、データをファイルの最後に追加して長さを増やす)とします。これを行うには、両方とも新しいブロックを割り当て、このブロックが存在するファイルのiノードに記録し、新しい大きなサイズを反映するようにファイルのサイズを変更する必要があります。(そのほかのことについては本の第3部で詳しく説明します)割り込みはいつでも発生する可能性があるので、これらの共有構造を更新するコード(例えば、割り当て用のビットマップまたはファイルのinode)はクリティカルセクションです。したがって、OS設計者は、割り込みの導入の当初から、OSが内部構造をどのように更新するかについて心配する必要がありました。タイミングの悪い割り込みが上記のすべての問題を引き起こします。当然のことながら、ページテーブル、プロセスリスト、ファイルシステム構造、およびほぼすべてのカーネルデータ構造には、適切な同期プリミティブを使用して、正しく動作するように慎重にアクセスする必要があります。

>> ASIDE: KEY CONCURRENCY TERMS CRITICAL SECTION, RACE CONDITION, INDETERMINATE, MUTUAL EXCLUSION  
>> これらの4つの用語は、並行コードの中核をなすものであり、明示的に呼び出す際には価値があると考えていました。 詳細については、Dijkstraの初期の作品[D65、D68]を参照してください。  
1.クリティカルセクションは、共有リソース(通常は変数またはデータ構造)にアクセスするコードです。  
2.競合状態は、実行の複数のスレッドがほぼ同じ時間にクリティカルセクションに入る場合に発生します。両方とも共用データ構造を更新しようとし、驚くべき(そしておそらく望ましくない)結果につながります。  
3.不確定プログラムは、1つまたは複数の競合条件で構成されます。実行時に実行されるスレッドの種類によって、プログラムの出力は実行ごとに異なります。その結果は決定論的ではなく、通常コンピュータシステムから期待されるものです。  
4.これらの問題を回避するために、スレッドは何らかの相互排他プリミティブを使用する必要があります。これにより、1つのスレッドだけがクリティカルセクションに入り、レースを回避し、確定的なプログラム出力を得ることが保証されます。  

##参考文献

[D65] “Solution of a problem in concurrent programming control”  
E. W. Dijkstra  
Communications of the ACM, 8(9):569, September 1965  
Pointed to as the first paper of Dijkstra’s where he outlines the mutual exclusion problem and a solution. The solution, however, is not widely used; advanced hardware and OS support is needed, as we will see in the coming chapters.

[D68] “Cooperating sequential processes”  
Edsger W. Dijkstra, 1968  
Available: http://www.cs.utexas.edu/users/EWD/ewd01xx/EWD123.PDF  
Dijkstra has an amazing number of his old papers, notes, and thoughts recorded (for posterity) on this website at the last place he worked, the University of Texas. Much of his foundational work, however, was done years earlier while he was at the Technische Hochshule of Eindhoven (THE), including this famous paper on “cooperating sequential processes”, which basically outlines all of the thinking that has to go into writing multi-threaded programs. Dijkstra discovered much of this while working on an operating system named after his school: the “THE” operating system (said “T”, “H”, “E”, and not like the word “the”).

[GR92] “Transaction Processing: Concepts and Techniques”  
Jim Gray and Andreas Reuter  
Morgan Kaufmann, September 1992  
This book is the bible of transaction processing, written by one of the legends of the field, Jim Gray. It is, for this reason, also considered Jim Gray’s “brain dump”, in which he wrote down everything he knows about how database management systems work. Sadly, Gray passed away tragically a few years back, and many of us lost a friend and great mentor, including the co-authors of said book, who were lucky enough to interact with Gray during their graduate school years.

[L+93] “Atomic Transactions”  
Nancy Lynch, Michael Merritt, William Weihl, Alan Fekete  
Morgan Kaufmann, August 1993  
A nice text on some of the theory and practice of atomic transactions for distributed systems. Perhaps a bit formal for some, but lots of good material is found herein.

[SR05] “Advanced Programming in the UNIX Environment”  
W. Richard Stevens and Stephen A. Rago  
Addison-Wesley, 2005  
As we’ve said many times, buy this book, and read it, in little chunks, preferably before going to bed. This way, you will actually fall asleep more quickly; more importantly, you learn a little more about how to become a serious UNIX programmer.

\newpage

# 27 Interlude: Thread API
この章では、スレッドAPIの主要部分について簡単に説明します。各部分については、APIの使い方を示すので、以降の章ではさらに詳しく説明します。詳細は、様々な書籍やオンライン情報源[B89、B97、B+96、K+96]で見つけることができます。以降の章では、ロックや条件変数の概念をよりゆっくりと紹介しています。これらには多くの例があります。したがって、この章は参考文献のように使用してください。

>> CRUX: HOW TO CREATE AND CONTROL THREADS  
>> スレッドの作成と制御のためにOSが提示すべきインタフェースは何ですか？これらのインターフェイスは、使いやすさとユーティリティ性を実現するためにどのように設計されるべきですか？
## 27.1 Thread Creation
マルチスレッドプログラムを作成するには、最初に新しいスレッドを作成する必要があります。したがって、ある種のスレッド作成インターフェイスが存在する必要があります。POSIXでは、簡単です。
```c
#include <pthread.h>
int
pthread_create( pthread_t * thread,
                const pthread_attr_t * attr,
                void * (*start_routine)(void*),
                void * arg);
```
この宣言は少し複雑に見えるかもしれませんが(特にCで関数ポインタを使用していない場合)、実際にはそれほど悪くはありません。スレッド、attr、開始ルーチン、argの4つの引数があります。最初のスレッドは、pthread型の構造体へのポインタです。この構造体を使ってこのスレッドとやりとりするので、初期化するために`pthread_create()`に渡す必要があります。

2番目の引数attrは、このスレッドが持つ可能性のある属性を指定するために使用されます。いくつかの例には、スタックサイズの設定や、おそらくスレッドのスケジューリング優先順位に関する情報が含まれます。属性は、`pthread_attr_init()`;を個別に呼び出して初期化されます。詳細については、マニュアルページを参照してください。しかし、ほとんどの場合、デフォルトは正常に動作します。この場合、単にNULLという値を渡します。

3番目の引数は最も複雑ですが、実際には尋ねています。このスレッドはどの関数で実行されるべきですか？Cでは、これを関数ポインタと呼びます。これは、関数名(開始ルーチン)がvoid *型の1つの引数を渡していることを示しています(開始ルーチンの後にかっこで示されています)void *型の値を返します(つまり、voidポインタ)このルーチンがvoidポインタの代わりに整数の引数を必要とする場合、宣言は次のようになります。
```c
int pthread_create(..., // first two args are the same
                    void * (*start_routine)(int),
                    int arg);
```  
代わりに、ルーチンがvoidポインタを引数として取りますが、整数を返した場合は、次のようになります。
```c
int pthread_create(..., // first two args are the same
int (*start_routine)(void *),
void * arg);
```  
最後に、第4引数argは、スレッドが実行を開始する関数に渡す引数とまったく同じです。あなたは質問するかもしれません。なぜこれらのvoidポインタが必要ですか？さて、答えは非常に簡単です。関数の開始ルーチンの引数としてvoidポインターを使用すると、任意の型の引数を渡すことができます。それを戻り値として持つと、スレッドはあらゆるタイプの結果を返すことができます。

図27.1の例を見てみましょう。ここでは、自分自身を定義する単一の型(myarg_t)にパッケージ化された2つの引数を渡すスレッドを作成します。作成されたスレッドは、その引数を予期した型に単にキャストするだけで、必要に応じて引数を展開することができます。

![](../27/img/fig27_1.PNG)

それがそこにあります！スレッドを作成すると、プログラム内に現在存在するすべてのスレッドと同じアドレス空間内で実行される独自の呼び出しスタックを持つ別の実行中のエンティティが実際に存在します。こうして楽しいことが始まります！

## 27.2 Thread Completion
上記の例は、スレッドを作成する方法を示しています。しかし、スレッドが完了するのを待つ場合はどうなりますか？あなたは完了を待つために特別なことをする必要があります。特に、ルーチン`pthread_join()`を呼び出す必要があります。
```c
int pthread_join(pthread_t thread, void **value_ptr);
```
このルーチンには2つの引数があります。最初の型はpthread型のもので、どのスレッドを待つかを指定するために使われます。この変数は、スレッド作成ルーチンによって初期化されます(`pthread_create()`への引数としてポインタを渡すとき)。あなたがそれを保持しているなら、それを使ってそのスレッドが終了するのを待つことができます。

2番目の引数は、返される戻り値へのポインタです。ルーチンは何も返すことができないので、voidへのポインタを返すように定義されています。`pthread_join()`ルーチンは渡された引数の値を変更するため、値そのものだけでなく、その値へのポインタを渡す必要があります。

別の例を見てみましょう(図27.2)コードでは、単一のスレッドが再び作成され、myarg構造体を介して2つの引数が渡されます。値を返すには、myret_t型が使用されます。スレッドの実行が終了すると、`pthread_join()`ルーチン1の内部で待機していたメインスレッドが戻り、スレッドから返された値、つまりmyret_tの値にアクセスできます。

![](../27/img/fig27_2.PNG)

この例についていくつか注意してください。まず、この苦しいパッキングとアンパックのすべてを行う必要はありません。たとえば、引数を持たないスレッドを作成するだけであれば、スレッドを作成するときに引数としてNULLを渡すことができます。同様に、戻り値を気にしなければ、`pthread_join()`にNULLを渡すことができます。次に、単一の値(intなど)を渡すだけの場合、引数としてパッケージ化する必要はありません。図27.3に例を示します。この場合、構造体の中に引数と戻り値をパッケージ化する必要がないので、少しシンプルです。

![](../27/img/fig27_3.PNG)

第3に、値がスレッドから返される方法に非常に注意する必要があることに注意してください。特に、スレッドの呼び出しスタックに割り当てられたものを参照するポインタを返すことはありません。そうすれば、どうなると思いますか？危険なコードの例を図27.3の例から変更しました。
```c
1 void *mythread(void *arg) {
2 myarg_t *m = (myarg_t *) arg;
3 printf("%d %d\n", m->a, m->b);
4 myret_t r; // ALLOCATED ON STACK: BAD!
5 r.x = 1;
6 r.y = 2;
7 return (void *) &r;
8 }
```  
この場合、変数rはmythreadのスタックに割り当てられます。しかし、それが返ってくると、値は自動的に割り当てが解除されます(そのため、スタックは使いやすくなります)ので、現在割り当てられていない変数にポインタを戻すと、あらゆる種類の悪い結果につながります。確かに、あなたが返すと思った値をプリントアウトすると、おそらく(必ずしもそうではありませんが)驚くことでしょう。それを試してみてください！

最後に、`pthread_create()`を使用してスレッドを作成し、それに続いて`pthread_join()`をすぐに呼び出すと、スレッドを作成するのは非常に奇妙な方法です。実際、この正確なタスクを達成するためのより簡単な方法があります。プロシージャコールと呼ばれます。明らかに、私たちは通常、1つ以上のスレッドを作成し、それが完了するのを待っています。そうでなければ、スレッドをまったく使用する目的はあまりありません。

マルチスレッドのすべてのコードが結合ルーチンを使用するわけではないことに注意してください。たとえば、マルチスレッドWebサーバーでは、多数のワーカースレッドが作成され、メインスレッドを使用して要求を受け入れ、それらをワーカーに無期限に渡すことがあります。このように長寿命のプログラムは必要がないかもしれません。しかし、特定のタスクを(並列に)実行するスレッドを作成する並列プログラムは、joinを使用して、そのようなすべての作業が終了して次の計算ステージに移る前に完了するようにします。

## 27.3 Locks
スレッドの作成と結合以外にも、おそらくPOSIXスレッドライブラリによって提供される次の最も有用な関数群は、ロックを介してクリティカルセクションに相互排除を提供するものです。この目的のために使用する最も基本的なルーチンのペアは、次のものによって提供されます。
```c
int pthread_mutex_lock(pthread_mutex_t *mutex);
int pthread_mutex_unlock(pthread_mutex_t *mutex);
```  
ルーチンは理解しやすく使いやすいものでなければなりません。クリティカルセクションであるコード領域があり、正しい操作を保証するために保護する必要がある場合、ロックは非常に便利です。
```c
pthread_mutex_t lock;
pthread_mutex_lock(&lock);
x = x + 1; // or whatever your critical section is
pthread_mutex_unlock(&lock);
```  
コードの目的は次のとおりです。`pthread_mutex_lock()`が呼び出されたときに他のスレッドがロックを保持しない場合、スレッドはロックを取得してクリティカルセクションに入ります。別のスレッドが実際にロックを保持している場合、ロックを取得しようとするスレッドは、ロックを取得するまで(ロックを保持しているスレッドがロック解除呼び出しによって解除したことを意味します)もちろん、多くのスレッドは、ロック取得関数内で所定の時間待機している可能性があります。ただし、ロックを取得したスレッドのみがunlockを呼び出す必要があります。

残念ながら、このコードは2つの重要な問題があります。最初の問題は、適切な初期化の欠如です。すべてのロックは、適切な値を持っていることを保証するために、適切に初期化されていなければなりません。

POSIXスレッドでは、ロックを初期化する2つの方法があります。これを行う1つの方法は、PTHREAD_MUTEX_INITIALIZERを次のように使用することです。
```c
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
```  
これを行うと、ロックがデフォルト値に設定され、ロックが使用可能になります。二つ目の方法は、これを実行する動的な方法(実行時)は、次のように`pthread_mutex_init()`を呼び出すことです。
```c
int rc = pthread_mutex_init(&lock, NULL);
assert(rc == 0); // always check success!
```  
このルーチンの最初の引数はロック自体のアドレスですが、2番目の引数はオプションの属性のセットです。単にNULLを渡すと、デフォルトが使用されます。いずれの方法でも動作しますが、通常は動的(後者)の方法を使用します。`pthread_mutex_destroy()`への対応する呼び出しは、ロックが完了したときにも行われることに注意してください。すべての詳細については、マニュアルページを参照してください。

上記のコードの2番目の問題は、ロックとロック解除を呼び出すときにエラーコードをチェックできないことです。UNIXシステムで呼び出すほとんどすべてのライブラリルーチンと同様に、これらのルーチンも失敗する可能性があります。コードでエラーコードが正しくチェックされないと、エラーが発生します。この場合、複数のスレッドがクリティカルセクションに入る可能性があります。最小限には、ルーチンが成功したことを主張するラッパーを使用します(たとえば、図27.4のように)。何かがうまくいかないときに、洗練されたプログラムの場合は、失敗をチェックして、ロックまたはロック解除が成功しないときに適切な何かを行うべきです。

![](../27/img/fig27_4.PNG)

ロックとアンロックルーチンは、pthreadsライブラリ内のロックと対話する唯一のルーチンではありません。特に興味のあるルーチンが2つあります。
```c
int pthread_mutex_trylock(pthread_mutex_t *mutex);
int pthread_mutex_timedlock(pthread_mutex_t *mutex,
                            struct timespec *abs_timeout);
```  
これら2つの呼び出しは、ロック取得に使用されます。ロックがすでに保持されている場合、trylockは失敗を返します。一方、timelockは、指定した時間までのロックの試行を試みます。したがって、タイムアウトが0のtimelockは、trylockと同じになってしまいます。これらのバージョンはどちらも一般的に避けなければなりません。しかし、将来の章では(例えば、デッドロックを調べるときなど)、ロック獲得ルーチンに突っ込まれることを避けること(おそらく無期限に)を避けることが有用な場合があります。

## 27.4 Condition Variables
スレッドライブラリのもう1つの主要なコンポーネントであり、確かにPOSIXスレッドの場合は、条件変数が存在します。条件変数は、あるスレッドが別のスレッドが続行する前に何かを実行するのを待っている場合に、何らかの種類のシグナルがスレッド間で行われなければならない場合に便利です。このように対話したいプログラムでは、2つの主要ルーチンが使用されます。
```c
int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex);
int pthread_cond_signal(pthread_cond_t *cond);
```  
条件変数を使用するには、この条件に関連付けられたロックがさらに必要です。上記のいずれかのルーチンを呼び出すときは、このロックを保持する必要があります。

最初のルーチンである`pthread_cond_wait()`は、呼び出し元のスレッドをスリープ状態にして、通常はスリープ中のスレッドが気にするプログラムの何かが変更されたときに、他のスレッドがそれを通知するのを待ちます。典型的な使用法は次のようになります。
```c
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t cond = PTHREAD_COND_INITIALIZER;
Pthread_mutex_lock(&lock);
while (ready == 0)
    Pthread_cond_wait(&cond, &lock);
Pthread_mutex_unlock(&lock);
```  
このコードでは、関連するロックおよび条件の初期化後、スレッドは、変数readyがまだゼロ以外の値に設定されているかどうかを確認します。そうでなければ、スレッドは他のスレッドが起動するまでスリープするために単にwaitルーチンを呼び出します。他のスレッドで実行されるスレッドを起動するコードは次のようになります。
```c
Pthread_mutex_lock(&lock);
ready = 1;
Pthread_cond_signal(&cond);
Pthread_mutex_unlock(&lock);
```
このコードシーケンスについていくつか注意してください。まず、シグナリング(グローバル変数の準備を変更する場合と同様)時には、常にロックを保持するようにします。これにより、誤ってコードに競合状態が導入されることはありません。

第2に、待機コールが第2パラメータとしてロックを取ることに気付くかもしれないが、信号コールは条件を取るだけです。この違いの理由は、呼び出し元のスレッドをスリープ状態にすることに加えて、待機呼び出しが、呼び出し元をスリープ状態にするときにロックを解放するためです。もしそうでなければ、他のスレッドがロックを取得して目覚めるように通知する方法はありますか？しかし、ウォッチした後に戻る前に、`pthread_cond_wait()`はロックを再取得します。したがって、待機中のスレッドが待機シーケンスの開始時に獲得されたロックと最後のロック解除の間で実行されている間、それはロックを保持します。

待機中のスレッドは、単純なif文ではなく、whileループで条件を再チェックします。この章では、将来の章で条件変数を学習するときにこの問題について詳しく説明しますが、一般的にwhileループを使うのは簡単で安全な方法です。条件を再チェックしますが(おそらく少しのオーバーヘッドを追加します)、待っているスレッドを擬似的に起動させるいくつかのpthread実装があります。このような場合、再チェックを行わずに、待機中のスレッドは、条件が変更されていないにもかかわらず変更されたとみなし続けます。したがって、絶対的な事実ではなく、何かが変化したかもしれないというヒントとして目を覚ますことは、より安全です。

条件変数と関連するロックの代わりに、単純なフラグを使用して2つのスレッド間で信号を送ることが魅力的であることに注意してください。たとえば、待機コードでは、上記の待機コードを次のように書き換えることができます。
```c
while (ready == 0)
; // spin
```  
関連するシグナリングコードは次のようになります。
```c
ready = 1;
```  
次のような理由からこれをしないでください。まず、多くの場合、パフォーマンスが低下します(CPUサイクルを浪費するだけの長時間の回転)。第二に、エラーが起こりやすい。最近の研究で[X+10]と表示されているように、フラグを使って(上記のように)スレッド間の同期をとると間違いを犯すのは驚くほど簡単です。その調査では、これらのアドホックな同期の使用の約半分がバグでした！たとえ、そうしなくてもできると思うときでさえ、条件変数を使用してください。

条件変数が混乱して聞こえる場合は、あまり心配する必要はありません(まだ)ので、後の章で詳しく説明します。それまでは、それらが存在することを知り、どのように、なぜそれらが使用されているかを知ることで十分です。

## 27.5 Compiling and Running
この章のすべてのコード例は、起動して実行するのが比較的簡単です。それらをコンパイルするには、コードにヘッダpthread.hを含める必要があります。リンクの行では、-pthreadフラグを追加して、pthreadsライブラリーと明示的にリンクする必要があります。たとえば、単純なマルチスレッドプログラムをコンパイルするには、次の操作が必要です。
```
prompt> gcc -o main main.c -Wall -pthread
```
main.cにpthreadsヘッダーが含まれている限り、並行プログラムを正常にコンパイルしました。ただし、この方法でいつものように、コンパイルがうまくいくかどうかは、まったく別の問題です。

## 27.6 Summary
スレッドの作成、ロックによる相互排他の構築、条件変数によるシグナル伝達と待機を含む、pthreadライブラリの基礎を紹介しました。あなたは、忍耐と大切なケアを除いて、堅牢で効率的なマルチスレッドコードを書くのに他に多くのものを必要としません！

この章では、マルチスレッドコードを記述するときに役立つヒントをまとめています(詳細は次のページを参照してください)興味深いAPIの他の側面があります。より多くの情報が必要な場合は、Linuxシステムでman -k pthreadと入力して、インタフェース全体を構成する100以上のAPIを確認してください。ただし、ここで説明する基本的な機能は、洗練された(うまくいけば、正しい、パフォーマンスの高い)マルチスレッドプログラムを構築できるようにする必要があります。スレッドを持つ難しい部分はAPIではなく、並行プログラムをどのように構築するかの難しい論理です。詳細は以下をお読みください。

>> ASIDE: THREAD API GUIDELINES  
>> POSIXスレッドライブラリ(または実際には任意のスレッドライブラリ)を使用してマルチスレッドプログラムを構築する際には、覚えておくべき重要なことはいくつかあります。  
• 単純にする。何よりも、スレッド間のロックやシグナルのコードはできるだけシンプルでなければなりません。トリッキーなスレッドのやり取りはバグにつながります。  
•スレッドのやりとりを最小限に抑えます。スレッドが相互作用する方法の数を最小限に保つようにしてください。それぞれの相互作用は、慎重に考察し、試練された真のアプローチで構築されるべきです(その多くは、今後の章で学ぶ)。  
•ロックと条件変数を初期化する。そうしないと、時にはうまく動作しないことがあり、時には非常に奇妙な方法で失敗することがあります。  
•リターンコードを確認します。もちろん、どのようなCやUNIXプログラミングでも、それぞれのリターンコードをチェックする必要があります。ここでもそうです。そうでなければ​​行動が分かりにくくなり、(a)悲鳴を上げる、(b)髪の毛を抜く、(c)両方をする可能性が高くなります。  
•スレッドに引数を渡したり、スレッドから値を返す方法には注意してください。特に、スタックに割り当てられた変数への参照を渡すときは、おそらく何か間違っているでしょう。  
•各スレッドには独自のスタックがあります。上記の点に関連して、各スレッドには独自のスタックがあることに注意してください。したがって、あるスレッドが実行している関数の中にローカルに割り当てられた変数がある場合、それはそのスレッドにとって本質的にプライベートです。他のスレッドはそれに(簡単に)アクセスできません。スレッド間でデータを共有するには、値がヒープにあるか、またはグローバルにアクセス可能なロケールでなければなりません。  
•常にスレッド間で信号を送るには、条件変数を使用します。それはしばしば単純なフラグを使用することを魅力的ですが、しないでください。  
•マニュアルページを使用してください。Linuxでは、特に、pthreadのmanページは非常に有益であり、ここで紹介したニュアンスの多くについて、さらに詳細に議論します。注意深く読んでください！  

## 参考文献
[B89] “An Introduction to Programming with Threads”  
Andrew D. Birrell  
DEC Technical Report,January, 1989  
Available: https://birrell.org/andrew/papers/035-Threads.pdf  
A classic but older introduction to threaded programming. Still a worthwhile read, and freely available.  

[B97] “Programming with POSIX Threads”  
David R. Butenhof  
Addison-Wesley, May 1997  
Another one of these books on threads.  

[B+96] “PThreads Programming:A POSIX Standard for Better Multiprocessing”  
Dick Buttlar, Jacqueline Farrell, Bradford Nichols  
O’Reilly, September 1996  
A reasonable book from the excellent, practical publishing house O’Reilly. Our bookshelves certainly contain a great deal of books from this company, including some excellent offerings on Perl, Python, and Javascript (particularly Crockford’s “Javascript: The Good Parts”.)

[K+96] “Programming With Threads”  
Steve Kleiman, Devang Shah, Bart Smaalders  
Prentice Hall, January 1996  
Probably one of the better books in this space. Get it at your local library. Or steal it from your mother. More seriously, just ask your mother for it – she’ll let you borrow it, don’t worry.  

[X+10] “Ad Hoc Synchronization Considered Harmful”  
Weiwei Xiong, Soyeon Park, Jiaqi Zhang, Yuanyuan Zhou, Zhiqiang Ma  
OSDI 2010, Vancouver, Canada  
This paper shows how seemingly simple synchronization code can lead to a surprising number of bugs. Use condition variables and do the signaling correctly!  

\newpage

# 28 Locks
並行処理の導入から同時プログラミングの基本的な問題の1つがわかりました。一連の命令を原子性で実行したいのですが、単一プロセッサ(または複数のプロセッサで同時に実行される複数のスレッド)に割り込みがあるため、私たちはできませんでした。この章では、ロックと呼ばれるものを導入して、この問題を直接攻撃しています。プログラマは、ソースコードにロックを付けてクリティカルセクションの周りに置くことで、そのようなクリティカルセクションが単一のアトミック命令であるかのように実行するようにします。

## 28.1 Locks: The Basic Idea
例として、クリティカルセクションが共有変数の標準的な更新であるこのように見えると仮定します。
```c
balance = balance + 1;
```
もちろん、リンクリストに要素を追加するなど、他の重要なセクションも可能ですが、ここではこの簡単な例を続けます。ロックを使用するには、次のようにクリティカルセクションの周りにいくつかのコードを追加します。
```c
1 lock_t mutex; // some globally-allocated lock ’mutex’
2 ...
3 lock(&mutex);
4 balance = balance + 1;
5 unlock(&mutex);
```  
ロックは単なる変数であるため、ロックを使用するには、何らかのロック変数(上記のmutexなど)を宣言する必要があります。このロック変数(または単に「ロック」と略記)は、任意の瞬間にロックの状態を保持します。これは使用可能(またはロック解除またはフリー)なので、スレッドがロックを保持していないか、または取得(またはロックまたは保持)されていないため、ちょうど1つのスレッドがロックを保持しており、ロックを保持しているスレッドや、ロック獲得を注文するためのキューなど、他の情報もデータ型に格納できますが、そのような情報はロックのユーザーからは隠されています。

`lock()`と`unlock()`ルーチンの中身は単純です。ルーチン`lock()`を呼び出すと、ロックを取得しようとします。他のスレッドがロックを保持していない(すなわち解放している)場合、スレッドはロックを取得してクリティカルセクションに入ります。このスレッドはロックの所有者と呼ばれることがあります。別のスレッドが、同じロック変数(この例ではmutex)の`lock()`を呼び出すと、ロックが別のスレッドによって保持されている間は戻りません。このようにして、ロックを保持している最初のスレッドがそこにある間に、他のスレッドがクリティカルセクションに入ることが防止されます。

ロックの所有者が`unlock()`を呼び出すと、ロックは再び使用可能(空き)になります。他のスレッドがロックを待っていない場合(すなわち、他のスレッドが`lock()`を呼び出していない場合)、ロックの状態は単にフリーに変更されます。待機スレッド(`lock()`にスタック)がある場合、そのうちの1つは、ロック状態のこの変更を通知(または通知)し、ロックを取得し、クリティカルセクションに入ります。

ロックは、プログラマーにスケジューリングするための制御を最小限に抑えます。一般に、スレッドはプログラマによって作成されたエンティティとして認識されますが、様々な方法でOSによってスケジュールされます。ロックはその制御の一部をプログラマーに返します。コードの一部にロックをかけることで、プログラマーはそのコード内で単一のスレッドしかアクティブにならないことを保証することができます。したがって、ロックは、伝統的なOSスケジューリングであるカオスをより制御されたアクティビティーに変換するのに役立ちます。

## 28.2 Pthread Locks
ロックのためにPOSIXライブラリが使用する名前は、スレッド間の相互排他を提供するために使用されるmutexです。つまり、あるスレッドがクリティカルセクションにある場合、他のスレッドはセクションを完了するまで入力を排除します。したがって、次のPOSIXスレッドコードを参照すると、上記と同じことが行われていることを理解する必要があります(ロックとロック解除時にエラーをチェックするラッパーも使用します)。
```c
1 pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
2
3 Pthread_mutex_lock(&lock); // wrapper for pthread_mutex_`lock()`
4 balance = balance + 1;
5 Pthread_mutex_unlock(&lock);
```  
また、異なる変数を保護するために異なるロックを使用している可能性があるため、POSIXバージョンでは変数をロックおよびロック解除することに注意してください。これにより、並行性が向上します。クリティカルセクションにアクセスするたびに使用される1つの大きなロック(粗いロック戦略)の代わりに、異なるデータとデータ構造を異なるロックで保護することが多くなります。したがって、一度に多くのスレッドをロックされたコードにすることができます。

## 28.3 Building A Lock
ここまでで、プログラマの視点から、ロックがどのように機能するかを理解しておく必要があります。しかし、どのようにロックを構築する必要がありますか？どのハードウェアサポートが必要ですか？どのOSがサポートしていますか？この章の残りの部分で取り上げるのは、この一連の質問です。

>> THE CRUX: HOW TO BUILD A LOCK  
>> どのようにして効率的なロックを構築できますか？効率的なロックは低コストで相互排除を提供し、また以下で議論するいくつかの他の特性を達成するかもしれない。どのハードウェアサポートが必要ですか？どのOSがサポートしていますか？

動くロックを構築するためには、私たちの昔の友人、ハードウェア、私たちの良い仲間、OSから何らかの助けが必要です。長年にわたり、さまざまなコンピュータアーキテクチャの命令セットに、いくつかの異なるハードウェアプリミティブが追加されました。私たちはこれらの命令がどのように実装されているのかを研究するつもりはないですが(結局のところ、コンピュータ・アーキテクチャ・クラスの話題である)、それらを使用してロックのような相互排他プリミティブを構築する方法を研究します。また、OSがどのように関係してロックを完成させ、洗練されたロックライブラリを構築できるようにするかについても検討します。

## 28.4 Evaluating Locks
ロックを構築する前に、まず目標が何であるかを理解し、特定のロック実装の有効性を評価する方法を尋ねる必要があります。ロックが機能するかどうかを評価するには、まず基本的な基準を確立する必要があります。最初は、ロックが基本的な作業を行うかどうかです。これは相互排除を提供することです。基本的には、ロックが機能し、複数のスレッドがクリティカルセクションに入るのを防ぎますか？

第二は公平です。ロックを獲得しようとしている各スレッドは、一度解放されるとすぐに獲得することができますか？これを見るもう1つの方法は、より極端なケースを調べることです。ロックを競合するスレッドは、その間に飢えてしまい、二度と取得できないでしょうか？

最終的な基準はパフォーマンスです。具体的には、ロックを使用して追加される時間オーバーヘッドです。ここで考慮する価値のあるいくつかの異なるケースがあります。1つは競合がない場合です。単一のスレッドが実行され、ロックを取得して解放するとき、そのようなオーバーヘッドは何ですか？もう1つは、複数のスレッドが1つのCPU上のロックに対して競合している場合です。この場合、パフォーマンスの問題はありますか？最後に、複数のCPUが関わっているときにロックがどのように動作し、それぞれがロックを競合しますか？これらのさまざまなシナリオを比較することで、以下で説明するように、さまざまなロック手法を使用することによるパフォーマンスの影響をよりよく理解できます。

## 28.5 Controlling Interrupts
相互排除を提供するために使用された最も初期の解決策の1つは、クリティカルセクションの割り込みを無効にすることでした。この解決策は、シングルプロセッサシステム向けに開発されました。コードは次のようになります。
```c
1 void lock() {
2 DisableInterrupts();
3 }
4 void unlock() {
5 EnableInterrupts();
6 }
```
このようなシングルプロセッサシステムで実行していると仮定します。クリティカルセクションに入る前に、ある種の特別なハードウェア命令を使用して割り込みをオフにすることで、クリティカルセクション内のコードが中断されないようにし、アトミックであるかのように実行します。終了すると、(ハードウェア命令を介して)割り込みを再度有効にして、プログラムを通常どおり実行します。

このアプローチの主な点は、単純さです。あなたは確かに、なぜこれが動作するかを理解するためにあなたの頭を悩ます必要はありません。中断することなく、スレッドは、実行するコードが実行され、他のスレッドがそのスレッドに干渉しないことを確実にすることができます。

残念ながら、欠点は多くあります。第1に、このアプローチでは、呼び出しスレッドが特権操作(割り込みのオンとオフを切り替える)を実行できるようにする必要があるため、この機能が悪用されていないことを信頼する必要があります。ご存知のように、任意のプログラムを信頼する必要がある場合は、おそらく問題に陥っている可能性があります。ここで、問題は数多くの形で現れます。欲張りなプログラムは、実行開始時に`lock()`を呼び出してプロセッサを独占することができます。悪質なプログラムや悪意のあるプログラムが`lock()`を呼び出して無限ループに陥る可能性があります。この後者の場合、OSはシステムの制御を元に戻すことはありません。解決する唯一の手段はシステムを再起動する方法です。汎用の同期ソリューションとして割り込みを無効にするには、アプリケーションをあまりにも多くの信頼を必要とします。

第2に、このアプローチはマルチプロセッサでは機能しません。複数のスレッドが異なるCPU上で実行されており、それぞれが同じクリティカルセクションを入力しようとすると、割り込みが無効かどうかは関係ありません。スレッドは他のプロセッサ上で実行できるため、クリティカルセクションに入る可能性があります。マルチプロセッサが普及している現在、我々の一般的な解決策はこれよりも優れていなければなりません。

第3に、割り込みを長時間オフにすると、割り込みが失われ、重大なシステムの問題が発生する可能性があります。たとえば、ディスクデバイスが読み取り要求を完了したという事実をCPUが逃したとします。OSは、この読み込みを待っているプロセスを復帰させる方法をどのように知っていますか？

最後に、おそらく最も重要はほぼないですが、このアプローチは非効率的である可能性があります。通常の命令実行と比較して、割り込みをマスクまたはアンマスクするコードは、最新のCPUによってゆっくり実行される傾向があります。これらの理由から、割り込みを無効にすることは、相互排除プリミティブとして限定されたコンテキストでのみ使用されます。たとえば、オペレーティングシステム自体が、自身のデータ構造にアクセスするとき、または少なくとも混乱した割り込み処理状況が発生しないようにするために、原子性を保証するために割り込みマスクを使用する場合があります。この使用法は理にかなっています。なぜなら、信頼問題がOS内部で消滅するためです。OSは、常に特権操作を実行することを常に信頼しています。

## 28.6 A Failed Attempt: Just Using Loads/Stores
割り込みベースの技術を超えて移動するためには、CPUハードウェアと適切なロックを構築するために提供される命令に依存する必要があります。最初に単一のフラグ変数を使用して単純なロックを構築しようとしましょう。この失敗した試みでは、ロックを構築するために必要ないくつかの基本的なアイデアが表示されます。なぜなら、単一の変数を使用し、通常のロードおよびストアを介してアクセスするだけでは不十分である理由が分かります。

この最初の試み(図28.1)では、アイデアは非常に単純です。単純な変数(フラグ)を使用して、あるスレッドがロックを所有しているかどうかを示します。クリティカルセクションに入る最初のスレッドは、フラグが1(この場合はそうではない)かどうかをテストする`lock()`を呼び出し、スレッドが現在ロックを保持していることを示すフラグを1に設定します。クリティカルセクションが終了すると、スレッドは`unlock()`を呼び出し、フラグをクリアして、ロックがもはや保持されていないことを示します。

![](../28/img/fig28_1.PNG)

最初のスレッドがクリティカルセクションにある間に別のスレッドが`lock()`を呼び出すと、そのスレッドが`unlock()`を呼び出してフラグをクリアするwhileループでスピン待機します。その最初のスレッドがそうすると、待機中のスレッドはwhileループから抜け出し、フラグを1に設定してクリティカルセクションに進みます。

残念ながら、コードには2つの問題があります。1つは正当性、もう1つはパフォーマンスです。正当性の問題は、一度コンカレント・プログラミングを考えるのに慣れれば、簡単に理解できます。図28.2のコードインタリーブを想像してみてください。開始するにはflag = 0とします。このインタリーブからわかるように、タイムリーな(もしくは、タイムリーではなくても)割り込みでは、両方のスレッドがフラグを1に設定し、両方のスレッドがクリティカルセクションに入ることができるケースを簡単に生成できます。この振る舞いは、専門家が「悪い」と呼ぶものであり、相互排除を提供するという最も基本的な要件を明らかに満たしていません。

パフォーマンス問題は、後で詳しく説明しますが、スレッドが既に保持されているロックを取得するために待機する方法です。つまり、フラグの値を無期限にチェックするという方法です。スピン待機は、別のスレッドがロックを解放するのを待って時間を浪費します。ウェイターとして待っているスレッドは実行できません(少なくともコンテキスト切り替えが発生するまで)、ユニプロセッサー上での廃棄は非常に高いです！したがって、より洗練されたソリューションを開発する際には、このような無駄を回避する方法も考慮する必要があります。

![](../28/img/fig28_2.PNG)

## 28.7 Building Working Spin Locks with Test-And-Set
割り込みを無効にすることは複数のプロセッサでは機能しないため、ロードとストアを使用する簡単な方法(前述)が機能しないため、システム設計者はハードウェアによるロックのサポートを開始しました。1960年代初期のBurroughs B5000 [M82]のような最も初期のマルチプロセッサシステムでは、このようなサポートがありました。今日では、単一のCPUシステムであっても、すべてのシステムがこのタイプのサポートを提供しています。理解するための最も簡単なハードウェアサポートは、アトミック交換とも呼ばれるテストアンドセット命令として知られています。以下のCコードスニペットを使って、テスト・アンド・セット命令の動作を定義します。
```c
1 int TestAndSet(int *old_ptr, int new) {
2 int old = *old_ptr; // fetch old value at old_ptr
3 *old_ptr = new; // store ’new’ into old_ptr
4 return old; // return the old value
5 }
```
テスト・アンド・セット命令が行うことは、次のとおりです。ptrが指す古い値を返し、同時にその値を新しい値に更新します。ここで重要な点は、この一連の操作が原子的に実行されることです。"テストと設定"と呼ばれる理由は、古い値(返される値)を"テスト"しながら同時にメモリ位置を新しい値に"設定"できるようにすることです。実際には、図28.3で検討しているように、このわずかに強力な説明は単純なスピンロックを構築するのに十分です。

>> ASIDE: DEKKER’S AND PETERSON’S ALGORITHMS  
>> 1960年代、ダイクストラは彼の友人に同時性問題を提起し、そのうちの1人、Theodorus Jozef Dekkerという数学者が解決策を思いついた[D68]。特別なハードウェア命令とOSサポートを使用するここで説明するソリューションとは異なり、Dekkerのアルゴリズムはロードとストアだけを使用します(ハードウェアが初期のハードウェアでは不可分であったとみなします)。デッカーのアプローチは後にPeterson [P81]によって洗練されました。再び、ロードとストアだけが使用され、2つのスレッドがクリティカルセクションに同時に入らないようにすることが考えられます。ここにPetersonのアルゴリズム(2つのスレッド用)があります。コードを理解できるかどうかを確認してください。フラグとターン変数は何のために使われますか？
```c
int flag[2];
int turn;

void init() {
    flag[0] = flag[1] = 0; // 1->thread wants to grab lock
    turn = 0; // whose turn? (thread 0 or 1?)
}
void lock() {
    flag[self] = 1; // self: thread ID of caller
    turn = 1 - self; // make it other thread’s turn
    while ((flag[1-self] == 1) && (turn == 1 - self))
        ; // spin-wait
}
void unlock() {
    flag[self] = 0; // simply undo your intent
}
```
>> なんらかの理由で、特別なハードウェアサポートなしで動作するロックを開発することは、しばらくの間盛り上がり、理論型に多くの問題を取り組んでいます。もちろん、この作業ラインは、ハードウェアサポートを少しでも簡単に実現できると気づいたときには無用になりました(実際、サポートはマルチプロセッシングの初期から行われていました)。さらに、上記のようなアルゴリズムは、(緩和されたメモリ一貫性モデルのため)現代的なハードウェアでは機能しないため、あまり有用ではありません。つまり、より多くの研究は歴史のゴミ箱に...

![](../28/img/fig28_3.PNG)

なぜこのロックが動作するのかを理解してみましょう。最初にスレッドが`lock()`を呼び出し、現在他のスレッドがロックを保持していない場合を想像してください。したがって、フラグは0になります。スレッドがTestAndSet(flag,1)を呼び出すと、ルーチンはflagの古い値を返します。これは0です。したがって、flagの値をテストしている呼び出し元のスレッドは、whileループでスピンを捕らえることはなく、ロックを取得します。スレッドはアトミックに値を1に設定し、ロックが現在保持されていることを示します。スレッドがクリティカルセクションで終了すると、`unlock()`を呼び出してフラグをゼロに戻します。

1つのスレッドが既にロックを保持している(すなわち、フラグが1である)ときに、想像できる第2の場合が生じます。この場合、このスレッドは`lock()`を呼び出し、TestAndSet(flag、1)も呼び出します。今度は、`TestAndSet()`はフラグを1に戻し(ロックが保持されているため)、再び1に設定します。ロックが別のスレッドによって保持されている限り、`TestAndSet()`は1を繰り返し返します。したがって、このスレッドは、ロックが最後に解放されるまで回転して回転します。フラグが最後に他のスレッドによって0に設定されると、このスレッドは再び`TestAndSet()`を呼び出します。これは、値を1に原子的に設定している間に0を返し、ロックを取得しクリティカルセクションに入ります。

(以前のロック値の)テストと新しい値のセットの両方を1つのアトミック操作にすることで、1つのスレッドだけがロックを取得するようにします。それで、相互排除の基本原則を構築する方法です！

このタイプのロックを通常スピンロックといいます。構築する最も単純なタイプのロックであり、ロックが利用可能になるまでCPUサイクルを使用して単純に回転します。単一のプロセッサ上で正しく動作するためには、プリエンプティブスケジューラ(すなわち、時々異なるスレッドを実行するために、タイマを介してスレッドを中断するスケジューラ)が必要である。プリエンプションなしでは、スピンロックは単一のCPUではあまり意味がありません。CPU上のスレッドが決してそれを放棄しないためです。

## 28.8 Evaluating Spin Locks
基本的なスピンロックが与えられているので、これまで説明した軸に沿ってどれだけ効果的かを評価することができます。 ロックの最も重要な側面は正しさです。それは相互排除を提供しますか？ ここでの答えは「はい」です。スピンロックでは、一度に1つのスレッドだけがクリティカルセクションに入ることができます。 したがって、我々は正しいロックを持っています。

>> TIP: THINK ABOUT CONCURRENCY AS MALICIOUS SCHEDULER  
>> この例から、同時実行を理解するために必要なアプローチを理解することができます。何をしようとすべきなのかは、あなたが悪意のある人物になってみることです。同期プリミティブをビルドする際の微妙な試行錯誤を避けるために、スレッドを最も不適切なタイミングで中断します。あなたはスケジューラなのですか？割り込みの正確なシーケンスは不可能かもしれませんが、それは可能であり、特定のアプローチが機能しないことを実証する必要があります。それは悪意を持って考えるのに役立ちます！(少なくとも、時々)

次の軸は公平です。待機スレッドへのスピンロックはどれくらい公正ですか？待っているスレッドがクリティカルセクションに入ることを保証できますか？残念ながら、ここの答えは悪いニュースです。スピンロックは公正を保証しません。実際、糸の回転は競合の下で永遠に回転することがあります。単純なスピンロック(これまで説明したように)は公平ではなく、飢餓につながる可能性があります。

最終的な軸はパフォーマンスです。スピンロックを使用するコストはいくらですか？これをより慎重に分析するには、いくつかの異なるケースについて考えることをおすすめします。最初は、単一のプロセッサ上でロックを競合するスレッドを想像してください。2番目の方法では、スレッドが多くのCPUに分散されていることを考慮してください。

スピンロックの場合、シングルCPUの場合、パフォーマンスのオーバーヘッドは非常に苦しいことがあります。ロックを保持しているスレッドがクリティカルセクション内で先取りされている場合を想像してください。その後、スケジューラは、他のすべてのスレッド(N-1個の他のスレッドがあると想像してください)を実行し、それぞれがロックを取得しようとします。この場合、それらのスレッドはそれぞれ、CPUを諦める前にタイムスライスの期間スピンし、CPUサイクルが浪費されます。

ただし、複数のCPUでは、スピンロックは正常に機能します(スレッド数がCPU数にほぼ等しい場合)。思考は次のようになります。スレッドAをCPU 1に、スレッドBをCPU 2上に想像してください。どちらもロックを競合しています。スレッドA(CPU1)がロックを取得し、スレッドBがそれを試みると、Bは(CPU2上で)スピンします。しかし、おそらくクリティカルセクションが短く、すぐにロックが利用可能になり、スレッドBによって取得されます。他のプロセッサで保持されているロックを待つためにスピンすることは、この場合には多くのサイクルを無駄にしないので効果的です。

## 28.9 Compare-And-Swap
いくつかのシステムが提供するもう1つのハードウェアプリミティブは、compare-and-swap命令(たとえばSPARC上で呼び出される命令)、またはcompare-and-exchange(x86上で呼び出される)と呼ばれます。この単一命令のC擬似コードは、図28.4にあります。基本的な考え方は、ptrで指定されたアドレスの値が期待どおりであるかどうかをテストするための比較とスワップです。その場合は、ptrが指し示すメモリ位置を新しい値で更新します。そうでない場合は、何もしないでください。どちらの場合でも、実際の値をそのメモリ位置に戻すことで、compare-and-swapを呼び出すコードが成功したかどうかを知ることができます。compare-and-swap命令を使用すると、テストセットの場合と非常によく似た方法でロックを構築できます。たとえば、上記の`lock()`ルーチンを次のものに置き換えることができます。

![](../28/img/fig28_4.PNG)

```c
1 void lock(lock_t *lock) {
2   while (CompareAndSwap(&lock->flag, 0, 1) == 1)
3       ; // spin
4 }
```
コードの残りの部分は上記のテストセットの例と同じです。このコードは全く同じように動作します。フラグが0であるかどうかを単純にチェックし、そうであれば、アトミックに1に入れ替えてロックを取得します。ロックが保持されている間にロックを取得しようとするスレッドは、ロックが最後に解放されるまで回転を停止します。compare-and-swapのC呼び出し可能なx86版を実際に作成する方法を知りたい場合は、このコードシーケンスが便利です([S05]から)
```c
1 char CompareAndSwap(int *ptr, int old, int new) {
2   unsigned char ret;
3
4   // Note that sete sets a ’byte’ not the word
5   __asm__ __volatile__ (
6       " lock\n"
7       " cmpxchgl %2,%1\n"
8       " sete %0\n"
9       : "=q" (ret), "=m" (*ptr)
10      : "r" (new), "m" (*ptr), "a" (old)
11      : "memory");
12  return ret;
13 }
```
最後に、感知したように、比較とスワップはテストとセットより強力な命令です。ロックフリー同期[H91]などのトピックを簡単に掘り下げて、今後このパワーを活用していく予定です。しかし、単純なスピンロックを作成するだけでは、その動作は上記で分析したスピンロックと同じです。

## 28.10 Load-Linked and Store-Conditional
プラットフォームによっては、クリティカルセクションを作成するのに役立つ一対の命令が用意されています。例えば、MIPSアーキテクチャ[H93]では、ロードリンク命令とストア条件命令は、ロックや他の並行構造を構築するために並行して使用できます。これらの命令のC擬似コードは図28.5のとおりです。Alpha、PowerPC、およびARMは同様の命令[W09]を提供しています。

![](../28/img/fig28_5.PNG)

ロードリンクは、一般的なロード命令のように動作し、メモリから値を取り出してレジスタに配置します。キーの違いはstore-conditionalにあります。store-conditionalは、アドレスへの介在するストアが発生していない場合にのみ成功します(ロードされたアドレスに格納された値を更新します)成功の場合、storeconditionalは1を返し、ptrの値をvalueに更新します。失敗した場合、ptrの値は更新されず、0が返されます。  
あなた自身の挑戦として、ロードリンクとストア条件付きを使用してロックを構築する方法を考えてみてください。次に、終了したら、以下のコードを見て、簡単な解決策を提供してください。解決策は図28.6のとおりです。

![](../28/img/fig28_6.PNG)

`lock()`コードは唯一興味深い部分です。最初に、スレッドは、フラグが0に設定されるのを待って回転します(したがって、ロックが保持されていないことを示します)。一旦そのスレッドがstore-conditionalを介してロックを取得しようとすると、成功した場合、スレッドはフラグの値を原子的に1に変更し、クリティカルセクションに進むことができます。ストア条件の失敗がどのように発生する可能性があるかに注意してください。1つのスレッドは`lock()`を呼び出し、load-linkedを実行し、ロックが保持されていない場合は0を返します。tore-conditionalを試みる前に、それは中断され、別のスレッドがロックコードを入力し、ロードリンクされた命令を実行し、0も取得し続けます。この時点で、2つのスレッドがそれぞれロードリンクされて実行され、それぞれがストア条件を試行しようとしています。これらの命令の重要な特徴は、フラグを1に更新してロックを獲得するのに、これらのスレッドの1つだけが成功することです。store-conditionalを試みる第2のスレッドは失敗し(他のスレッドがload-linkedとstoreconditionalの間でフラグの値を更新したため)、再度ロックを取得しようとする必要があります。

>> TIP: LESS CODE IS BETTER CODE (LAUER’S LAW)  
>> プログラマーは、何かをするために書いたコードの量を自慢する傾向があります。そうすることは根本的に壊れている。自慢するべきは、むしろ、与えられた仕事を達成するために書いたコードの量です。簡潔で簡潔なコードが常に優先されます。理解しやすくなり、バグも少なくなります。Hugh Lauer氏は、パイロットオペレーティングシステムの構築について議論するとき、「同じ人が2倍の時間があれば、コードの半分でシステムを生み出すことができます」[L81]このLauerの法則を覚えておく価値があります。次回は、割り当てを完了するために書いたコードの量がどれほどであるか自慢しています。もう一度考え直してください。できるだけ明確かつ簡潔にコードを書き直してください。

数年前の授業では、学部生のDavid Capelが短絡ブール条件を楽しむあなたのために、上記のより簡潔な形式を提案しました。なぜそれが同等であるかを知ることができるかどうかを見てください。それは確かに短いです！
```c
1 void lock(lock_t *lock) {
2   while (LoadLinked(&lock->flag)||!StoreConditional(&lock->flag, 1))
3       ; // spin
4 }
```

## 28.11 Fetch-And-Add
1つの最終的なハードウェアプリミティブはフェッチアンドアド命令です。この命令は、アトミックに値をインクリメントし、特定のアドレスに古い値を戻します。fetch-and-add命令のC疑似コードは次のようになります。
```c
1 int FetchAndAdd(int *ptr) {
2   int old = *ptr;
3   *ptr = old + 1;
4   return old;
5 }
```
この例では、Mellor-CrummeyとScott [MS91]が紹介したように、fetch-and-addを使用してより面白いチケットロックを構築します。ロックとアンロックのコードは図28.7のようになります。このソリューションでは、単一の値の代わりにチケットとターン変数を組み合わせて使用してロックを構築します。基本的な操作は非常に簡単です。スレッドがロックを取得したい場合は、最初にアトミックフェッチとアペンドをチケットの値で行います。その値はこのスレッドの「ターン」(myturn)とみなされます。次に、グローバルに共有されているlock->turnを使用して、そのスレッドの順番を判断します。特定のスレッドで(myturn == turn)のときは、そのスレッドがクリティカルセクションに入るのです。アンロックは、次の待機スレッド(存在する場合)がクリティカルセクションに入ることができるように、ターンをインクリメントするだけで達成されます。

![](../28/img/fig28_7.PNG)

このソリューションとの重要な違いは、これまでの試みとは異なり、すべてのスレッドの進捗状況を保証します。スレッドにチケットの値が割り当てられると、そのスレッドの前にあるスレッドがクリティカルセクションを通過してロックを解除されると、将来のある時点でスケジュールされます。以前の試みでは、そのような保証は存在しませんでした。他のスレッドがロックを取得して解放しても、テストセットで回転するスレッドは永遠に回転する可能性があります。

## 28.12 Too Much Spinning: What Now?
私たちのシンプルなハードウェアベースのロックはシンプルで(数行のコードしかありません)、どんなシステムやコードの2つの優れた特性でも動作します(あなたが望むなら、いくつかのコードを書いても証明できます)。しかし、場合によっては、これらのソリューションは非常に非効率的である可能性があります。1つのプロセッサで2つのスレッドを実行しているとします。今度は、1つのスレッド(スレッド0)がクリティカルセクションにあり、したがってロックが保持され、残念なことに中断されることを想像してください。2番目のスレッド(スレッド1)はロックを取得しようとしましたが、保持されています。したがって、それは回転を開始します。スピンします。その後、もう少し回転します。最後に、タイマー割込みがオフになり、スレッド0が再び実行されてロックが解除され、最後に(スレッドが次に実行されるとき)、スレッド1はそれほど回転する必要はなく、ロックを取得できるようになります。このように、スレッドがこのような状況で回転したときはいつでも、変更を行わない値をチェックするだけで時間切れ全体を無駄にします。問題は、ロックを競合するN個のスレッドで悪化します。N - 1タイムスライスは同様の方法で無駄になり、単なるスレッドが回転を止めてロックを解除するのを待っています。

>> THE CRUX: HOW TO AVOID SPINNING  
>> CPUの時間を無駄にしないロックを開発するにはどうしたらいいですか？

ハードウェアのサポートだけでは問題を解決することはできません。私たちもOSのサポートが必要です！それがどのように機能するかを今考えてみましょう。

## 28.13 A Simple Approach: Just Yield, Baby
ハードウェアのサポートは私たちをかなり遠くにしてくれました。つまり、作業ロック、ロック取得の公平性(チケットロックの場合のように)が得られるようになりました。しかし、我々はまだ問題があります。クリティカルセクションでコンテキストスイッチが発生し、スレッドが無限に回転し始め、中断された(ロックを保持している)スレッドが再び実行されるのを待っていますか？

私たちの最初の試みは、シンプルでフレンドリーなアプローチです。スピンするときに、代わりにCPUを別のスレッドであきらめてください。あるいは、アル・デイヴィスが言うように、“just yield, baby!” [D91]。図28.8にそのアプローチを示します。

![](../28/img/fig28_8.PNG)

このアプローチでは、CPUをあきらめて別のスレッドを実行させたいときにスレッドが呼び出すことができるオペレーティングシステムのプリミティブ`yield()`を想定しています。スレッドは3つの状態(実行中、準備中、ブロック中)のいずれかになります。yieldは、呼び出し元を実行状態から準備完了状態に移動させるシステムコールであり、したがって別のスレッドを実行するように促します。したがって、諦めたプロセスは本質的にそれ自体をスケジュールから外します。

1つのCPU上に2つのスレッドを持つ例を考えてみましょう。この場合、yield base アプローチは非常にうまく機能します。スレッドが`lock()`を呼び出して保持されているロックを見つけた場合、それは単にCPUを諦めるため、もう一方のスレッドで実行してクリティカルセクションを終了します。この単純なケースでは、利回りアプローチがうまく機能します。

ここで、ロックを繰り返し競合するスレッド(たとえば100)が多数ある場合を考えてみましょう。この場合、一方のスレッドがロックを取得して解放する前に先取りされた場合、他方の99スレッドはそれぞれ`lock()`を呼び出し、保持されているロックを見つけてCPUをy諦めます。いくつかの種類のラウンドロビンスケジューラーを仮定すると、ロックを保持しているスレッドが再び実行される前に99のそれぞれがこの実行とyieldをします。回転方法(99回のスライスが回転するのを無駄にする)よりは優れていますが、この方法はまだコストがかかります。コンテクストスイッチのコストが相当に高くなる可能性があり、そのために無駄が多くなります。

さらに悪いことに、我々は飢餓問題に全く取り組んでいません。他のスレッドが繰り返しクリティカルセクションに入り、クリティカルセクションを終了する間、スレッドは無限のyieldループで捕まえられることがあります。この問題に直接対処するアプローチが必要であることは明らかです。

## 28.14 Using Queues: Sleeping Instead Of Spinning
私たちの以前のアプローチの本当の問題は、あまりにも多くのチャンスを残すことです。スケジューラは、次に実行するスレッドを決定します。スケジューラが悪い選択をした場合、ロックを待つ(最初のアプローチ)か、すぐにCPUを降らなければならないスレッドが実行されます(2番目のアプローチ)。いずれにしても、無駄になる可能性があり、飢餓を防ぐことはできません。

したがって、現在の保有者がロックを解放した後にスレッドが次にロックを獲得するような制御を明示的に行う必要があります。これを行うには、もう少しOSサポートと、ロックを獲得するのを待っているスレッドを追跡するキューが必要です。

わかりやすくするために、私たちは、呼び出し元のスレッドをスリープ状態にするための`park()`と、threadIDで指定された特定のスレッドを起動させるためのunpark(threadID)の2つの呼び出しに関して、Solarisによって提供されるサポートを使用します。これらの2つのルーチンは、保持されたロックを獲得しようとすると呼び出し元をスリープ状態にし、ロックが解放されたときにスリープ状態に入るロックを構築するために並行して使用できます。このようなプリミティブの1つの可能な使用を理解するために、図28.9のコードを見てみましょう。

この例では、いくつか面白いことをしています。まず、以前のテストと設定のアイデアをロックを待っているスレッドの明示的なキューと組み合わせて、より効率的なロックを作成します。次に、キューを使用して、次にロックを取得するユーザーを制御し、飢餓を回避します。

![](../28/img/fig28_9.PNG)

基本的には、ロックが使用しているフラグやキューの操作を中心としたスピンロックとして、ガードがどのように使用されているかを知ることができます(図28.9)。したがって、このアプローチはスピン待機を完全に回避しません。ロックを取得または解放している間にスレッドが中断され、他のスレッドがこのスレッドを再び実行するために回転待ちする原因となります。しかし、スピニングに費やされる時間は非常に限られており(ユーザー定義のクリティカルセクションではなく、ロックおよびアンロックコード内のほんの数個の命令)、このアプローチは妥当である可能性があります。

第2に、`lock()`では、スレッドがロックを取得できない(既に保持されている)ときに、自分自身をキューに追加することに注意して(`gettid()`関数を呼び出して現在のスレッドのスレッドIDを取得します)、ガードを0に設定してCPUを諦めさせます。ここで読者のための質問です。ガードロックのリリースが`park()`の後に来た場合、どうなりますか？ヒントは「何か悪い」です。

別のスレッドが起動したときにフラグが0に戻らないという興味深い事実に気付くかもしれません。どうしてこうなるのでしょうか？さて、それはエラーではなく、むしろ必然です！スレッドが起動すると、あたかも`park()`から戻るかのようになります。ただし、コードのその時点でガードを保持していないため、flagを1に設定することさえできません。したがって、ロックをスレッドから直接渡して、次のスレッドにロックを解放します。flagは0の間に設定されません。

最後に、`park()`の呼び出しの直前に、ソリューション内で認識される競合状態に気付くかもしれません。タイミングが間違っていると、ロックは保持されなくなるまでスリープしなければならないとして、スレッドは`park()`しようとしています。その時点で他のスレッド(例えば、ロックを保持しているスレッド)に切り替えると、問題が発生する可能性があります。たとえば、そのスレッドがロックを解除した場合、最初のスレッドによる次の`park()`は、永遠に(潜在的に)スリープ状態になり、時にはウェイクアップ/待機レースと呼ばれる問題が発生します。

>> ASIDE: MORE REASON TO AVOID SPINNING: PRIORITY INVERSION  
>> スピンロックを避ける良い理由の1つはパフォーマンスです。メインテキストで説明したように、スレッドがロックを保持している間中断された場合、スピンロックを使用する他のスレッドは、ロックが利用可能になるのを待つだけの大量のCPU時間を費やします。しかし、いくつかのシステムでスピンロックを避ける別の興味深い理由があることが判明しました。気をつけなければならない問題は、優先度の逆転として知られています。残念なことに、地球[M15]と火星[R97]上で発生する銀河系の間違いです！  
あるシステムに2つのスレッドがあるとします。スレッド2(T2)のスケジューリング優先度は高く、スレッド1(T1)の優先度は低くなります。この例では、実際に両方が実行可能である場合、CPUスケジューラが常にT2を実行すると仮定します。T1は、T2がそうすることができないときにのみ実行されます(例えば、T2がI/Oでブロックされるとき)  
ここで問題です。何らかの理由でT2がブロックされたとします。そのため、T1が走って、スピンロックをつかんでクリティカルセクションに入ります。T2はブロックされなくなり(おそらくI/Oが完了したため)、CPUスケジューラは直ちにスケジューリングします(T1のスケジューリング)。T2は今ではロックを取得しようとします(T1がロックを保持できないため)、回転し続けます。ロックはスピンロックであるため、T2は永遠に回転し、システムはハングアップします。スピンロックの使用を避けるだけで、残念ながら、反転の問題を避けることはできません。3つのスレッドT1、T2、T3を考えてみましょう。T3は最高優先度、T1は最低です。今、T1がロックを取得したと想像してください。T3が起動し、T1よりも優先順位が高いため、すぐに実行されます(T1を先取りします)。T3はT1が保持しているロックを取得しようとしますが、T1はまだそのロックを保持しているため、スタックで待機します。T2が実行を開始すると、T1より優先度が高くなり、T2が実行されます。T2より優先度の高いT3は、T1が待機中のところにスタックされています。強烈なT3は走れず、T2がCPUをコントロールしているのは悲しいことですか？高い優先順位を持つことは、これまでのようなものではありません。  
優先順位の逆転の問題には、さまざまな方法で対応できます。スピンロックによって問題が発生する特定のケースでは、スピンロックの使用を避けることができます(詳細は後述)。より一般的には、優先度の低いスレッドを待っている優先度の高いスレッドは、優先度の高い継承として知られている逆転を実行して克服できるように、下位スレッドの優先度を一時的に引き上げることができます。最後の解決策は最も簡単です。すべてのスレッドが同じ優先順位を持っていることを確認してください。

Solarisでは、`setpark()`という3番目のシステムコールを追加することでこの問題を解決しています。このルーチンを呼び出すことによって、スレッドはパークしようとしていることを示すことができます。その後、パークが実際に呼び出される前に中断され、別のスレッドがパーク解除を呼び出すと、後続のパークはスリープする代わりにすぐに戻ります。`lock()`内のコード変更は非常に小さいです。
```c
1 queue_add(m->q, gettid());
2 setpark(); // new code
3 m->guard = 0;
```  
別の解決方法では、ガードをカーネルに渡すことができます。その場合、カーネルはロックをアトミックに解放し、実行中のスレッドをデキューするための予防措置を講じることができます。

## 28.15 Different OS, Different Support
これまで、スレッドライブラリでより効率的なロックを構築するために、OSが提供できるサポートの1つを見てきました。 他のOSも同様のサポートを提供しています。 詳細は異なります。

たとえば、Linuxでは、Solarisインタフェースに似ていますが、より多くのカーネル内の機能を提供するフューテックスが提供されています。具体的には、各フューテックスには、フューテックス内カーネルキューと同様に、特定の物理メモリロケーションが関連付けられています。発信者はフューテックスコール(後述)を使用して、必要に応じてsleepとwakeを行うことができます。

具体的には、2つの呼が利用可能です。futex wait(address、expected)の呼び出しは、addressの値がexpectedと等しいと仮定して、呼び出しスレッドをスリープ状態にします。同じでない場合、呼び出しはただちに戻ります。ルーチンfutex wake(アドレス)への呼び出しは、キューで待機しているスレッドを起動します。Linux mutexでのこれらの呼び出しの使用法を図28.10に示します。

![](../28/img/fig28_10.PNG)

nptlライブラリ(gnu libcライブラリの一部)[L09]のlowlevellock.hにあるこのコードスニペットは、いくつかの理由で興味深いものです。まず、ロックが保持されているかどうか(整数の上位ビット)とロックのウェイタ数(その他のすべてのビット)を追跡するために、単一の整数を使用します。したがって、ロックが負の場合、ロックは保持されています(上位ビットが設定され、そのビットが整数の符号を決定するため)。

次に、コードスニペットは、一般的なケース、特にロックの競合がない場合に最適化する方法を示しています。ロックを取得して解放するスレッドが1つしかないため、ほとんど作業が行われません(アトミックビットのテストとロックを設定し、ロックを解除するためのアトミックな追加を行う)

この"実世界"ロックの残りの部分をパズルを当てはめて、その動作を理解できるかどうかを確認してください。それをやって、少なくとも誰かが何かこの本について聞いてきたときに答えれるようなLinuxのlockマスターになりましょう。

## 28.16 Two-Phase Locks
最終的な注記：Linuxのアプローチは、古いアプローチを好んで使っており、少なくとも数年にわたってオン・オフされ、1960年代初めにダム・ロックズ(Dahm Locks)[M82]までさかのぼる古いアプローチである2フェーズロックをもっています。2フェーズロックは、特にロックが解放されようとしている場合に、回転が便利であることを認識しています。したがって、最初のフェーズでは、ロックはロックを獲得できることを期待してしばらくの間、回転します。

ただし、最初のスピンフェーズでロックが取得されない場合は、呼び出し元がスリープ状態になる2番目のフェーズに入り、後でロックが解放されたときにのみ起動します。上のLinuxロックはそのようなロックの一種ですが、一回だけ回転します。futexサポートを使用してスリープする前に、これを一般化すると固定された時間ループ内で回転する可能性があります。

2フェーズロックは、2つの良いアイデアを組み合わせることで、より良いアイデアを生むハイブリッドアプローチのもう一つの例です。もちろん、それはハードウェア環境、スレッド数、その他の仕事量の詳細など、多くのことに強く依存します。いつものように、すべての可能なユースケースに適した単一の汎用ロックを作成することは非常に困難です。

## 28.17 Summary
上記のアプローチは、最近のロックの構築方法を示しています。いくつかのハードウェアサポート(より強力な命令の形式)とオペレーティングシステムのサポート(例えば、Solarisの`park()`やun`park()`プリミティブ、futex Linuxの場合)もちろん、詳細は異なります。そのようなロックを実行する正確なコードは、通常、高度に調整されています。詳細を表示したい場合は、SolarisまたはLinuxのコードベースを調べてください。これらは魅力的な読書です[L09、S09]。現代のマルチプロセッサ[D+13]のロック戦略の比較のためのDavidらの優れた作業も参照してください。

## 参考文献

[D91] “Just Win, Baby: Al Davis and His Raiders”  
Glenn Dickey, Harcourt 1991  
There even exists a book about Al Davis and his famous “just win” quote. Or, we suppose, the book is more about Al Davis and the Raiders, and maybe not just the quote. Read the book to find out? But just to be clear: we are not recommending this book, we just needed a citation for the quote.

[D+13] “Everything You Always Wanted to Know about Synchronization  
but Were Afraid to Ask”  
Tudor David, Rachid Guerraoui, Vasileios Trigonakis  
SOSP ’13, Nemacolin Woodlands Resort, Pennsylvania, November 2013
An excellent recent paper comparing many different ways to build locks using hardware primitives. A great read to see how many ideas over the years work on modern hardware.

[D68] “Cooperating sequential processes”  
Edsger W. Dijkstra, 1968  
Available: http://www.cs.utexas.edu/users/EWD/ewd01xx/EWD123.PDF  
One of the early seminal papers in the area. Discusses how Dijkstra posed the original concurrency problem, and Dekker’s solution.

[H93] “MIPS R4000 Microprocessor User’s Manual”  
Joe Heinrich, Prentice-Hall, June 1993  
Available: http://cag.csail.mit.edu/raw/documents/R4400 Uman book Ed2.pdf  

[H91] “Wait-free Synchronization”  
Maurice Herlihy  
ACM Transactions on Programming Languages and Systems (TOPLAS)  
Volume 13, Issue 1, January 1991  
A landmark paper introducing a different approach to building concurrent data structures. However, because of the complexity involved, many of these ideas have been slow to gain acceptance in deployed systems.

[L81] “Observations on the Development of an Operating System”  
Hugh Lauer  
SOSP ’81, Pacific Grove, California, December 1981  
A must-read retrospective about the development of the Pilot OS, an early PC operating system. Fun and full of insights.

[L09] “glibc 2.9 (include Linux pthreads implementation)”  
Available: http://ftp.gnu.org/gnu/glibc/  
In particular, take a look at the nptl subdirectory where you will find most of the pthread support in Linux today.

[M82] “The Architecture of the Burroughs B5000  
20 Years Later and Still Ahead of the Times?”  
Alastair J.W. Mayer, 1982  
www.ajwm.net/amayer/papers/B5000.html  
From the paper: “One particularly useful instruction is the RDLK (read-lock). It is an indivisible operation which reads from and writes into a memory location.” RDLK is thus an early test-and-set primitive, if not the earliest. Some credit here goes to an engineer named Dave Dahm, who apparently invented a number of these things for the Burroughs systems, including a form of spin locks (called “Buzz Locks”) as well as a two-phase lock eponymously called “Dahm Locks.”

[M15] “OSSpinLock Is Unsafe”  
John McCall  
Available: mjtsai.com/blog/2015/12/16/osspinlock-is-unsafe  
A short post about why calling OSSpinLock on a Mac is unsafe when using threads of different priorities – you might end up spinning forever! So be careful, Mac fanatics, even your mighty system sometimes is less than perfect...

[MS91] “Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors”  
John M. Mellor-Crummey and M. L. Scott  
ACM TOCS, Volume 9, Issue 1, February 1991  
An excellent and thorough survey on different locking algorithms. However, no operating systems support is used, just fancy hardware instructions.

[P81] “Myths About the Mutual Exclusion Problem”  
G.L. Peterson  
Information Processing Letters, 12(3), pages 115–116, 1981  
Peterson’s algorithm introduced here.  

[R97] “What Really Happened on Mars?”  
Glenn E. Reeves  
Available: research.microsoft.com/en-us/um/people/mbj/Mars Pathfinder/Authoritative Account.html  
A detailed description of priority inversion on the Mars Pathfinder robot. This low-level concurrent code matters a lot, especially in space!

[S05] “Guide to porting from Solaris to Linux on x86”  
Ajay Sood, April 29, 2005  
Available: http://www.ibm.com/developerworks/linux/library/l-solar/

[S09] “OpenSolaris Thread Library”  
Available: http://src.opensolaris.org/source/xref/onnv/onnv-gate/  
usr/src/lib/libc/port/threads/synch.c  
This is also pretty interesting to look at, though who knows what will happen to it now that Oracle owns Sun. Thanks to Mike Swift for the pointer to the code.

[W09] “Load-Link, Store-Conditional”  
Wikipedia entry on said topic, as of October 22, 2009  
http://en.wikipedia.org/wiki/Load-Link/Store-Conditional  
Can you believe we referenced wikipedia? Pretty lazy, no? But, we found the information there first, and it felt wrong not to cite it. Further, they even listed the instructions for the different architectures: ldl l/stl c and ldq l/stq c (Alpha), lwarx/stwcx (PowerPC), ll/sc (MIPS), and ldrex/strex (ARM version 6 and above). Actually wikipedia is pretty amazing, so don’t be so harsh, OK?

[WG00] “The SPARC Architecture Manual: Version 9”  
David L. Weaver and Tom Germond, September 2000  
SPARC International, San Jose, California  
Available: http://www.sparc.org/standards/SPARCV9.pdf  
Also see: http://developers.sun.com/solaris/articles/atomic sparc/ for some more details on Sparc atomic operations.

\newpage

# 29 Lock-based Concurrent Data Structures
ロックの説明を終わる前に、いくつかの一般的なデータ構造でロックを使用する方法をまず説明します。データ構造体にロックを追加してスレッドで使用できるようにすると、構造体のスレッドが安全になります。もちろん、そのようなロックがどのように追加されるかは、データ構造の正確さとパフォーマンスの両方を決定します。

>> CRUX: HOW TO ADD LOCKS TO DATA STRUCTURES  
>> 特定のデータ構造が与えられたら、それを正しく動作させるために、どのようにロックを追加するべきですか？さらに、データ構造が高性能をもたらすようにロックを追加すると、多くのスレッドが同時に構造にアクセスできるようになります。

もちろん、すべてのデータ構造や並行処理を追加するためのすべてのメソッドをカバーすることは難しいでしょう。何年もの間研究されてきたトピックであり、数千もの研究論文が出版されています。したがって、私たちは、必要とされる思考のタイプを十分に紹介し、あなた自身のさらなる調査のために、いくつかの良い資料源を参照することを望みます。私たちはMoirとShavitの調査が大きな情報源であることを発見しました[MS04]。

## 29.1 Concurrent Counters
最も単純なデータ構造の1つはカウンタです。これは一般的に使用され、簡単なインタフェースを持つ構造です。図29.1に単純な非同期カウンタを定義します。

![](../29/img/fig29_1.PNG)

### Simple But Not Scalable
ご覧のように、非同期カウンタは簡単なデータ構造であり、実装するには少量のコードしか必要としません。私たちは次の課題を抱えています。このコードを安全にするにはどうしたらいいですか？図29.2に、その方法を示します。

![](../29/img/fig29_2.PNG)

この並行カウンタは単純で、正しく動作します。実際、単純で最も基本的な並行データ構造に共通する設計パターンに従います。データ構造を操作するルーチンを呼び出すときに取得され、呼び出しから戻ったときに解放される単一のロックを単純に追加します。この方法では、モニターを使用して構築されたデータ構造[BH73]に似ています。ここでは、ロックを取得し、オブジェクトメソッドから呼び出して戻すときに自動的に解放されます。

この時点で、作業中の同時データ構造があります。問題はパフォーマンスです。データ構造が遅すぎる場合は、単なるロックを追加する以上のことが必要です。そのような最適化は、必要に応じて、この章の残りの部分のトピックです。データ構造が遅すぎない場合は、それ以上何もしないでよいことに注意してください。シンプルなものがうまくいけば何か面白くする必要はありません。

単純なアプローチのパフォーマンスコストを理解するために、各スレッドが単一の共有カウンタを一定回数更新するベンチマークを実行します。スレッドの数を変更します。図29.3は、1〜4つのスレッドをアクティブにしたときの合計時間を示しています。各スレッドはカウンタを100万回更新します。この実験は、Intel 2.7GHz i5 CPU 4台を搭載したiMac上で実行されました。より多くのCPUがアクティブになると、単位時間あたりの総作業量が増えることを期待します。

図の一番上の行(正確に表示されている)から、同期カウンタのパフォーマンスが不十分であることがわかります。1回のスレッドではわずかな時間(約0.03秒)で100万回のカウンタ更新を完了できますが、2つのスレッドでそれぞれ100万回更新すると5秒以上かかることになります。スレッド数が増えれば悪化します。

![](../29/img/fig29_3.PNG)

理想的には、複数のプロセッサで一つのスレッドを一つのプロセッサで完了するのが理想的です。この目的を達成することを完璧なスケーリングといいます。より多くの作業が行われても、それは並行して行われるため、タスクを完了するのにかかる時間は増加しません。

### Scalable Counting
驚いたことに、研究者は、よりスケーラブルなカウンターを何年も構築する方法を研究しました[MS04]。オペレーティングシステムパフォーマンス分析の最近の作業では[B+10]が示されているため、スケーラブルカウンタが重要であるという事実はさらに驚くべきことです。スケーラブルなカウントをせずに、Linux上で実行される仕事量の中には、マルチコアマシンでのスケーラビリティの問題が深刻です。この問題を攻撃するために多くのテクニックが開発されていますが、ここでは特定のアプローチを説明します。最近の研究[B+10]で紹介されたこのアイデアは、お粗末なカウンターとして知られています。

粗雑なカウンタは、多数のローカル物理カウンタ(CPUコアごとに1つ)と単一のグローバルカウンタを一つの論理カウンタとして動作します。具体的には、4つのCPUを持つマシンでは、4つのローカルカウンタと1つのグローバルカウンタがあります。これらのカウンタに加えて、それぞれにロックがあります。(ローカルカウンタごとに1つ、グローバルカウンタに1つ)

粗末なカウントの基本的な考え方は次のとおりです。特定のコアで実行されているスレッドがカウンタをインクリメントしたい場合は、ローカルカウンタをインクリメントします。このローカルカウンタへのアクセスは、対応するローカルロックを介して同期されます。各CPUには独自のローカルカウンタがあるため、CPU間のスレッドは競合することなくローカルカウンタを更新できるため、カウンタの更新はスケーラブルです。

しかし、グローバルカウンタを最新の状態に保つために(スレッドがその値を読みたい場合)、ローカル値はグローバルロックを取得し、ローカルカウンタの値でインクリメントすることによって、グローバルカウンタに定期的に転送されます。ローカルカウンタはゼロにリセットされます。

どのくらいの頻度でこのローカルからグローバルへの転送が発生するかは、閾値(ここではSと呼ぶ)によって決まります。Sが小さいほど、カウンタは上記のスケーラブルではないカウンタのように動作します。Sが大きければ大きいほど、カウンターのスケーラビリティーは向上しますが、実際のカウントからグローバル値は遠く離れたものになる可能性があります。正確な値を得るためにすべてのローカル・ロックとグローバル・ロックを(デッドロックを回避するために指定された順序で)取得できますが、それはスケーラブルではありません。

これを明確にするために、例を見てみましょう(図29.4)。この例では、しきい値Sは5に設定され、ローカルカウンタL1〜L4を更新する4つのCPUのそれぞれにスレッドがあります。トレースにはグローバルカウンタ値(G)も表示され、時間が下がります。各時間ステップで、ローカルカウンタをインクリメントすることができる。ローカル値が閾値Sに達すると、ローカル値がグローバルカウンタに転送され、ローカルカウンタがリセットされます。

![](../29/img/fig29_4.PNG)

図29.3の下段には、閾値Sが1024の粗末なカウンタのパフォーマンスが示されています。4つのプロセッサで400万回のカウンタを更新するのにかかる時間は、1つのプロセッサで100万回更新するのにかかる時間よりもほとんど変わりません。

図29.6に、閾値Sの重要性を示します。4つのスレッドがそれぞれ4つのCPUで100万回カウンタをインクリメントします。Sが低い場合、パフォーマンスは低下します(ただし、グローバルカウントは常に正確です)。Sが高い場合、パフォーマンスは優れていますが、グローバルカウントが遅れます(CPUの数にSを掛けたものが最大でも)。この精度/性能のトレードオフは、厄介なカウンタが可能にするものです。

![](../29/img/fig29_6.PNG)

このようなちょっとしたカウンターの大まかなバージョンが図29.5にあります。それを読んで、あるいはより良い方法で、それがどのように機能するかをよりよく理解するために、いくつかの実験をしてみてください。

![](../29/img/fig29_5.PNG)

## 29.2 Concurrent Linked Lists
次に、より複雑な構造、linked listを調べます。基本的なアプローチからもう一度始めましょう。簡単にするために、このようなリストには明白なルーチンをいくつか省略し、同時に挿入することに重点を置いていきます。私たちは読者にルックアップや削除などについて考えるようにします。図29.7に、この初歩的なデータ構造のコードを示します。

![](../29/img/fig29_7.PNG)

コードでわかるように、コードは入力時に挿入ルーチン内のロックを取得し、終了時に解放します。`malloc()`が失敗する(まれなケース)場合、小さなトリッキーな問題が発生します。この場合、コードは挿入を失敗する前にロックを解除する必要があります。

この種の例外的な制御フローは、かなりエラーを起こしやすいことが示されています。最近のLinuxカーネルパッチの調査では、稀にしか使われていないコードパスでは、バグの巨大な部分(ほぼ40％)が検出されていることが分かりました(実際に、この観察は私たち自身の研究の一部を喚起しました。Linuxファイルシステムであり、より堅牢なシステムとなる[S+11])

私たちは挿入とルックアップルーチンが同時挿入のもとで正しいままであるように書き直すことはできますが、失敗パスでもロックを解除する呼び出しを追加する必要がある場合は避ける必要があるのでしょうか？

この場合の答えは「はい」です。具体的には、ロックと解放が挿入コード内の実際のクリティカルセクションのみを囲み、参照コードで共通の終了パスが使用されるように、コードを少し並べ替えることができます。

前者は実際にはルックアップの一部がロックされる必要がないため動作します。`malloc()`自体がスレッドセーフであると仮定すると、各スレッドは競合状態や他の並行性のバグを心配することなくスレッドに呼び出すことができます。共有リストを更新するときだけ、ロックを保持する必要があります。これらの変更の詳細については、図29.8を参照してください。

![](../29/img/fig29_8.PNG)

ルックアップルーチンは、メインの検索ループから単一のリターンパスにジャンプする単純なコード変換です。再度コードをロックすると、コード内のロック取得/解放ポイントの数が減り、誤ってバグ(コードを返す前にロックを解除することを忘れるなど)がコードに挿入される可能性が低くなります。

### Scaling Linked Lists
私たちは再び基本的な並行リンクリストを持っていますが、もう一度、それは特にうまく調整できない状況にあります。リスト内でより多くの並行処理を可能にするために研究者が検討した手法の1つは、ハンドオーバーハンドロック(a.k.a.lock coupling)[MS04]です。

アイデアはかなり簡単です。リスト全体を単一のロックにする代わりに、リストのノードごとにロックを追加します。リストをトラバースすると、コードは最初に次のノードのロックを取得し、現在のノードのロックを解除します(これがhand-over-handの名前を意味します)

概念的には、ハンドオーバーハンドリンクリストは素晴らしいです。これはリスト操作で高度な並行性を実現します。しかし、実際には、リストトラバーサルの各ノードのロックを取得して解放するオーバーヘッドが法外なので、単純なシングルロックアプローチよりも速くそのような構造を作るのは難しいです。

非常に大きなリストと多数のスレッドがあっても、複数の進行中のトラバーサルを許可することによって可能になる同時実行性は、単純に単一のロックを取得し、操作を実行し、解放するよりも速くなる可能性は低いです。多分、ある種のハイブリッド(あなたが非常に多くのノードごとの新しいロックを取得する場所)は調査する価値があります。

>> TIP: MORE CONCURRENCY ISN’T NECESSARILY FASTER  
>> 設計するスキームがオーバーヘッドを増やす場合(例えば、ロックを一度ではなく頻繁に取得して解放するなど)、それがより同時であるという事実は重要ではないかもしれません。シンプルなスキームは、特にコストのかかるルーチンをめったに使用しないとうまくいく傾向があります。より多くのロックと複雑さを追加することはあなたの没落につながります。すべてのことには、両方の選択肢(単純だが同時性は低く、複雑性は高いがそれ以上のもの)を構築し、それがどのように行われるかを実際に比べる1つの方法があります。結局のところ、あなたはパフォーマンスを欺くことはできません。あなたの考えはより速いか、そうではないかです。

>> TIP: BE WARY OF LOCKS AND CONTROL FLOW  
>> 並行コードおよび他の場所で有用な一般的な設計のヒントは、関数の戻り、終了、または関数の実行を停止させる他の類似のエラー条件につながる制御フローの変化に注意することです。エラーが発生したときにロックを獲得したり、メモリを割り当てたり、他の同様のステートフルな操作を行うことで多くの関数が始まるので、コードはエラーの起こりやすい状態に戻る前にすべての状態を元に戻す必要があります。したがって、このパターンを最小限にするためにコードを構造化することが最善です。

## 29.3 Concurrent Queues
今のところ分かっているように、同時ロック・データ構造を作る標準的な方法があります。大きなロックを追加することです。キューの場合は、それを把握できると仮定して、そのアプローチをスキップします。

代わりに、MichaelとScott [MS98]によって設計されたわずかに並行したキューを見てみましょう。このキューに使用されるデータ構造とコードは、次のページの図29.9にあります。

![](../29/img/fig29_9.PNG)

このコードを注意深く調べると、キューの先頭と末尾の2つのロックがあることに気付くでしょう。これらの2つのロックの目的は、エンキュー操作とデキュー操作の同時実行を可能にすることです。一般的なケースでは、エンキュールーチンはテールロックにのみアクセスし、ヘッドロックのみをデキューします。

MichaelとScottが使うトリックの1つは、(キューの初期化コードで割り当てられた)ダミーノードを追加することです。このダミーは、頭と尾の動作の分離を可能にします。コードを研究するか、それを入力して実行し、それを測定し、それがどのように深く働くかを理解してください。

キューはマルチスレッドアプリケーションで一般的に使用されます。しかし、ここで(ロックだけで)使用されるキューのタイプは、しばしばそのようなプログラムのニーズを完全に満たすものではありません。キューが空であるか過度に満たされている場合にスレッドが待機できるように、より完全に開発されたバウンド・キューは、条件変数に関する次の章で集中的に学んでいきます。それを見てください。

## 29.4 Concurrent Hash Table
シンプルで広く適用可能な並行データ構造であるハッシュテーブルを使用して、議論を終了します。サイズを変更しない単純なハッシュテーブルに焦点を当てます。私たちが読者のための練習として残すリサイズを扱うためにはもう少し作業が必要です(申し訳ありません)。

![](../29/img/fig29_10.PNG)

この並行ハッシュテーブルは簡単で、先に開発した並行リストを使用して作成され、非常にうまく機能します。その優れたパフォーマンスの理由は、構造全体に対して単一のロックを持つ代わりに、ハッシュ・バケットごとのロックを使用するためです(それぞれがリストで表されます)。これにより、多くの同時操作が可能になります。

図29.11には、同時更新中のハッシュテーブルのパフォーマンス(同一のiMacで4つのCPUを使用して、4つのスレッドそれぞれから10,000〜50,000の同時更新が行われています)を示しています。また、比較のために、linked list(単一のロックを持つ)のパフォーマンスを示します。

![](../29/img/fig29_11.PNG)

グラフからわかるように、このシンプルな同時ハッシュテーブルは非常にスケーラブルです。対照的に、linked listはそうではありません。

>> TIP: AVOID PREMATURE OPTIMIZATION (KNUTH’S LAW)  
>> 並行データ構造を構築する場合は、最も基本的なアプローチから始めます。これは、同期アクセスを提供するために単一の大きなロックを追加することです。そうすることで、適切なロックを構築することができます。パフォーマンス上の問題を抱えていることが判明した場合は、それを改良して、必要に応じて高速化するだけです。Knuthが有名に述べたように、「時期尚早最適化はすべての悪の根源です。  
多くのオペレーティングシステムでは、最初にSun OSやLinuxなどのマルチプロセッサに移行するときに、単一のロックを利用していました。後者の場合、このロックには大きなカーネルロック(BKL)という名前があります。長年にわたり、この単純なアプローチは良いものでしたが、マルチCPUシステムが標準となったときに、カーネル内の単一のアクティブなスレッドを一度に許可するだけでは、パフォーマンスのボトルネックになりました。したがって、最終的には、改善された並行性の最適化をこれらのシステムに追加することになりました。Linuxでは、より単純なアプローチが採用されました。一つのロックを多くのものに置き換えます。  
Sunの中ではより根本的な決断が下されました。Solarisと呼ばれるまったく新しいオペレーティングシステムを構築します。このオペレーティングシステムは、より基本的に同時実行性を組み込んでいます。これらの魅力的なシステムの詳細については、LinuxおよびSolarisカーネルの本を読んでください。[BC05、MM00]

## 29.5 Summary
カウンターからリストとキューへの並行データ構造のサンプリングを導入し、最後にユビキタスであり、よく使用されるハッシュテーブルに導入しました。私たちは、その途中でいくつかの重要な教訓を学びました。制御フローの変更に伴うロックの獲得と解放に注意すること。並行性を高めることが必ずしもパフォーマンスを向上させるとは限りません。そのパフォーマンスの問題は、それらが存在する場合にのみ修正する必要があります。この最後の点は、時期尚早な最適化を避けるパフォーマンスに配慮した開発者の中心です。最適化することでアプリケーションの全体的なパフォーマンスが改善されない場合には意味がありません。

もちろん、私たちは高性能構造の表面をみました。詳細については、Moir and Shavitの優れた調査と、他の情報源へのリンク[MS04]を参照してください。特に、他の構造(Bツリーなど)に興味があるかもしれません。これはデータベースクラスが最善の策です。伝統的なロックをまったく使用しないテクニックに興味があるかもしれません。このようなノンブロッキングのデータ構造は、一般的な並行性のバグに関する章で味わうことができるものですが、率直に言えば、この謙虚な本よりも多くの研究を必要とする知識の全領域です。あなたが興味を持っていることは、いつものようにあなた自身でさらに調べてください。

## 参考文献
[B+10] “An Analysis of Linux Scalability to Many Cores”  
Silas Boyd-Wickizer, Austin T. Clements, Yandong Mao, Aleksey Pesterev, M. Frans Kaashoek, Robert Morris, Nickolai Zeldovich  
OSDI ’10, Vancouver, Canada, October 2010  
A great study of how Linux performs on multicore machines, as well as some simple solutions.  

[BH73] “Operating System Principles”  
Per Brinch Hansen, Prentice-Hall, 1973  
Available: http://portal.acm.org/citation.cfm?id=540365  
One of the first books on operating systems; certainly ahead of its time. Introduced monitors as a concurrency primitive.

[BC05] “Understanding the Linux Kernel (Third Edition)”  
Daniel P. Bovet and Marco Cesati  
O’Reilly Media, November 2005  
The classic book on the Linux kernel. You should read it.  

[L+13] “A Study of Linux File System Evolution”  
Lanyue Lu, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, Shan Lu  
FAST ’13, San Jose, CA, February 2013  
Our paper that studies every patch to Linux file systems over nearly a decade. Lots of fun findings in there; read it to see! The work was painful to do though; the poor graduate student, Lanyue Lu, had to look through every single patch by hand in order to understand what they did.

[MS98] “Nonblocking Algorithms and Preemption-safe Locking on Multiprogrammed Sharedmemory Multiprocessors”  
M. Michael and M. Scott  
Journal of Parallel and Distributed Computing, Vol. 51, No. 1, 1998  
Professor Scott and his students have been at the forefront of concurrent algorithms and data structures for many years; check out his web page, numerous papers, or books to find out more.

[MS04] “Concurrent Data Structures”  
Mark Moir and Nir Shavit  
In Handbook of Data Structures and Applications  
(Editors D. Metha and S.Sahni)  
Chapman and Hall/CRC Press, 2004  
Available: www.cs.tau.ac.il/˜shanir/concurrent-data-structures.pdf  
A short but relatively comprehensive reference on concurrent data structures. Though it is missing some of the latest works in the area (due to its age), it remains an incredibly useful reference.

[MM00] “Solaris Internals: Core Kernel Architecture”  
Jim Mauro and Richard McDougall  
Prentice Hall, October 2000  
The Solaris book. You should also read this, if you want to learn in great detail about something other than Linux.

[S+11] “Making the Common Case the Only Case with Anticipatory Memory Allocation”  
Swaminathan Sundararaman, Yupu Zhang, Sriram Subramanian, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau  
FAST ’11, San Jose, CA, February 2011  
Our work on removing possibly-failing calls to malloc from kernel code paths. The idea is to allocate all potentially needed memory before doing any of the work, thus avoiding failure deep down in the storage stack.

\newpage

# 30 Condition Variables
これまでのところ、ロックの概念を開発し、ハードウェアとOSの適切な組み合わせで適切に構築する方法を見てきました。残念ながら、ロックは、並行プログラムを構築するために必要な唯一の方法ではありません。

特に、スレッドは、実行を続行する前に条件が真であるかどうかをチェックしたい場合があります。たとえば、親スレッドは、子スレッドが完了する前に完了しているかどうかを確認したい場合があります(これはよく`join()`と呼ばれます)。どのようにそのような待って実装する必要がありますか？図30.1を見てみましょう。

![](../30/img/fig30_1.PNG)

ここで見たいのは、次の出力です。
```
parent: begin
child
parent: end
```  
図30.2に示すように、共有変数を使用してみることもできます。このソリューションは一般的には機能しますが、親が回転してCPU時間を無駄にするため、非常に非効率的です。ここで私たちが望むのは、私たちが待っている状態(例えば、子供が実行されている)が成立するまで、親を寝かせる何らかの方法です。

![](../30/img/fig30_2.PNG)

>> THE CRUX: HOW TO WAIT FOR A CONDITION  
>> マルチスレッドプログラムでは、スレッドが先に進む前に、ある条件が真となるのを待つことがしばしば役に立ちます。条件が真になるまで回転するという単純なアプローチは、非常に効率が悪く、CPUサイクルを浪費し、場合によっては正しくない可能性があります。したがって、スレッドはどのように条件を待つべきですか？

## 30.1 Definition and Routines
条件が真となるのを待つために、スレッドは条件変数として知られているものを利用することができます。条件変数は、ある実行状態(すなわち、ある条件)が(条件を待つことによって)望ましくないときに、スレッドが自分自身を置くことができる明示的なキューです。その状態を変更すると、他のスレッドは、それらのwait中のスレッドのうちの1つ(またはそれ以上)をウェイクさせることができ、したがってそれらを継続して実行することができます(条件を通知することによって)。ダイクストラの「プライベートセマフォー」[D68]の使用に基づいています。類似のアイデアは後でホアレ(Hoare)によってモニター上での作業で「条件変数(condition variable)」と命名されました[H74]。

そのような条件変数を宣言するには、次のような文を書きます。pthread cond t c;これは条件変数としてcを宣言します(注：適切な初期化も必要です)条件変数には、`wait()`と`signal()`という2つの操作が関連付けられています。`wait()`呼び出しはスレッドがスリープ状態にしたいときに実行されます。`signal()`コールは、スレッドがプログラム内で何かを変更したときに実行され、この状態でwaitしているスリープ中のスレッドをスリープ解除するために必要です。具体的には、POSIX呼び出しは次のようになります。
```c
pthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m);
pthread_cond_signal(pthread_cond_t *c);
```  
わかりやすくするために、これらを`wait()`と`signal()`と呼ぶことがよくあります。`wait()`呼び出しについて気付くかもしれないことの1つは、mutexもパラメータとして渡されることです。`wait()`が呼び出されたときにこのmutexがロックされているとみなします。`wait()`の役割は、ロックを解放し、呼び出しスレッドをスリープ状態(アトミック)にすることです。スレッドが起動したとき(他のスレッドがそれを通知した後)、呼び出し元に戻る前にロックを再取得する必要があります。この複雑さは、スレッドがスリープ状態になるときに特定の競合状態が発生しないようにするということに由来します。これをよりよく理解するために、結合問題(図30.3)の解を見てみましょう。

![](../30/img/fig30_3.PNG)

考慮すべき2つのケースがあります。最初は、親スレッドは子スレッドを作成しますが、自分自身を実行し続けます(単一のプロセッサしか持っていないと仮定しています)。すぐに`thr_join()`を呼び出して子スレッドが完了するのを待ちます。この場合、ロックを取得し、子が完了しているかどうかをチェックし、子が完了していないかどうかを確認し、`wait()`を呼び出すことでロックを解除します。子プロセスは最終的に実行され、メッセージ"child"を出力し、`thr_exit()`を呼び出して親スレッドを起動します。このコードはロックを取得し、状態変数を設定し、親に信号を送り、それを目覚めさせるだけです。最後に、親プロセスが実行され(ロックが保持された状態で`wait()`から戻る)ロックが解除され、最後のメッセージ"parent：end"が出力されます。

2番目のケースでは、子は作成直後に実行され、1に設定されたセットはスリープしているスレッドをスリープ解除するためのsignalを呼び出します(ただし、何もないので返されます)。親が実行され、`thr_join()`が呼び出され、doneが1であるとみなされるため、waitせずに戻ります。

最後の注意点：条件を待つかどうかを決定する際には、親がif文の代わりにwhileループを使用することがわかります。これはプログラムのロジックごとに厳密には必要ではないようですが、以下に示すように、常に良いアイデアです。

`exit()`と`thr_join()`の各コードの重要性を理解するために、いくつかの代替実装を試してみましょう。まず、状態変数が必要かどうか疑問に思うかもしれません。コードが下の例のように見える場合はどうなりますか？これは効果がありますか？

![](../30/img/fig30_3_1.PNG)

残念ながら、このアプローチではうまくいきません。子がすぐに実行され、すぐに`exit()`を呼び出した場合を想像してください。この場合、子供はsignalをだしますが、この状態では眠っているスレッドはありません。親が実行されると、単にwaitを呼び出してスタックします。スレッドはそれを起こすことはありません。この例から、状態変数の重要性を理解する必要があります。スレッドが知りたい値を記録します。sleep、wake、そしてすべてをロックすることは、その周りに構築されます。

ここに別の貧弱な実装があります。この例では、signalとwaitのためにロックを保持する必要がないと想定しています。どのような問題がここで起こるでしょうか？それについて考えてみましょう！

![](../30/img/fig30_3_2.PNG)

ここでの問題は微妙な競争条件です。具体的には、親が`thr_join()`を呼び出してdoneの値をチェックすると、それが0であることがわかり、スリープ状態になります。しかし、それが寝るのを待つ直前に、親は中断され、子供は走ります。子はステート変数doneを1に変更してシグナルを送りますが、スレッドは待機していないため、スレッドは起動しません。親が再び動くとき、永遠に眠ります。

>> TIP: ALWAYS HOLD THE LOCK WHILE SIGNALING  
>> すべてのケースで厳密には必要ではありませんが、条件変数を使用するときにsignalを送信している間は、ロックを保持するのが最も簡単で最善の方法です。上記の例では、ロックを正しい状態に保つ必要がある場合を示しています。しかし、そうでない可能性が高い他のケースがいくつかありますが、避けるべきことが多くあります。したがって、簡単にするために、信号を呼び出すときにロックを保持してください。

このヒントの逆、つまりwaitを呼び出すときにロックを保持するのは、ヒントだけでなく、waitのセマンティクスによって義務づけられています。常にwait(a)はロックを保持しているとみなし、(b)呼び出し元をスリープ状態にするときにロックを解除し、(c)復帰する直前にロックを再獲得することを特徴とする。したがって、このtipの一般化は正しいです。signalまたはwaitを呼び出すときにロックを保持すると、常に良い形になります。

うまくいけば、この単純な結合の例から、条件変数を適切に使用する基本的な要件の一部を見ることができます。あなたが理解していることを確かめるために、より複雑な例、つまりプロデューサ/コンシューマまたは有限バッファ問題(有限なバッファの問題)を調べます。

## 30.2 The Producer/Consumer (Bounded Buffer) Problem
この章で直面する次の同期の問題は、プロデューサ/コンシューマの問題、またはダイクストラ[D72]によって最初に提起された有限のあるバッファの問題(有界バッファ)として知られています。実際、ダイクストラと彼の同僚に一般化されたセマフォ(ロック変数または条件変数のいずれかとして使用することができる)を発明するのは、まさにプロデューサ/コンシューマの問題でした[D01]。セマフォについて後で詳しく説明します。

1つ以上のプロデューサスレッドと1つ以上のコンシューマスレッドを想像してください。プロデューサはデータ項目を生成し、バッファに格納します。コンシューマはバッファからアイテムを取り出し、何らかの方法でそれらを消費します。この配置は、多くの実際のシステムで発生します。例えば、マルチスレッド・ウェブ・サーバでは、プロデューサは、HTTP要求を作業キュー(すなわち、有限バッファ)に入れます。コンシューマスレッドはこのキューから要求を取り出して処理します。

有界バッファは、あるプログラムの出力を別のプログラムにパイプするときにも使用されます(例：grep foo file.txt | wc -l)この例では、2つのプロセスを同時に実行します。grepは、file.txtの文字列をfooという文字列で標準出力とみなして書き込みます。UNIXシェルは、パイプシステムコールによって作成されたUNIXパイプと呼ばれるものに出力をリダイレクトします。このパイプのもう一方の端はプロセスwcの標準入力に接続されています。これは単純に入力ストリームの行数を数え、その結果を出力します。したがって、grepプロセスはプロデューサです。wcプロセスはコンシューマです。それらの間にはカーネル内の有界バッファがあります。この例では、幸せなユーザーだけです。

有界バッファは共有リソースであるため、競合状態が発生しないように、同期アクセスを要求する必要があります。この問題をよりよく理解するには、実際のコードを調べてみましょう。最初に必要なのは、共有バッファで、プロデューサがデータを入れ、コンシューマがデータを取り込みます。単純に単一の整数を使用してみましょう(代わりに、このスロットにデータ構造体へのポインタを置くことを想像してください)そして、2つの内部ルーチンが値を共有バッファに入れ、バッファから値を取得します。詳細は図30.4を参照してください。

![](../30/img/fig30_4.PNG)

かなりシンプルでしょう？`put()`ルーチンは、バッファが空であるとみなし(アサーションでこれをチェックする)、共有バッファに値を格納し、countを1に設定してバッファをフルにします。`get()`ルーチンは反対にバッファを空にして値を返します(つまり、カウントを0に設定します)この共有バッファには1つのエントリしかないことを心配しないでください。後で、複数のエントリを保持できるキューに一般化します。

バッファにアクセスしてデータを入れたり、そこからデータを取り出したりするために、いつバッファにアクセスするのが良いかを知るルーチンを書く必要があります。これは、カウントがゼロのとき(すなわち、バッファが空のとき)のみデータをバッファに入れ、カウントが1のとき(すなわち、バッファがいっぱいのとき)にのみデータをバッファから取得するという条件が明らかでなければならない。そうでなければ、プロデューサがデータをフルバッファに入れるか、コンシューマが空のデータからデータを取得するような同期コードを書くといった、何か間違ったことが起こります(このコードではアサーションが発生します)。

この作業は、2種類のスレッドによって行われます。1つはプロデューサスレッドと呼ばれ、もう1つはコンシューマスレッドと呼ばれます。図30.5は、共有バッファループ回数に整数を代入するプロデューサのコードと、共有バッファから引き出されたデータ項目を出力するたびにその共有バッファからデータを取得する(永久に)コンシューマを示しています。

![](../30/img/fig30_5.PNG)

### A Broken Solution
今私たちが単一のプロデューサと単一のコンシューマを持っていると想像してください。明らかに`put()`と`get()`ルーチンは、`put()`がバッファを更新し、`get()`がそこから読み込むので、それらの中にクリティカルセクションを持っています。しかし、コードの周りにロックをかけることはできません。解決策としてもっと別の仕組みが必要です。驚くことではないが、何らかの条件変数があります。この(壊れた)最初の試行(図30.6)では、単一の条件変数condと関連するロックミューテックスがあります。

![](../30/img/fig30_6.PNG)

プロデューサーとコンシューマの間のシグナルロジックを調べてみましょう。プロデューサはバッファを埋めることを望むとき、それが空であるのを待ちます(p1-p3)コンシューマはまったく同じロジックを持っていますが、満腹度(c1〜c3)の異なる状態を待ちます。単一のプロデューサと単一のコンシューマだけでは、図30.6のコードが機能します。しかしながら、これらのスレッド(例えば、2つのコンシューマ)のうちの2つ以上を有する場合、解決策は2つの重大な問題があります。それらは一体何でしょうか？

...(考えるためにここで休憩)...

最初の問題を理解しましょう。待つ前にifステートメントと関係があります。2人のコンシューマ(Tc1とTc2)と1人のプロデューサー(Tp)が存在すると仮定します。まず、コンシューマ(Tc1)が走ります。ロック(c1)を取得し、使用準備ができているバッファがあるかどうかを確認し(c2)、存在しないことを確認し、ロックを解除する(c3)のを待ちます。

その後、プロデューサー(Tp)が実行されます。ロック(p1)を取得し、すべてのバッファが満杯(p2)であるかどうかを調べ、そうでないと判断した場合は、先に進みバッファ(p4)を埋めます。次に、プロデューサは、バッファが満たされたことを通知します(p5)。クリティカルな部分は、最初のコンシューマ(Tc1)が条件変数をスリープ状態からレディキューに移動します。Tc1は今実行することができます(ただし、まだ実行されていません)。プロデューサは、バッファが一杯になったことが実現するまで続きます。バッファがいっぱいになると、その時点でスリープします(p6、p1-p3)。

問題が発生した場所は次のとおりです。別のコンシューマ(Tc2)がバッファ内に存在する1つの既存の値(c1、c2、c4、c5、c6、バッファがいっぱいなのでc3の待機をスキップ)今度はTc1が動作すると仮定します。待機から戻ってくる直前に、ロックを再取得してからリターンします。それから、`get()`(c4)を呼び出しますが、消費するバッファはありません！アサーションがトリガされ、コードは必要に応じて機能しませんでした。見てわかるように、Tc2が消費してきたため、Tc1が消費しようとすることを何らかの形で妨げていたはずです。図30.7は、各スレッドが実行するアクションと、そのスケジューラの状態(Ready、Running、Sleeping)を経時的に示しています。

![](../30/img/fig30_7.PNG)

簡単な理由で問題が発生します。プロデューサがTc1を覚ました後、Tc1が実行される前に、バインドされたバッファの状態が変更されました(Tc2のおかげで)。スレッドへのシグナリングは、それらを目覚めさせるだけです。したがって、状態
(この場合、バッファに値が設定されている)が変更されましたが、起きたスレッドが実行されても状態は希望どおりに保たれるという保証はありません。このような方法で条件変数を構築した最初の研究の後に、信号が意味することのこの解釈は、しばしばメサの意味論と呼ばれます(LR80)。Hoareセマンティクスと呼ばれるコントラストは構築するのが難しいですが、目覚めたスレッドが目覚めた直後に実行されるという強力な保証を提供します[H74]。実際に構築されたすべてのシステムは、メサのセマンティクスを採用しています。

### Better, But Still Broken: While, Not If
幸いにも、この修正は簡単です(図30.8)：ifをwhileに変更してください。なぜこれが働くか考えてみてください。今度はコンシューマTc1が起動し、(ロックを保持して)共有変数(c2)の状態を直ちに再チェックします。その時点でバッファが空の場合、コンシューマは単にスリープ状態に戻ります(c3)。当然ながら、プロデューサ(p2)もifからwhileに変更されます。

![](../30/img/fig30_8.PNG)

メサのセマンティクスのおかげで、条件変数で覚えておく簡単なルールは、常にwhileループを使用することです。場合によっては、条件を再確認する必要はありませんが、いつでも行うことができます。(そうすることが確実です)しかし、このコードにはまだバグがあり、上記の2つの問題の2番目のものです。わかりますか？条件変数が1つしかないという事実と関係があります。先読みをする前に、問題が何であるかを把握してみてください。

...(もう一度、自分で考えてください。また、少し目を閉じて考えてみてください)...

あなたがそれを正しく理解したことを確認しましょう。この問題は、2つのコンシューマが最初に起動し(Tc1とTc2)、両方ともスリープ状態になる(c3)場合に発生します。次に、プロデューサが実行され、バッファに値が格納され、コンシューマの1つを起動させます(Tc1など)。次に、プロデューサはループバック(途中でロックの解除と再取得)を行い、バッファにさらにデータを入れようとします。バッファがいっぱいであるため、代わりにプロデューサは条件を待機します(したがってスリープします)。これで、1つのコンシューマは実行準備が整っており(Tc1)、2つのスレッドが1つの条件(Tc2とTp)でスリープしています。私たちは問題を起こそうとしています。物事はエキサイティングになっています！

次に、コンシューマTc1は、`wait()`(c3)から復帰してウェイクし、条件(c2)を再確認し、バッファが満杯であることを見つけると、値(c4)を消費します。このコンシューマは、クリティカル条件前で待機中の唯一のスレッドを起こし、条件(c5)にシグナルを出します。しかし、どのスレッドが目を覚ますべきですか？

コンシューマはバッファを空にしたので、明らかにプロデューサーを目覚めさせるべきです。しかし、それがコンシューマTc2を目覚めさせた場合(確かに待ち行列がどのように管理されるかによっては可能です)、問題があります。具体的には、コンシューマTc2は起こしてバッファを空にし(c2)、スリープ状態に戻ります(c3)。バッファに入れる値を持つプロデューサTpはスリープ状態になります。他のコンシューマスレッドTc1もスリープ状態に戻ります。3つのスレッドはすべてスリープ状態になっています。この悲しいステップについては、図30.9を参照してください。シグナリングは明らかに必要ですが、より直接的でなければなりません。コンシューマは、他のコンシューマやプロデューサだけを目覚めさせてはならず、またその逆もあってはいけません。

### The Single Buffer Producer/Consumer Solution
システムの状態が変化したときにどのタイプのスレッドが起きるべきかを適切に伝えるために、1つではなく2つの条件変数を使用します。 結果のコードを図30.10に示します。

![](../30/img/fig30_10.PNG)

上のコードでは、プロデューサスレッドは空の状態で待機し、シグナルがいっぱいになります。逆に、コンシューマスレッドはいっぱいになるのを待ってシグナルを空にします。こうすることで、上記の第2の問題は、コンシューマが誤ってコンシューマを目覚めさせることはなく、プロデューサーは誤ってプロデューサーを目覚めさせることはありません。

### The Correct Producer/Consumer Solution
私たちは現在、完全に一般的なものではありませんが、実際のプロデューサ/コンシューマのソリューションを持っています。最後に行う変更は、より並行性と効率性を実現することです。具体的には、バッファースロットを追加して、スリープする前に複数の値を生成し、同様にスリープする前に複数の値を消費することができます。単一のプロデューサとコンシューマだけでは、このアプローチはコンテクストスイッチを減らすので効率的です。複数のプロデューサまたはコンシューマ(またはその両方)を使用すると、同時の生成または消費が可能になり、並行性が向上します。幸いにも、それは現在のソリューションの小さな変更です。

この正しい解決策の最初の変更は、バッファ構造自体とそれに対応する`put()`および`get()`(図30.11)内にあります。また、sleepの有無を判断するために、プロデューサとコンシューマが確認する条件を少し変更します。図30.12は、正しい待機およびシグナリングロジックを示しています。プロデューサは、すべてのバッファが現在いっぱいになるとスリープします(p2)。同様に、コンシューマは、すべてのバッファが現在空である場合にのみスリープする(c2)。それで、私たちはプロデューサ/コンシューマの問題を解決します。

![](../30/img/fig30_11.PNG)

![](../30/img/fig30_12.PNG)

>> TIP: USE WHILE (NOT IF) FOR CONDITIONS  
>> マルチスレッドプログラムで条件をチェックするときは、whileループを使用することは常に正しいです。 if文を使用するのは、シグナリングのセマンティクスに依存します。したがって、常にwhileを使用すると、コードが期待通りに動作します。  
>>条件付きチェックのwhileループを使用すると、疑似wakeupsが発生するケースも処理されます。いくつかのスレッドパッケージでは、実装の詳細により、ただ1つのsignalが発生したにもかかわらず、2つのスレッドが起動する可能性があります[L11]。疑似wakeupsは、スレッドが待機している状態を再確認するもう1つの理由です。

## 30.3 Covering Conditions
次に、条件変数の使用方法のもう1つの例を見てみましょう。このコードの研究は、先に説明したMesaセマンティクスを最初に実装した同じグループのPilot [LR80]のLampsonとRedellの論文(彼らが使用した言語はMesaなので、その名前)から引き出されています。彼らが遭遇した問題は、シンプルな例(この場合は単純なマルチスレッドメモリ割り当てライブラリ)で最もよく分かります。図30.13に問題を示すコードスニペットを示します。

![](../30/img/fig30_13.PNG)

コードで見てきたように、スレッドがメモリ割り当てコードを呼び出すと、より多くのメモリが解放されるのを待たなければならないかもしれません。逆に、スレッドがメモリを解放すると、より多くのメモリが解放されたことを通知します。しかし、上のコードでは問題があります。待機中のスレッド(複数のスレッドが存在する可能性があります)を起動する必要がありますか？

以下のシナリオを考えてみましょう。0バイトの空きがあると仮定します。スレッドTaはallocate(100)を呼び出し、スレッドTbはallocate(10)を呼び出してメモリを少なくするよう要求します。こうしてTaとTbの両方がこの状態を待って眠りにつきます。これらの要求のいずれかを満たすのに十分な空きバイトがありません。

その時点で、第3のスレッドTcがfree(50)を呼び出すと仮定します。残念ながら、待機中のスレッドをスリープ解除するためのシグナルを呼び出すと、解放されるのを10バイトだけ待っている正しい待機中のスレッドTbをスリープ解除できないことがあります。十分な記憶がまだ空いていないので、Taは待っておくべきである。したがって、図のコードは動作しません。他のスレッドを起動するスレッドは、起動するスレッド(またはスレッド)を認識しません。

LampsonとRedellによって提案された解決方法は簡単です。上のコードの`pthread_cond_signal()`呼び出しを、`pthread_cond_broadcast()`の呼び出しで置き換えてください。そうすることで、目覚めすべきスレッドがあることを保証します。もちろん、(まだ)目を覚ますべきではない他の多くの待機スレッドを不必要に起動させる可能性があるので、欠点はパフォーマンスに悪影響を与える可能性があります。これらのスレッドは単に起床し、条件を再確認してすぐにスリープ状態に戻ります。

LampsonとRedellは、スレッドが目を覚ます必要があるすべてのケース(控えめに)をカバーするため、このような条件をカバー条件と呼びます。これまで述べてきたように、コストはあまりにも多くのスレッドが起きる可能性があります。鋭い読者は、このアプローチを早期に使用することができたことに気づいているかもしれません(ただ一つの条件変数でプロデューサ/コンシューマの問題を参照してください)。しかし、その場合、より良い解決策が利用でき、それを使用しました。一般に、あなたのプログラムがあなたのシグナルをブロードキャストに変更したときにのみ動作することがわかった場合(ただし、必ずしもそうする必要はないです)、おそらくバグがあれば修正しましょう。しかし、上記のメモリアロケータのような場合、ブロードキャストは利用可能な最も簡単な解決策かもしれません。

## 30.4 Summary
ロック以外の重要な同期プリミティブ、条件変数の導入を見てきました。いくつかのプログラム状態が望ましくないときにスレッドをスリープさせることにより、CVsは、有名な(そして依然として重要な)プロデューサ/コンシューマの問題やカバー条件を含むいくつかの重要な同期問題をきれいに解決することができます。「彼はビッグブラザーを愛しました」[O49]など、より劇的な結論文がここに来るでしょう。

## 参考文献
[D68] “Cooperating sequential processes”  
Edsger W. Dijkstra, 1968  
Available: http://www.cs.utexas.edu/users/EWD/ewd01xx/EWD123.PDF  
Another classic from Dijkstra; reading his early works on concurrency will teach you much of what you need to know.

[D72] “Information Streams Sharing a Finite Buffer”  
E.W. Dijkstra  
Information Processing Letters 1: 179180, 1972  
Available: http://www.cs.utexas.edu/users/EWD/ewd03xx/EWD329.PDF  
The famous paper that introduced the producer/consumer problem.

[D01] “My recollections of operating system design”  
E.W. Dijkstra  
April, 2001  
Available: http://www.cs.utexas.edu/users/EWD/ewd13xx/EWD1303.PDF  
A fascinating read for those of you interested in how the pioneers of our field came up with some very basic and fundamental concepts, including ideas like “interrupts” and even “a stack”!

[H74] “Monitors: An Operating System Structuring Concept”  
C.A.R. Hoare  
Communications of the ACM, 17:10, pages 549–557, October 1974  
Hoare did a fair amount of theoretical work in concurrency. However, he is still probably most known for his work on Quicksort, the coolest sorting algorithm in the world, at least according to these authors.

[L11] “Pthread cond signal Man Page”  
Available: http://linux.die.net/man/3/pthread cond signal  
March, 2011  
The Linux man page shows a nice simple example of why a thread might get a spurious wakeup, due to race conditions within the signal/wakeup code.

[LR80] “Experience with Processes and Monitors in Mesa”  
B.W. Lampson, D.R. Redell  
Communications of the ACM. 23:2, pages 105-117, February 1980  
A terrific paper about how to actually implement signaling and condition variables in a real system, leading to the term “Mesa” semantics for what it means to be woken up; the older semantics, developed by Tony Hoare [H74], then became known as “Hoare” semantics, which is hard to say out loud in class with a straight face.

[O49] “1984”  
George Orwell, 1949, Secker and Warburg  
A little heavy-handed, but of course a must read. That said, we kind of gave away the ending by quoting the last sentence. Sorry! And if the government is reading this, let us just say that we think that the government is “double plus good”. Hear that, our pals at the NSA?

\newpage

# 31 Semaphores
今のところわかっているように、関連性のある興味深い並行性の問題を解決するためには、ロックと条件変数の両方が必要です。この数年前に実現した最初の人のひとりは、Edsger Dijkstra(正確な歴史[GR92]を知ることは難しいが)である。グラフ理論[D59]の有名な「最短経路」アルゴリズムで知られている。「Goto Statements Considered Harmful」[D68a](これはすばらしいタイトルです)という名前の構造化プログラミングについて説明し、ここではセマフォー[D68b、D72]と呼ばれる同期プリミティブの導入を検討します。実際、Dijkstraらは、セマフォを同期に関連するすべてのものの単一のプリミティブとして考案しました。表示されるように、セマフォはロックと条件変数の両方として使用できます。

>> THE CRUX: HOW TO USE SEMAPHORES  
>> ロックと条件変数の代わりにセマフォを使用するにはどうすればよいですか？セマフォの定義は何ですか？バイナリセマフォとは何ですか？ロックと条件変数からセマフォーを構築するのは簡単ですか？セマフォからロックと条件変数を構築するにはどうすればよいでしょう？

## 31.1 Semaphores: A Definition
セマフォは、2つのルーチンで操作できる整数値を持つオブジェクトです。POSIX標準では、これらのルーチンは`sem_wait()`と`sem_post()`です。セマフォの初期値は、セマフォと相互作用する他のルーチンを呼び出す前に、その動作を決定するため、図31.1のコードと同じように、最初にセマフォをある値に初期化する必要があります。

![](../31/img/fig31_1.PNG)

この図では、セマフォsを宣言し、3番目の引数として1を渡して値1に初期化します。`sem_init()`の2番目の引数は、すべての例で0に設定されています。セマフォが同じプロセス内のスレッド間で共有されていることを示します。セマフォーの他の使用法(つまり、異なるプロセス間でアクセスを同期させるためにそれらをどのように使用することができるか)の詳細については、第2引数の値が異なる必要があります。

セマフォが初期化された後、`sem_wait()`または`sem_post()`の2つの関数のいずれかを呼び出すことができます。これらの2つの機能の動作を図31.2に示します。

![](../31/img/fig31_2.PNG)

今のところ、これらのルーチンの実装には関心がありません。`sem_wait()`と`sem_post()`を呼び出す複数のスレッドでは、これらのクリティカルセクションを管理するために必要があることは明らかです。ここでは、これらのプリミティブの使用方法に焦点を当てます。後に、それらがどのように構築されるかを議論するかもしれません。

ここではインターフェイスのいくつかの顕著な側面について議論する必要があります。`sem_wait()`は`sem_wait()`を呼び出したときにセマフォの値が1以上だったためすぐに返されるか、呼び出し側が後続のポストを待って実行を中断させます。もちろん、複数の呼び出しスレッドが`sem_wait()`を呼び出すことがあります。したがって、すべてが呼び出されるのを待ってキューに入れられます。

第2に、`sem_post()`は`sem_wait()`のように特定の条件が成立するのを待たないことがわかります。むしろ、単にセマフォの値をインクリメントし、次に目覚めようとしているスレッドがあれば、それらの1つを起動させます。

第3に、セマフォの値が負の場合、待機スレッドの数に等しい[D68b]。この値は一般的にセマフォのユーザには見られませんが、知る価値があり、セマフォがどのように機能するかを覚えておくのに役立ちます。

セマフォー内で可能な競合状態を心配しないでください。それらのアクションがアトミックに実行されると仮定します。私たちはこれを行うためにロックと条件変数を使用します。

## 31.2 Binary Semaphores (Locks)
セマフォを使用する準備ができました。私たちの最初の使い方は、すでによく知られているものです。それは、セマフォをロックとして使用します。コードスニペットについては、図31.3を参照してください。重要なセクションを`sem_wait()`/`sem_post()`のペアで囲むだけであることがわかります。ただし、この作業を行う上で重要なのは、セマフォmの初期値です(図のXに初期化されています)。Xは何をすべきですか？

![](../31/img/fig31_3.PNG)

...(先に進む前にそれについて考えてみてください)...

上記の`sem_wait()`と`sem_post()`ルーチンの定義を振り返ってみると、初期値は1であるはずです。

これを明確にするために、2つのスレッドを持つシナリオを想像してみましょう。最初のスレッド(スレッド0)は`sem_wait()`を呼び出します。セマフォの値を最初にデクリメントし、0に変更します。その後、値が0以上でない場合にのみ待機します。値が0であるため、`sem_wait()`は単に戻り値を返しますし継続します。スレッド0はクリティカルセクションに自由に入ります。スレッド0がクリティカルセクション内にある間に他のスレッドがロックを取得しようとしない場合、`sem_post()`を呼び出すと、セマフォの値を1に復元します(存在しないため待機スレッドを起動しません)。図31.4に、このシナリオのトレースを示します。

![](../31/img/fig31_4.PNG)

スレッド0がロックを保持し(つまり`sem_wait()`はまだ`sem_post()`と呼ばれていない)、別のスレッド(スレッド1)`sem_wait()`を呼び出してクリティカルセクションに入ることを試みているとき、スレッド1はセマフォの値を減らして-1にし、待機します(プロセッサをスリープさせて放棄する)スレッド0が再び実行されると、最終的に`sem_post()`が呼び出され、セマフォの値がゼロに戻されて待機スレッド(スレッド1)が起動され、ロックが獲得されます。スレッド1が終了すると、セマフォの値が再びインクリメントされ、再び1に戻されます。

図31.5に、この例のトレースを示します。スレッド動作に加えて、図は各スレッドのスケジューラ状態、すなわち実行中、実行可能(実行可能であるが実行中ではない)、およびスリープを示しています。特に、すでに保持されているロックを取得しようとすると、スレッド1はスリープ状態になります。スレッド0が再び実行された場合にのみ、スレッド1を起動し、潜在的に再び実行することができます。

![](../31/img/fig31_5.PNG)

独自の例を使用して作業したい場合は、複数のスレッドがロックを待って待機するシナリオを試してみてください。このようなトレース中にセマフォの値はどのようになりますか？

このようなとき、セマフォをロックとして使用することができます。ロックは2つの状態(保持され、保持されていない)しか持たないので、ロックとして使用されるセマフォをバイナリセマフォと呼ぶことがあります。注意として、このバイナリ形式でのみセマフォを使用している場合は、ここに示す汎用セマフォより簡単な方法で実装できます。

## 31.3 Semaphores For Ordering
セマフォは、並行プログラムでイベントを順序付けるのにも役立ちます。例えば、スレッドは、リストが空でなくなるのを待つことを望むかもしれないので、そこから要素を削除することができます。この使用パターンでは、あるスレッドが何か起こるのを待っていて、別のスレッドが何か起こっていることを確認してから、それが起こったことをシグナルで待機スレッドを呼び起こします。このようにして、セマフォを順序付けプリミティブとして使用しています(以前の条件変数の使用と同様)。

簡単な例は次のとおりです。スレッドが別のスレッドを作成し、実行を完了するのを待っているとします(図31.6)。このプログラムが実行されると、次の情報が表示されます。

![](../31/img/fig31_6.PNG)  
```
parent: begin
child
parent: end
```
問題は、この効果を達成するためにセマフォを使用する方法です。それが判明したとき、答えは比較的理解しやすいです。コードでわかるように、親は`sem_wait()`と子`sem_post()`を呼び出して、実行を終了した子の状態が真になるのを待つだけです。しかし、これにより、このセマフォの初期値はどうなるべきかという疑問が生じます。

(やはり先読みではなく、ここで考えてみてください)

答えはもちろん、セマフォの値を0に設定する必要があります。考慮すべき2つのケースがあります。最初に、親が子を作成するが、子はまだ実行されていない(すなわち、準備完了キューに入っているが実行されていない)と仮定します。この場合(図31.7)、親は`sem_post()`を呼び出す前に`sem_wait()`を呼び出します。私たちは親が子供が走るのを待つことを望みます。これが起こる唯一の方法は、セマフォの値が0より大きくない場合です。従って、0が初期値になります。親が実行され、セマフォを減らして-1にしてから、スリープします。子が最終的に実行されると、`sem_post()`を呼び出し、セマフォの値を0にインクリメントし、親をスリープ解除して`sem_wait()`から戻り、プログラムを終了します。

![](../31/img/fig31_7.PNG)

2番目のケース(図31.8、ページ364)は、親が`sem_wait()`を呼び出す前に子プロセスが完了するまで実行されたときに発生します。この場合、子プロセスは`sem_post()`を最初に呼び出し、セマフォの値を0から1にインクリメントします。親プロセスが実行されると、`sem_wait()`がコールされ、セマフォの値が1かどうかを検索します。そうだったとき、親はその値を(0に)デクリメントし、待たずに`sem_wait()`から戻り、望んだ効果を得ます。

![](../31/img/fig31_8.PNG)

## 31.4 The Producer/Consumer (Bounded Buffer) Problem
この章で直面する次の問題は、プロデューサ/コンシューマの問題、時には限定されたバッファの問題[D72]として知られています。この問題は、前の章の条件変数で詳しく説明しています。詳細はこちらをご覧ください。

### First Attempt
この問題を解決するための最初の試みは、空といっぱいという2つのセマフォを導入します。これらのスレッドは、バッファエントリが空になったとき、またはいっぱいになったときを示すために使用します。putとgetルーチンのコードは図31.9にあり、プロデューサとコンシューマの問題を解決するための試みは図31.10にあります。

![](../31/img/fig31_9.PNG)
![](../31/img/fig31_10.PNG)

この例では、プロデューサはまずデータを格納するためにバッファが空になるのを待ち、コンシューマはバッファを使用する前にバッファがいっぱいになるのを待ちます。最初にMAX = 1(配列内にiという1つのバッファしかありません)と仮定し、これが機能するかどうかを見てみましょう。

再度、プロデューサとコンシューマの2つのスレッドがあると想像してください。単一のCPUで特定のシナリオを検討しましょう。コンシューマが最初に走ると仮定します。したがって、コンシューマは、sem_wait(＆full)を呼び出す図31.10のC1行目にヒットします。fullは値0に初期化されているため、コールはfullを減らして(-1)、コンシューマをブロックし、必要に応じて別のスレッドがfullで`sem_post()`を呼び出すのを待ちます。

プロデューサが実行されているとします。これはP1行目でヒットし、sem_wait(＆empty)ルーチンを呼び出します。コンシューマとは異なり、emptyは値MAX(この場合は1)に初期化されるため、プロデューサはこの行を介して継続します。したがってemptyは0にデクリメントされ、プロデューサはデータ値をバッファの最初のエントリに入れます(P2行目)。次に、プロデューサはP3に進み、sem_post(＆full)を呼び出し、fullのセマフォの値を-1から0に変更し、コンシューマを目覚めさせます(例えば、ブロックから準備完了に移動する)。

この場合、2つのうちの1つが起こる可能性があります。プロデューサが走り続けると、ループして再びP1行目にヒットします。ただし、emptyのセマフォの値が0であるため、今回はブロックされます。プロデューサが中断される代わりに、コンシューマが実行を開始すると、sem_wait(＆full)(C1行目)が呼び出され、消費します。いずれの場合でも、望んだの挙動を達成します。

この同じ例をもっと多くのスレッドで試すことができます(複数のプロデューサや複数のコンシューマなど)。それでも動作するはずです。

ここで、MAXが1より大きい(たとえばMAX = 10)と想像してみましょう。この例では、複数のプロデューサと複数のコンシューマが存在すると仮定します。ここで、問題を抱えています。それは競争状態です。どこで発生するのか見えていますか？(しばらく時間をとって見てください)見えない場合は、ヒントがあります。それは、`put()`と`get()`コード付近を詳しく見てください。

さて、問題を理解しましょう。2人のプロデューサー(PaとPb)が`put()`をほぼ同時に呼び出すとします。プロデューサPaが最初に実行され、最初のバッファエントリ(F1行目でfill = 0)を書き始めると仮定します。Paがfillカウンタを1にインクリメントするチャンスを得る前に、それは中断されます。プロデューサーPbが実行を開始し、F1行目でバッファーの0番目の要素にもデータが書き込まれます。つまり、古いデータが上書きされます。これはよくありません。プロデューサからのデータが失われることは望ましくありません。

### A Solution: Adding Mutual Exclusion
ご覧のとおり、ここで忘れてしまったのは相互排除です。バッファの充填とバッファへのインデックスのインクリメントはクリティカルセクションであるため、注意深く管理する必要があります。そのため、バイナリセマフォを使用していくつかのロックを追加しましょう。図31.11に私たちの試みを示します。

![](../31/img/fig31_11.PNG)

これで、NEW LINEコメントで示されているように、コードの`put()`/`get()`部分全体にいくつかのロックが追加されました。それは正しいアイデアのように見えますが、実際はうまくいきません。どうしてでしょうか？ヒントはデッドロックです。なぜデッドロックが発生するのでしょうか？それを考えてみましょう。デッドロックが発生するケースを見つけてみましょう。プログラムがデッドロックするためにはどのような手順を実行する必要がありますか？

### Avoiding Deadlock
さて、あなたは図を理解したので、ここに答えがあります。1つのプロデューサと1つのコンシューマの2つのスレッドを想像してください。まずコンシューマが最初に走ります。ミューテックス(C0行目)を取得し、fullセマフォ(c1行目)で`sem_wait()`を呼び出します。まだデータがないため、この呼び出しによってコンシューマはブロックしてCPUを生成します。重要なことですが、コンシューマはまだロックを保持しています。

プロデューサーが実行されます。それは生産するデータを持っていて、それが動くことができれば、それはコンシューマのスレッドを起こすことができ、すべてがうまくいくでしょう。残念なことに、最初に行うことは、バイナリミューテックスセマフォ(P0行目)に対して`sem_wait()`を呼び出すことです。ロックはすでに保持されています。したがって、プロデューサーは現在も待っています。

ここには単純なサイクルがあります。コンシューマはミューテックスを保持し、誰かがフルシグナルするのを待っています。プロデューサはフルシグナルを送ることができますが、ミューテックスを待っています。したがって、プロデューサとコンシューマはお互いに待っています。古典的なデッドロックです。

### At Last, A Working Solution
この問題を解決するには、単にロックの範囲を小さくする必要があります。図31.12に正しい解を示します。ご覧のとおり、ミューテックスの取得と解放をクリティカルセクションのまわりで行うだけです。フルおよび空の待機およびシグナルコードは外部に残されます。その結果、マルチスレッドプログラムでよく使用されるパターンである単純で作業効率の良い有限バッファーが得られます。それを今理解してください。後でそれを使用してください。あなたは何年も私たちに感謝することでしょう。

![](../31/img/fig31_12.PNG)

## 31.5 Reader-Writer Locks
別の古典的な問題は、異なるデータ構造アクセスが異なる種類のロックを必要とする可能性があることを認める、より柔軟なロックプリミティブに対する要望から生じます。たとえば、挿入や簡単な検索など、多数の並行リスト操作を想像してみてください。挿入がリストの状態を変える(したがって伝統的なクリティカルセクションが意味をなさない)間に、ルックアップは単にデータ構造を読み込みます。挿入が行われていないことを保証できれば、多くのルックアップを並行して進めることができます。このタイプの操作をサポートするために開発する特殊タイプのロックは、リーダライタロック[CHP71]として知られています。このようなロックのコードは、図31.13で使用できます。

![](../31/img/fig31_13.PNG)

コードはかなり簡単です。問題のデータ構造を更新したいスレッドがあれば、書き込みロックを獲得するための`rwlock_acquire_writelock()`とそれを解放するための`rwlock_release_writelock()`という新しい同期操作のペアを呼び出す必要があります。内部的には、これらは単に書き込みロックセマフォを使用して、1人のライターだけがロックを解除し、問題のデータ構造を更新するクリティカルセクションに入ることができるようにします。

さらに興味深いのは、読み取りロックを取得して解放するためのルーチンのペアです。読取りロックを獲得するとき、読者は最初にロックを獲得し、次に読者変数をインクリメントして、現在データ構造内にある読者の数を追跡します。`rwlock_acquire_readlock()`内で実行される重要なステップは、最初の読者がロックを取得したときに発生します。その場合、読者は、書き込みロックセマフォに対して`sem_wait()`を呼び出し、`sem_post()`を呼び出してロックを解放することによって、書き込みロックを取得します。

したがって、読者が読取りロックを取得すると、より多くの読者が読取りロックを取得することも許可されます。ただし、書き込みロックを取得するスレッドは、すべての読者が終了するまで待機する必要があります。クリティカルセクションを終了する最後のものは"writelock"の`sem_post()`を呼び出すので、待機中のライターはロックを獲得できます。このアプローチは(必要に応じて)機能しますが、特に公平性に関しては、いくつかのネガティブな点があります。特に、読者がライターを飢えさせるのは比較的簡単です。この問題に対するより高度な解決策が存在します。おそらくあなたはより良い実装を考えることができますか？ヒント：ライターが待っていると、より多くの読者がロックに入るのを防ぐために、あなたがする必要があることを考えてください。

最後に、リーダライタのロックを注意して使用する必要があることに注意してください。それらはしばしばオーバーヘッドが発生します(特により洗練された実装では)、単純で高速なロックプリミティブ[CB08]を使用するのと比べてパフォーマンスのスピードアップにつながりません。いずれにしても、セマフォを興味深く有用な方法で使用する方法をもう一度紹介します。

>> TIP: SIMPLE AND DUMB CAN BE BETTER (HILL’S LAW)  
>> シンプルで愚かなアプローチが最高のものになるという考えを過小評価するべきではありません。ロックすると、実装が簡単で高速であるため、単純なスピンロックが最適な場合があります。リーダライタのようなものはロックされていますが、複雑で複雑なものは遅いという意味です。したがって、まず、シンプルで愚かなアプローチをまず試みてください。
シンプルさに訴えるこのアイデアは、多くの場所で見られます。初期の情報源は、Mark Hillの論文[H87]であり、CPU用のキャッシュを設計する方法を研究しました。ヒル氏は、シンプルなダイレクトマップキャッシュは、ファンシーで先進的なデザインよりも優れていることを発見しました(キャッシングでは、デザインが単純なため、検索が高速になります)。ヒルが簡潔に彼の作品を要約したように、「巨大で愚かな方が良い」と言いました。

## 31.6 The Dining Philosophers
ダイクストラによって提起され、解決されたもっとも有名な並行性の問題の1つは、食堂の哲学者の問題[D71]として知られています。この問題は、それが楽しく、多分知的に面白いので有名です。しかし、その実用性は低いです。しかし、その名声はここに含まれています。実際には、ある面接でそれについて尋ねられるかもしれませんし、あなたがその質問を見落として仕事を得れなかったら、あなたはOSの教授を本当に恨むでしょう。逆に、あなたが仕事を得たら、あなたのOS教授に素敵なメモ、またはいくつかのストックオプションを送ってください。

問題の基本的な設定はこれです(図31.14を参照)。テーブルの周りに5人の"哲学者"が座っているとします。哲学者の各ペアの間には1つのフォーク(したがって合計5つ)があります。哲学者はそれぞれ考えている時間があり、フォークや食べる時間は必要ありません。食べるためには、哲学者は2本のフォークを必要とします。左側のものと右側のものの両方があります。これらのフォークの競合、それに続く同期の問題は、これを同時プログラミングで研究する問題となっています。

![](../31/img/fig31_14.PNG)

各哲学者の基本的なループは次のとおりです。
```c
while (1) {
    think();
    getforks();
    eat();
    putforks();
}
```
キーとなる挑戦は、デッドロックがなく、哲学者が飢えず、食べることもなく、並行性が高い(つまり、同時に多くの哲学者が同時に食べることができるように)書き込みのルーチンである`getforks()`と`putfork()`をできるだけ実行できるようにすることです。

ダウニーのソリューション[D08]に続いて、いくつかのヘルパー関数を使用して、私たちを解決策に導きます。
```c
int left(int p) { return p; }
int right(int p) { return (p + 1) % 5; }
```
哲学者pが左のフォークを参照したいとき、彼らは単にleft(p)を呼び出します。同様に、哲学者pの右側のフォークはright(p)を呼び出すことによって参照されます。その中の剰余演算子は、最後の哲学者(p = 4)がフォークが0であるとき右手でつかもうとする処理をします。

また、この問題を解決するためにセマフォーが必要になります。それぞれのフォークに対して1つずつ、sem_t forks [5]が5つあるとします。

### Broken Solution
我々はこの問題に対する最初の解決策を試みます。各セマフォー(フォーク配列内の)を1の値に初期化するものとします。各哲学者がそれ自身の数(p)を知っているとします。したがって、図31.15に示す`getforks()`と`putforks()`ルーチンを記述することができます。

![](../31/img/fig31_15.PNG)

この(壊れた)ソリューションの背後にある直感は次のとおりです。フォークを取得するには、まずそれぞれのロックを取得します。最初は左に、次に右にあるロックです。食べ終わると、私たちはそれを解放します。シンプルですよね？残念ながら、この場合、単純な手段は壊れています。発生した問題を理解できますか？それについて考えてみましょう。

問題はデッドロックです。哲学者が自分の右にあるフォークをつかむ前に、各哲学者が左にあるフォークをつかんでしまったら、それぞれのフォークをつかんで別のものを永遠に待つことになります。具体的には、哲学者0はフォーク0、哲学者1はフォーク1、哲学者2はフォーク2、哲学者3はフォーク3を、哲学者4はフォーク4をつかむ。すべてのフォークが獲得され、すべての哲学者が別の哲学者が所有するフォークを待っています。私たちはすぐにデッドロックを詳しく研究します。今のところ、これは実用的な解決策ではないと言っても過言ではありません。

### A Solution: Breaking The Dependency
この問題を攻撃する最も簡単な方法は、哲学者の少なくとも1人がフォークを取得する方法を変更することです。確かに、これはDijkstra自身がどのように問題を解決したかです。具体的には、哲学者4(一番高い番号の哲学者)がフォークを別の順序で取得すると仮定しましょう。これを行うコードは次のとおりです。

![](../31/img/fig31_15_1.PNG)

最後の哲学者は左手の前に右手をつかみようとしているので、各哲学者が1つのフォークをつかみ、別のフォークを待っている状況はありません。これで待ちのサイクルが解消されました。このソリューションの成果を考えて、それが機能することを自分自身で動かしてください。

このような他の「有名な」問題、例えばたばこ喫煙者の問題または睡眠中の理髪師の問題などがあります。それらのほとんどは、並行性について考えることの言い訳です。それらのいくつかは魅力的な名前を持っています。より多くのことを学ぶことに興味がある場合は、それらを見てください。あるいは同時により多くの練習を同時に行うことができます[D08]。

## 31.7 How To Implement Semaphores
最後に、低レベルの同期プリミティブ、ロック、および条件変数を使用して、...(ドラムロール)というセマフォの独自のバージョンを構築しましょう...Zemaphores。図31.16に示すように、この作業はかなり簡単です。

![](../31/img/fig31_16.PNG)

この図からわかるように、セマフォの値を追跡するために、ロックと条件変数を1つだけ使用し、状態変数を使用します。あなたが本当に理解するまで、あなた自身のためにコードを研究してください。ダイクストラによって定義されたZemaphoreと純粋なセマフォの1つの微妙な違いは、セマフォの値が負の場合は待機スレッドの数を反映するという不変値を維持しないことです。実際には、値は決してゼロより低くなることはありません。この動作は実装が容易で、現在のLinuxの実装と一致します。


>> TIP: BE CAREFUL WITH GENERALIZATION  
>>したがって、一般化の抽象的な技法は、システム設計において非常に有用であり、そこでは、1つの良いアイデアをわずかに広げてより大きなクラスの問題を解決することができます。ただし、一般化するときは注意してください。Lampsonは私たちに"一般化しないでください。一般化は一般に間違っている[L83]。  
>>セマフォをロックと条件変数の一般化として見ることができます。しかし、そのような一般化が必要ですか？また、セマフォの上に条件変数を実現するのが難しいことを考えると、おそらくこの一般化は一般的ではありません。

不思議なことに、セマフォから条件変数を構築することは、はるかにトリッキーな命題です。経験の豊富な並行プログラマの中には、Windows環境でこれを実行しようとしたものがあり、さまざまなバグが発生しました[B04]。自分で試して、セマフォからビルド条件変数を表示するのがなぜ難しいかを理解できるかどうかを確認してください。

## 31.8 Summary
セマフォは、並行プログラムを作成するための強力かつ柔軟なプリミティブです。一部のプログラマーは、そのシンプルさと有用性に、ロックと条件変数を避け、独占的にそれらを使用しています。この章では、いくつかの古典的な問題と解決策を紹介しました。詳細を知りたい場合は、参照できる他の多くの資料があります。1つの偉大な(そして自由な参照)は、並行性とセマフォー[D08]によるプログラミングに関するAllen Downeyの本です。この本は、一般的に特定の並行性と並行性の両方のセマフォの理解を向上させるために取り組むことができる多くのパズルを持っています。真の同時実行性の専門家になるには長年の努力が必要です。それが、このクラスで学んだことを超えて、マスターする鍵です。

# 参考文献
[B04] “Implementing Condition Variables with Semaphores”  
Andrew Birrell  
December 2004  
An interesting read on how difficult implementing CVs on top of semaphores really is, and the mistakes the author and co-workers made along the way. Particularly relevant because the group had done a ton of concurrent programming; Birrell, for example, is known for (among other things) writing various thread-programming guides.

[CB08] “Real-world Concurrency”  
Bryan Cantrill and Jeff Bonwick  
ACM Queue. Volume 6, No. 5. September 2008  
A nice article by some kernel hackers from a company formerly known as Sun on the real problems faced in concurrent code.

[CHP71] “Concurrent Control with Readers and Writers”  
P.J. Courtois, F. Heymans, D.L. Parnas  
Communications of the ACM, 14:10, October 1971  
The introduction of the reader-writer problem, and a simple solution. Later work introduced more complex solutions, skipped here because, well, they are pretty complex.

[D59] “A Note on Two Problems in Connexion with Graphs”  
E. W. Dijkstra  
Numerische Mathematik 1, 269271, 1959  
Available: http://www-m3.ma.tum.de/twiki/pub/MN0506/WebHome/dijkstra.pdf  
Can you believe people worked on algorithms in 1959? We can’t. Even before computers were any fun to use, these people had a sense that they would transform the world...

[D68a] “Go-to Statement Considered Harmful”  
E.W. Dijkstra  
Communications of the ACM, volume 11(3): pages 147148, March 1968  
Available: http://www.cs.utexas.edu/users/EWD/ewd02xx/EWD215.PDF  
Sometimes thought as the beginning of the field of software engineering.

[D68b] “The Structure of the THE Multiprogramming System”  
E.W. Dijkstra  
Communications of the ACM, volume 11(5), pages 341346, 1968  
One of the earliest papers to point out that systems work in computer science is an engaging intellectual endeavor. Also argues strongly for modularity in the form of layered systems.

[D72] “Information Streams Sharing a Finite Buffer”  
E.W. Dijkstra  
Information Processing Letters 1: 179180, 1972  
Available: http://www.cs.utexas.edu/users/EWD/ewd03xx/EWD329.PDF  
Did Dijkstra invent everything? No, but maybe close. He certainly was the first to clearly write down what the problems were in concurrent code. However, it is true that practitioners in operating system design knew of many of the problems described by Dijkstra, so perhaps giving him too much credit would be a misrepresentation of history.

[D08] “The Little Book of Semaphores”  
A.B. Downey  
Available: http://greenteapress.com/semaphores/  
A nice (and free!) book about semaphores. Lots of fun problems to solve, if you like that sort of thing.

[D71] “Hierarchical ordering of sequential processes”  
E.W. Dijkstra  
Available: http://www.cs.utexas.edu/users/EWD/ewd03xx/EWD310.PDF  
Presents numerous concurrency problems, including the Dining Philosophers. The wikipedia page about this problem is also quite informative.

[GR92] “Transaction Processing: Concepts and Techniques”  
Jim Gray and Andreas Reuter  
Morgan Kaufmann, September 1992  
The exact quote that we find particularly humorous is found on page 485, at the top of Section 8.8: “The first multiprocessors, circa 1960, had test and set instructions ... presumably the OS implementors worked out the appropriate algorithms, although Dijkstra is generally credited with inventing semaphores many years later.”

[H87] “Aspects of Cache Memory and Instruction Buffer Performance”  
Mark D. Hill  
Ph.D. Dissertation, U.C. Berkeley, 1987  
Hill’s dissertation work, for those obsessed with caching in early systems. A great example of a quantitative dissertation.

[L83] “Hints for Computer Systems Design”  
Butler Lampson  
ACM Operating Systems Review, 15:5, October 1983  
Lampson, a famous systems researcher, loved using hints in the design of computer systems. A hint is something that is often correct but can be wrong; in this use, a signal() is telling a waiting thread that it changed the condition that the waiter was waiting on, but not to trust that the condition will be in the desired state when the waiting thread wakes up. In this paper about hints for designing systems, one of Lampson’s general hints is that you should use hints. It is not as confusing as it sounds.

\newpage

## 32 Common Concurrency Problems
研究者は長年にわたり並行性のバグを探すために多大な時間と労力を費やしてきました。初期の作業の多くは、デッドロックに焦点を絞っていました。これは過去の章で触れたことのあるトピックですが、深く[C + 71]に浸透します。最近の研究は、他のタイプの共通の並行性バグ(非デッドロックバグ)の研究に重点を置いています。この章では、実際のコードベースに見られるいくつかの並行処理の問題の例を簡単に見て、どのような問題を把握するかをより深く理解します。したがって、この章の中心的な問題は次のとおりです。

>> CRUX: HOW TO HANDLE COMMON CONCURRENCY BUGS  
>> 並行性のバグは、さまざまな共通パターンになる傾向があります。より堅牢で正確な並行コードを作成するための第一歩です。

## 32.1 What Types Of Bugs Exist?
最初の、そして最も明白な問題は、複雑な並行プログラムでどのような並行性バグが現れるかということです。この質問は一般的には答えにくいですが、幸いなことに他の人たちが私たちの仕事をしてくれました。具体的には、我々はLuらの研究に頼っています。[L + 08]は、実用上どのような種類のバグが発生しているのかを把握するために、多数の一般的な並列アプリケーションを詳細に分析しています。

この研究では、MySQL(人気の高いデータベース管理システム)、Apache(よく知られているWebサーバー)、Mozilla(有名なWebブラウザ)、OpenOffice(MS Officeスイートの無料版で一部の人々が実際に使用しています)の4つの主要で重要なオープンソースアプリケーションに焦点を当てています。。この調査では、開発者の作業を定量的なバグ分析に変えながら、これらのコードベースで検出され、修正された並行性のバグを調べました。これらの結果を理解することで、成熟したコードベースで実際に発生する問題の種類を理解するのに役立ちます。

図32.1に、Lu氏と同僚が研究したバグの概要を示します。図から、合計105個のバグがあり、そのほとんどはデッドロックではなかったことが分かります(74)。残りの31個はデッドロックバグでした。さらに、各アプリケーションから調査されたバグの数を確認できます。OpenOfficeには8つの同時実行性のバグしかありませんでしたが、Mozillaは約60個のバグがありました。

![](../32/img/fig32_1.PNG)

これらの異なるクラスのバグ(非デッドロック、デッドロック)にもう少し深く入り込むようになりました。非デッドロックバグの最初のクラスでは、この調査の例を使って議論を進めています。デッドロックバグの2番目のクラスについては、デッドロックの防止、回避、または処理のいずれかで行われた長い作業について説明します。

## 32.2 Non-Deadlock Bugs
Luの研究によると、デッドロックのないバグが同時性バグの大部分を占めています。しかし、どのタイプのバグですか？彼らはどのように起きますか？どうすれば修正できますか？我々は今から、Luらによって発見された2種類の非デッドロックバグについて議論します。それは原子性違反バグと順序違反バグです。

### Atomicity-Violation Bugs
遭遇する第1のタイプの問題は、原子性違反と呼ばれます。ここでは、MySQLにある簡単な例を示します。説明を読む前に、バグが何であるかを調べてみてください。

![](../32/img/fig32_1_1.PNG)

この例では、2つの異なるスレッドが構造体thcのproc_infoフィールドにアクセスします。最初のスレッドは、値がNULLでないかどうかをチェックし、その値を出力します。2番目のスレッドはNULLに設定します。明らかに、最初のスレッドがチェックを実行したがfputsの呼び出しの前に中断された場合、2番目のスレッドが中間で実行され、ポインタがNULLに設定される可能性があります。最初のスレッドが再開すると、NULLポインタがfputsによって参照解除されるため、クラッシュします。

Lu他によれば、「複数のメモリアクセスの間の望んだ直列化可能性が侵害されている(すなわち、コード領域はアトミックであることが意図されているが、実行中にアトミック性は強制されない)」という原則違反のより正式な定義です。上記の例では、コードは、proc_infoのNULLでないことのチェックと`fputs()`呼び出しでのproc_infoの使用について原子性の仮定(Luの言葉で)を持っています。前提が正しくない場合、コードは必要に応じて機能しません。

このタイプの問題に対する修正を見つけることは、(必ずしもそうではないが)簡単です。上記のコードを修正する方法を考えてみませんか？この解決策では、共有変数参照をロックするだけで、いずれかのスレッドがproc_infoフィールドにアクセスするときにロックが保持されるようになります(proc_info_lock)。もちろん、構造体にアクセスする他のコードでも、このロックを取得してからロックを取得する必要があります。

![](../32/img/fig32_1_2.PNG)

### Order-Violation Bugs
Luらによって発見された別の一般的なタイプの非デッドロックバグは順序違反(order violation)として知られています。ここに別の簡単な例があります。もう一度、以下のコードにバグがある理由を理解できるかどうかを確認してください。

![](../32/img/fig32_1_3.PNG)

図からわかるとおり、スレッド2のコードは、変数mThreadがすでに初期化されている(NULLではない)と仮定しているようです。ただし、スレッド2が一度作成されるとすぐに、スレッド2の`mMain()`内でmThreadの値がアクセスされたときに値が設定されず、NULLポインタの逆参照でクラッシュする可能性があります。注意としてはmThreadの値は最初はNULLであると仮定しています。もしそうでなければ、任意のメモリ位置がスレッド2の逆参照によってアクセスされるので、よく分からないことが起こってしまうかもしれません。

より正式な定義の順序違反(order violation)は、「2つの(グループの)メモリアクセスの間の望んだ順序は反転します」です。(すなわち、Aは常にBの前に実行されなければならないが、実行中はその順序は強制されない)[L + 08 ]。

この種のバグを修正するには、一般に順序付けが必要です。前に詳細に説明したように、条件変数を使用すると、このスタイルの同期を最新のコードベースに追加するのは簡単で堅牢な方法です。上記の例では、次のようにコードを書き直すことができます：

![](../32/img/fig32_1_4.PNG)

この固定コードシーケンスでは、ロック(mtLock)と対応する条件変数(mtCond)と状態変数(mtInit)を追加しました。初期化コードが実行されると、mtInitの状態が1に設定され、完了したことを通知します。この時点より前にスレッド2が実行された場合、スレッド2はこの信号および対応する状態の変更を待機します。それが後で実行されると、状態をチェックし、初期化が既に行われている(すなわち、mtInitが1にセットされている)ことを確認し、したがって適切なまま続行します。状態変数自体としてmThreadを使用する可能性がありますが、ここでは簡略化のためにそうしないことに注意してください。スレッド間で問題を発注する際には、条件変数(またはセマフォ)が助ける可能性があります。

### Non-Deadlock Bugs: Summary
Luらが研究した非デッドロックバグの大部分(97％)原子性または順序違反のいずれかです。したがって、これらのタイプのバグパターンについて注意深く考えることによって、プログラマーはそれらを避けるより良い仕事をする可能性があります。さらに、より自動化されたコードチェックツールが開発されるにつれて、デプロイメントで見つかった非デッドロックバグの大部分を構成するため、これらの2種類のバグに重点を置くべきです。

残念ながら、すべてのバグが上記の例と同じように簡単に解決できるわけではありません。あるものは、プログラムが何をしているのか、コードやデータ構造の再編成をより多く理解する必要があります。詳細は、Luらの優れた(そして判読可能な)論文を読んでください。

## 32.3 Deadlock Bugs
上記で説明した並行性のバグ以外にも、複雑なロックプロトコルを持つ多くの同時システムで発生する古典的な問題は、デッドロックと呼ばれています。例えば、スレッド(スレッド1)がロック(L1)を保持し、ロック(L2)を保持しているスレッドを待っています。残念ながら、ロックL2を保持するスレッド(スレッド2)は、L1が解放されるのを待っています。このような潜在的なデッドロックを示すコードスニペットは次のとおりです。
```c
Thread 1:               Thread 2:
pthread_mutex_lock(L1); pthread_mutex_lock(L2);
pthread_mutex_lock(L2); pthread_mutex_lock(L1);
```
このコードが実行されると、デッドロックが必ずしも発生しないことに注意してください。一方で起こる可能性があるとしたら、たとえば、スレッド1がL1をロックしてからスレッド2にコンテキストスイッチが発生した場合、スレッド2はL2を取得してL1を取得しようとします。したがって、各スレッドはもう一方を待っており、どちらも実行できないため、デッドロックが発生します。図32.2を参照してください。グラフ内のサイクルの存在は、デッドロックを示す。

図は問題をはっきりさせるはずです。デッドロックを何らかの方法で処理するために、プログラマーはどのようにコードを書くべきですか？

![](../32/img/fig32_2.PNG)

>> CRUX: HOW TO DEAL WITH DEADLOCK  
>> どのようにしてデッドロックを防止、回避、または少なくとも検出して回復するためのシステムを構築する必要がありますか？これは今日のシステムで本当の問題ですか？

### Why Do Deadlocks Occur?
あなたが思っているように、上記のような単純なデッドロックは容易に避けることができます。たとえば、スレッド1と2の両方が同じ順序でロックを取得することを確実にした場合、デッドロックは決して発生しません。なぜデッドロックが起こるのですか？

1つの理由は、大きなコードベースでは、複雑な依存関係がコンポーネント間で発生するということです。たとえば、オペレーティングシステムを起動します。仮想メモリシステムは、ディスクからブロックをページするためにファイルシステムにアクセスする必要があります。ファイルシステムはその後、ブロックを読み込んで仮想メモリシステムに接続するためにメモリのページを要求することがあります。したがって、コード内で自然に発生する可能性のある循環依存関係の場合、デッドロックを回避するために、大規模システムでのロック戦略の設計を慎重に行う必要があります。

別の理由はカプセル化の性質によるものです。ソフトウェア開発者として、実装の詳細を隠し、ソフトウェアをモジュール化しやすくするように教えています。残念なことに、このようなモジュール化はロックとうまく合致しません。Julaらは、 [J + 08]を指摘すると、いくつかの一見無害なインターフェースは、ほとんどあなたをデッドロックに招待します。たとえば、Java Vectorクラスとメソッド`AddAll()`を使用します。このルーチンは次のように呼び出されます。
```c
Vector v1, v2;
v1.AddAll(v2);
```
内部的には、メソッドがマルチスレッドセーフである必要があるため、(v1)に追加されるベクトルとパラメータ(v2)の両方のロックが取得される必要があります。ルーチンは、v2の内容をv1に追加するために、任意の順序(例えばv1、v2)で前記ロックを獲得します。しかし、もし、他のスレッドがv2.AddAll(v1)をほぼ同時に呼び出すと、呼び出し元アプリケーションからは隠されたデッドロックの可能性があります。

### Conditions for Deadlock
デッドロックが発生するには4つの条件が必要です[C + 71]。
- 相互排除：スレッドは、必要なリソースの排他制御を要求します(スレッドはロックを取得します)。

- ホールド・アンド・ウェイト：スレッドは、追加のリソース(例えば、取得したいロック)を待つ間に、スレッドに割り当てられたリソース(例えば、既に獲得したロック)を保持します。

- プリエンプションなし：リソース(ロックなど)は、リソースを保持しているスレッドから強制的に削除することはできません。

- 循環待ち：各スレッドが、チェーン内の次のスレッドによって要求されている1つ以上のリソース(例えば、ロック)を保持するように、スレッドの循環チェーンが存在する。

これらの4つの条件のいずれかが満たされない場合、デッドロックは発生しません。そこで、まず、デッドロックを防止するための手法を探る。これから説明するそれぞれの戦略は、上記の条件の1つが発生するのを防ぎ、デッドロック問題を処理する1つのアプローチです。

### Prevention
### Circular Wait
おそらく最も実用的な予防技術(そして確かに頻繁に採用されているもの)は、あなたが循環待ちを誘発しないようにあなたのロッキングコードを書くことでしょう。これを実行する最も簡単な方法は、ロック取得のトータルオーダーを行うことです。たとえば、システムに2つのロック(L1とL2)しかない場合、L2の前に常にL1を取得することでデッドロックを防ぐことができます。このような厳密な順序付けによって、循環待ちが発生しないことが保証されます。したがって、デッドロックは発生しません。

もちろん、より複雑なシステムでは、2つ以上のロックが存在するため、完全なロック順序を達成するのが難しい場合があります(そして、おそらく完全なロック順序付けは不要です)。したがって、部分順序付けは、デッドロックを回避するためにロック獲得を構造化するための有用な方法となり得ます。部分ロック順序の優れた実際の例は、Linux [T + 94]のメモリマッピングコードで見ることができます。ソースコードの先頭にあるコメントは、「i_mutex before i_mmap_mutex」などの単純なロック取得命令と、「i_mmap_mutex before private_lock before swap_lock before mapping->tree_lock」のような複雑な命令を含む10種類のロック取得オーダーを示しています。

あなたが想像することができるように、全部と部分的な順序付けはロック戦略の慎重な設計を必要とし、細心の注意を払って構築しなければなりません。さらに、順序付けは単に規約に過ぎず、誤ったプログラマーは簡単にロックプロトコルを無視し、デッドロックを引き起こす可能性があります。最後に、ロックの順序付けには、コードベースの深い理解と、さまざまなルーチンの呼び出し方法が必要です。たった1つの間違いが「D」という言葉になる可能性があります。

>> TIP: ENFORCE LOCK ORDERING BY LOCK ADDRESS
>> 場合によっては、関数が2つ(またはそれ以上)のロックを取得する必要があります。したがって、我々は注意または、デッドロックが発生する可能性があることを知っています。do_something(mutex t * m1、mutex t * m2)と呼ばれる関数を想像してみてください。もし、常にm2より前にm1(または常にm1より前にm2)を取得してしまうとデッドロックを発生する可能性があります。なぜなら、一つのスレッドがdo_something(L1、L2)を呼び出すことができるため、別のスレッドがdo_something(L2、L1)を呼び出すことができるからです。  
この特別な問題を回避するために、巧妙なプログラマはロックの取得を順序付けする方法として各ロックのアドレスを使用することができます。high-to-lowまたはlow-to-highのどちらかのアドレス順でロックを取得することで、どの順序でも関係なく同じ順序(決まった順序)でロックを取得することが保証されます。このような例です：  
![](../32/img/fig32_2_2.PNG)  
この単純な技術を使用することにより、プログラマは、マルチロック取得のシンプルで効率的なデッドロックフリー実装を保証することができます。

### Hold-and-wait
デッドロックのホールド・アンド・ウェイトの要件は、すべてのロックを一度に取得することで回避できます。実際には、これは以下のように達成します。  
![](../32/img/fig32_2_3.PNG)  
最初にロック防止を取得することにより、このコードは、ロック取得の途中でスレッド交換が行われないこと、そのようなデッドロックを再び回避できることを保証します。もちろん、スレッドがロックを取得するたびに、グローバル防御ロックを取得する必要があります。たとえば、別のスレッドがロックL1とL2を別の順序で取得しようとしていた場合は、その間にロック防止を保持しているのでOKです。

解決策にはいくつかの理由で問題があることに注意してください。以前のように、カプセル化は私たちに対して働いています。ルーチンを呼び出すときに、このアプローチでは、ロックを保持する必要があることを正確に把握し、事前に取得する必要があります。この技術はまた、すべてのロックを真に必要とされるのではなく、早めに(一度に)取得する必要があるため、並行性を低下させる可能性があります。

### No Preemption
ロック解除が呼び出されるまで、一般にロックを保持しているとみなすので、複数のロックを取得すると、ロックを待っているときに別のロックを持っているので、問題が発生することがあります。多くのスレッドライブラリは、このような状況を回避するために、より柔軟なインタフェースを提供します。具体的には、ルーチン`pthread_mutex_trylock()`はロックを取得して(利用可能な場合)、成功を返します。ロックが保持されていることを示すエラーコードを返します。後者の場合、そのロックを取得したい場合は、後で再試行できます。このようなインタフェースは、次のように使用して、デッドロックのない、順序付けが可能なロック取得プロトコルを構築することができます。  
![](../32/img/fig32_2_4.PNG)  
別のスレッドは同じプロトコルに従うことができますが、他の順序(L2からL1)でロックを取得し、プログラムはデッドロックフリーであることに注意してください。しかし、新たな問題が生まれます。それは。ライブロックです。2つのスレッドがこのシーケンスを繰り返し試行し、両方のロックを繰り返し取得できない可能性があります(おそらくありえません)この場合、両方のシステムはこのコードシーケンスを何度も繰り返し実行しています(したがって、デッドロックではありません)が進行中ではありません。したがって、ライブロックという名前があります。ライブロックの問題に対する解決策もあります。たとえば、ループバックする前にランダムな遅延を追加し、全体をもう一度試すことで、競合するスレッド間の干渉が繰り返される可能性を減らすことができます。

この解決策についての最後の1つのポイントは、それはtrylockアプローチを使用するという困難な部分を取り巻いています。再度存在する可能性のある第1の問題は、カプセル化です。これらのロックのうちの1つが呼び出されているルーチンに埋め込まれていると、先頭に戻ってジャンプするというより複雑な実装になります。コードが途中でいくつかのリソース(L1以外)を取得していた場合は、それらも注意深く解放する必要があります。たとえば、L1を取得した後でコードにメモリが割り当てられていた場合、L2を取得できなかった場合にそのメモリを解放してから、先頭に戻ってシーケンス全体をやり直す必要があります。しかしながら、限定された状況(例えば、前述のJavaベクトル法)では、このタイプのアプローチはうまくいく可能性があります。

### Mutual Exclusion
最終的な防止手法は、相互排除の必要性を全く避けることです。一般的に、実行したいコードには本当に重要な部分があるので、これは困難です。だから私たちは何をすることができますか？Herlihyは、ロックなしで様々なデータ構造を設計できるという考え方を持っていました[H91、H93]。これらのロックフリー(および関連する待機フリー)アプローチのアイデアは簡単です。強力なハードウェア命令を使用すると、明示的なロックを必要としない方法でデータ構造を構築することができます。  
簡単な例として、compare-and-swap命令があるとしましょう。これは、ハードウェアが提供する以下のようなアトミック命令です。  
![](../32/img/fig32_2_5.PNG)  
ある量だけ値を原子的に増やしたかったとします。私たちは以下のようにそれを行うことができました。  
![](../32/img/fig32_2_6.PNG)  
ある特定の値をアトミックにインクリメントしたいとしたら、その値を新しい量に更新しようと繰り返し試み、比較とスワップを使用してアプローチを構築したとします。この方法では、ロックは取得されず、デッドロックは発生しません(ライブロックはまだ可能ですが)。もう少し複雑な例を考えてみましょう。それはリストの挿入です。リストの先頭に挿入するコードを次に示します。  
![](../32/img/fig32_2_7.PNG)  
このコードは簡単な挿入を行いますが、"同時に"複数のスレッドから呼び出された場合、競合状態になります(理由を調べることができるかどうかを確認してください)。もちろん、このコードをロックの獲得と解放で囲むことで、これを解決できます。  
![](../32/img/fig32_2_8.PNG)  
このソリューションでは、従来の方法でロックを使用しています。代わりに、単純にcompare-and-swap命令を使用してこの挿入をロックフリーの方法で実行しようとします。可能なアプローチは次のとおりです。  
![](../32/img/fig32_2_9.PNG)  
ここのコードは、現在のヘッドを指し示す次のポインタを更新し、新しく作成されたノードをリストの新しいヘッドとしてスワップしようとします。しかし、一方で他のスレッドが新しいヘッドでスワップを成功させ、このスレッドを新しいヘッドで再試行すると、これは失敗します。

もちろん、有用なリストを作成するには単なるリストの挿入以上のものが必要であり、意外なことに、ロックフリーの方法で挿入、削除、および検索を実行できるリストを構築することは簡単ではありません。ロックフリーと待ち時間のない同期に関する豊富な文献を読んで詳細[H01、H91、H93]を参照してください。

### Deadlock Avoidance via Scheduling
デッドロック防止の代わりに、デッドロック回避が望ましい場合もあります。回避は、様々なスレッドが実行中にどのロックを獲得するかについてのグローバルなロックに関する知識を必要とし、その後デッドロックが発生しないことを保証するように前記スレッドをスケジュールします。

たとえば、2つのプロセッサと4つのスレッドをスケジューリングする必要があるとします。 さらに、スレッド1(T1)がL1とL2をロックしていることを知っているとします(T2は実行中のある時点で何らかの順序でロックします)、L1とL2も同様に把持し、T3はL2だけを捕捉し、T4はロックをまったく持ちません。 これらのスレッドのロック獲得要求を表形式で示すことができます。  
![](../32/img/fig32_2_10.PNG)  
したがって、スマートスケジューラは、T1とT2が同時に実行されない限り、デッドロックは発生しない可能性があると計算できます。そのようなスケジュールの1つがあります  
![](../32/img/fig32_2_11.PNG)  
(T3とT1)または(T3とT2)が重なってもかまいません。T3はL2をロックしますが、1つのロックしか持たないため、他のスレッドと並行してデッドロックを引き起こすことはありません。もう1つの例を見てみましょう。この場合、次の競合テーブルで示されるように、同じリソース(再びL1およびL2のロック)に対してより多くの競合が発生します。  
![](../32/img/fig32_2_12.PNG)  
特に、スレッドT1、T2、およびT3は、実行中のある時点で両方のロックL1およびL2を取得する必要があります。デッドロックが発生しないことを保証するスケジュールがあります  
![](../32/img/fig32_2_13.PNG)  
ご覧のように、静的スケジューリングは、T1、T2、T3がすべて同じプロセッサ上で実行されるという控えめなアプローチにつながります。したがって、ジョブを完了するための合計時間が大幅に長くなります。これらのタスクを同時に実行することは可能かもしれませんが、デッドロックの恐れが私たちを妨げ、コストとして支払うのはパフォーマンスです。

このようなアプローチの有名な例として、DijkstraのBanker's Algorithm [D64]があり、多くの同様のアプローチが文献に記載されています。残念ながら、非常に限られた環境、例えば、実行する必要のある一連のタスクと必要なロックを完全に把握している組み込みシステムの場合にのみ役立ちます。さらに、このようなアプローチは、上記の2番目の例で見たように、並行性を制限する可能性があります。したがって、スケジューリングによるデッドロックの回避は、汎用的なソリューションではありません。

### Detect and Recover
最終的な一般的な戦略の1つは、デッドロックが頻繁に発生するようにし、そのようなデッドロックが検出されたら何らかのアクションをとることです。たとえば、OSが1年に1回凍結した場合、OSを再起動して作業で喜んで(またはうんざりして)取得します。デッドロックがまれである場合、そのような非解決策は確かにかなり実用的です。

>> TIP: DON’T ALWAYS DO IT PERFECTLY (TOM WEST’S LAW)  
>> 古典的なコンピュータ業界の書籍「Soul of a New Machine [K81]」の主題で有名なTom Westは、「価値あることは何もかもうまくいくわけではない」という著名な技術的格言である。悪いことがまれにしか起こらない場合は、特に、悪いことが発生するコストが小さい場合には、そのことを防ぐために多大な努力を費やすべきではありません。一方、スペースシャトルを建設していて、間違ったことのコストがスペースシャトルが爆発している場合は、このアドバイスを無視してください。

多くのデータベースシステムは、デッドロックの検出と回復のテクニックを採用しています。デッドロック検出器は定期的に実行され、リソースグラフを作成し、サイクルごとにチェックします。サイクル(デッドロック)が発生した場合は、システムを再起動する必要があります。より複雑なデータ構造の修復が最初に必要とされる場合、プロセスを容易にするために人間が関与するかもしれません。データベースの並行性、デッドロック、および関連する問題の詳細は、他の場所で確認できます[B + 87、K87]。これらの作品を読んだり、データベースを使ってこの豊富で興味深いトピックについて学んでください。

## 32.4 Summary
この章では、並行プログラムで発生するバグの種類について検討しました。最初のタイプの非デッドロックバグは、驚くほど一般的ですが、そのほとんどは修正するのが簡単です。これらには、一緒に実行されるべき一連の命令ではなく、2つのスレッド間で必要な順序が強制されていない違反を命令する、原子性違反が含まれます。

デッドロックについても簡単に説明しました。デッドロックはなぜ発生するのか、それについて何ができるのですか。この問題は並行処理自体と同じくらい古いものであり、このトピックについて何百もの論文が書かれています。実際の最善の解決策は、慎重に、ロック獲得命令を開発し、最初にデッドロックが発生するのを防ぐことです。待ち受けのないデータ構造は、Linuxを含む一般的なライブラリやクリティカルなシステムへの道を切り開いているため、待ち時間のないアプローチも有望です。しかし、一般性の欠如と新しい待ち時間のないデータ構造を開発する複雑さが、このアプローチの全体的な有用性を制限する可能性があります。おそらく、最良のソリューションは、新しい並行プログラミングモデルを開発することです。例えば、MapReduce(Googleから)[GD02]のようなシステムでは、プログラマはロックなしで特定のタイプの並列計算を記述することができます。ロックはその性質上問題があります。おそらく、私たちが本当に必要でない限り、それらを使用しないようにするべきです。

## 参考文献
[B+87] “Concurrency Control and Recovery in Database Systems”  
Philip A. Bernstein, Vassos Hadzilacos, Nathan Goodman  
Addison-Wesley, 1987  
The classic text on concurrency in database management systems. As you can tell, understanding concurrency, deadlock, and other topics in the world of databases is a world unto itself. Study it and find out for yourself.

[C+71] “System Deadlocks”  
E.G. Coffman, M.J. Elphick, A. Shoshani  
ACM Computing Surveys, 3:2, June 1971  
The classic paper outlining the conditions for deadlock and how you might go about dealing with it. There are certainly some earlier papers on this topic; see the references within this paper for details.

[D64] “Een algorithme ter voorkoming van de dodelijke omarming”  
Edsger Dijkstra  
Circulated privately, around 1964  
Available: http://www.cs.utexas.edu/users/EWD/ewd01xx/EWD108.PDF  
Indeed, not only did Dijkstra come up with a number of solutions to the deadlock problem, he was the first to note its existence, at least in written form. However, he called it the “deadly embrace”, which (thankfully) did not catch on.

[GD02] “MapReduce: Simplified Data Processing on Large Clusters”  
Sanjay Ghemawhat and Jeff Dean  
OSDI ’04, San Francisco, CA, October 2004  
The MapReduce paper ushered in the era of large-scale data processing, and proposes a framework for performing such computations on clusters of generally unreliable machines.

[H01] “A Pragmatic Implementation of Non-blocking Linked-lists”  
Tim Harris  
International Conference on Distributed Computing (DISC), 2001  
A relatively modern example of the difficulties of building something as simple as a concurrent linked list without locks.

[H91] “Wait-free Synchronization”  
Maurice Herlihy  
ACM TOPLAS, 13:1, January 1991  
Herlihy’s work pioneers the ideas behind wait-free approaches to writing concurrent programs. These approaches tend to be complex and hard, often more difficult than using locks correctly, probably limiting their success in the real world.

[H93] “A Methodology for Implementing Highly Concurrent Data Objects”  
Maurice Herlihy  
ACM TOPLAS, 15:5, November 1993  
A nice overview of lock-free and wait-free structures. Both approaches eschew locks, but wait-free approaches are harder to realize, as they try to ensure than any operation on a concurrent structure will terminate in a finite number of steps (e.g., no unbounded looping).

[J+08] “Deadlock Immunity: Enabling Systems To Defend Against Deadlocks”  
Horatiu Jula, Daniel Tralamazza, Cristian Zamfir, George Candea  
OSDI ’08, San Diego, CA, December 2008  
An excellent recent paper on deadlocks and how to avoid getting caught in the same ones over and over again in a particular system.

[K81] “Soul of a New Machine”  
Tracy Kidder, 1980  
A must-read for any systems builder or engineer, detailing the early days of how a team inside Data General (DG), led by Tom West, worked to produce a “new machine.” Kidder’s other books are also excellent, including Mountains beyond Mountains. Or maybe you don’t agree with us, comma?

[K87] “Deadlock Detection in Distributed Databases”  
Edgar Knapp  
ACM Computing Surveys, 19:4, December 1987  
An excellent overview of deadlock detection in distributed database systems. Also points to a number of other related works, and thus is a good place to start your reading.

[L+08] “Learning from Mistakes — A Comprehensive Study on Real World Concurrency Bug Characteristics”  
Shan Lu, Soyeon Park, Eunsoo Seo, Yuanyuan Zhou  
ASPLOS ’08, March 2008, Seattle, Washington  
The first in-depth study of concurrency bugs in real software, and the basis for this chapter. Look at Y.Y. Zhou’s or Shan Lu’s web pages for many more interesting papers on bugs.

[T+94] “Linux File Memory Map Code”  
Linus Torvalds and many others  
Available: http://lxr.free-electrons.com/source/mm/filemap.c  
Thanks to Michael Walfish (NYU) for pointing out this precious example. The real world, as you can see in this file, can be a bit more complex than the simple clarity found in textbooks...

\newpage

# 33 Event-based Concurrency (Advanced)
これまでは、並列アプリケーションを構築する唯一の方法がスレッドを使用する方法であるかのように、並行性について書いてきました。人生の多くの事のように、これは完全に真実ではありません。具体的には、GUIベースのアプリケーション[O96]といくつかのタイプのインターネットサーバー[PDZ99]の両方で、異なるスタイルの並行プログラミングがよく使用されます。イベントベースの並行処理と呼ばれるこのスタイルは、node.js [N13]などのサーバー側のフレームワークを含む現代的なシステムでは一般的になっていますが、その根本は以下で説明するC/UNIXシステムにあります。

イベントベースの並行処理が解決する問題は2倍です。1つ目は、マルチスレッドアプリケーションでの同時実行性を正しく管理することが難しいことです。私たちが議論したように、ロックの不足、デッドロック、および他の厄介な問題が発生する可能性があります。2つ目は、マルチスレッドアプリケーションでは、開発者は特定の瞬間にスケジュールされているものをほとんど、またはまったく制御できないことです。むしろプログラマはスレッドを作成し、基盤となるOSが利用可能なCPUを介して合理的な方法でそれらをスケジュールすることを期待しています。すべての仕事量ですべてのケースでうまく動作する汎用スケジューラを構築することが難しい場合、OSは最適でない方法で作業をスケジュールすることがあります。

>> THE CRUX: HOW TO BUILD CONCURRENT SERVERS WITHOUT THREADS  
>> スレッドを使用せずに並行サーバーを構築することで、並行性の制御を維持するだけでなく、マルチスレッド・アプリケーションに悩まされている問題のいくつかを回避できますか？

## 33.1 The Basic Idea: An Event Loop
前述のように、基本的なアプローチは、イベントベースの並行処理と呼ばれます。このアプローチは非常に簡単です。何か(つまり、「イベント」)が発生するのを待つだけです。その場合、それはどのタイプのイベントであるかをチェックし、必要な少量の作業(I/O要求の発行、または将来の処理のための他のイベントのスケジューリングなど)を行います。それで終了です！

詳細に入る前に、最初に正規のイベントベースのサーバーの外観を調べてみましょう。そのようなアプリケーションは、イベントループとして知られる簡単な構成に基づいています。イベントループの擬似コードは次のようになります。
```c
while (1) {
    events = getEvents();
    for (e in events)
        processEvent(e);
}
```
それは本当に簡単です。メインループは(上記のコードで`getEvents()`を呼び出すことによって)何かを待ってから、返されたイベントごとに一度に1つずつ処理します。各イベントを処理するコードをイベントハンドラと呼びます。重要なことに、ハンドラがイベントを処理するとき、それはシステム内で行われる唯一のアクティビティです。したがって、次に処理するイベントを決定することは、スケジューリングと同じです。スケジューリングに対するこの明示的な制御は、イベントベースのアプローチの基本的な利点の1つです。

しかし、この議論から、より大きな疑問が浮かび上がってきます。イベントベースのサーバーは、特にネットワークやディスクI/Oに関してどのようなイベントが起こっているかを正確に判断しますか？具体的には、イベント・サーバーは、メッセージが到着したかどうかをどのように伝えることができますか？

## 33.2 An Important API: `select()` (or `poll()`)
その基本的なイベントループを念頭に置いて、次にイベントを受け取る方法の問題に取り組まなければなりません。ほとんどのシステムでは、`select()`または`poll()`システムコールのいずれかを使用して基本APIを使用できます。これらのインターフェイスがプログラムに行うことができることは簡単です。着信するI/Oがあるかどうかを確認します。たとえば、ネットワークアプリケーション(Webサーバーなど)が、サービスを提供するために、ネットワークパケットが到着したかどうかを確認したいと考えているとします。

これらのシステムコールは、あなたがそうすることを可能にします。たとえば`select()`を実行します。マニュアルページ(Mac版)では、このようにAPIについて説明しています。
```c
int select(int nfds,
fd_set *restrict readfds,
fd_set *restrict writefds,
fd_set *restrict errorfds,
struct timeval *restrict timeout);
```
マニュアルページの実際の記述：`select()`は、readfds、writefds、およびerrorfdsに渡されたアドレスを持つI/Oディスクリプタセットを調べて、ディスクリプタの一部が読み込み準備ができているか、書き込み準備ができているか、状態はそれぞれ保留中かです。最初のnfdsディスクリプタは、各組においてチェックされ、すなわち、ディスクリプタ集合内の0からnfds-1までのディスクリプタが検査されます。戻り時に、`select()`は、指定されたディスクリプタセットを、要求された操作の準備ができているディスクリプタで構成されるサブセットに置き換えます。`select()`は、すべてのセットの準備完了ディスクリプタの合計数を返します。

>> ASIDE: BLOCKING VS. NON-BLOCKING INTERFACES  
>> ブロッキング(または同期)インターフェイスは、呼び出し元に戻る前にすべての作業を行います。ノンブロッキング(または非同期)インターフェイスはいくつかの作業を開始しますが、直ちに戻るため、実行する必要がある作業はすべてバックグラウンドで完了します。  
コールをブロックする際の通常の原因は、ある種のI/Oです。たとえば、完了するためにコールをディスクから読み取らなければならない場合、コールはブロックされ、返されるディスクに送信されたI/O要求を待機します。  
ノンブロッキングインターフェースは、どのようなスタイルのプログラミング(スレッドなど)でも使用できますが、ブロックがすべての進捗を停止させるコールとして、イベントベースのアプローチでは不可欠です。  

`select()`に関する2つの点があります。まず、ディスクリプタの読み込みと書き込みの可否をチェックできることに注意してください。前者は、新しいパケットが到着し、処理が必要であるとサーバに判断させるが、後者は、応答がOKであるとき(すなわち、送信キューが満杯でないとき)にサービスに知らせます。

次に、タイムアウト引数です。ここでの一般的な使用法の1つは、タイムアウトをNULLに設定することです。これにより、`select()`が何らかの記述子が準備できるまで無期限にブロックされます。ただし、より堅牢なサーバーは通常、何らかの種類のタイムアウトを指定します。1つの一般的な手法は、タイムアウトをゼロに設定し、`select()`への呼び出しを使用してすぐに戻ることです。

`poll()`システムコールはかなり似ています。詳細については、マニュアルページ、またはStevens and Rago [SR05]を参照してください。いずれにしても、これらの基本的なプリミティブは、ノンブロッキングイベントループを構築する方法を提供します。単に、着信パケットをチェックし、メッセージを含むソケットから読み込み、必要に応じて応答します。

## 33.3 Using `select()`
これをより具体的にするために、`select()`を使ってどのネットワークディスクリプタにメッセージが入ってくるかを調べる方法を調べてみましょう。図33.1に簡単な例を示します。

![](../33/img/fig33_1.PNG)

このコードは、実際にはかなり理解しやすいです。初期化の後、サーバーは無限ループに入ります。ループ内では、まず`FD_ZERO()`マクロを使用してファイル記述子のセットをクリアした後、`FD_SET()`を使用して、minFDからmaxFDまでのすべてのファイル記述子をセットに含めます。この記述子の集合は、例えば、サーバが注目しているすべてのネットワークソケットを表すことができます。最後に、サーバーは`select()`を呼び出して、どの接続がデータを利用できるかを調べます。その後、ループ内で`FD_ISSET()`を使用することによって、イベント・サーバーは、どのデータ記述子がデータを準備していて、入ってくるデータを処理するかを見ることができます。

もちろん、実際のサーバーはこれよりも複雑であり、メッセージの送信、ディスクI/Oの発行、およびその他の多くの詳細でロジックを使用する必要があります。詳細については、API情報についてはStevens and Rago [SR05]、またはPai et。 alまたはWelsh et al。イベントベースのサーバー[PDZ99、WCB01]の一般的な流れの概要については、これらをご覧ください。

## 33.4 Why Simpler? No Locks Needed
単一のCPUとイベントベースのアプリケーションでは、並行プログラムで見つかった問題はもう存在しません。具体的には、一度に1つのイベントしか処理されないため、ロックを取得または解放する必要はありません。イベントベースのサーバは、明らかにシングルスレッドであるため、別のスレッドによって中断することはできません。したがって、スレッド化されたプログラムに共通する並行性のバグは、基本的なイベントベースのアプローチでは現れません。

>> TIP: DON’T BLOCK IN EVENT-BASED SERVERS  
>> イベントベースのサーバーを使用すると、タスクのスケジューリングをきめ細かく制御できます。ただし、このような制御を維持するために、呼び出し元の実行をブロックする呼び出しはどんなときでも作成できません。このデザインのヒントに従わないと、イベントベースのサーバーがブロックされ、クライアントが欲求不満になり、この本を読んだかどうかについて深刻な疑問が生じます。

## 33.5 A Problem: Blocking System Calls
これまでのところ、イベントベースのプログラミングは素晴らしく聞こえるでしょうか？単純なループをプログラムし、発生したイベントを処理します。ロックについて考える必要はありません！しかし、問題があります。イベントがブロックする可能性のあるシステムコールを発行する必要がある場合はどうなりますか？

例えば、要求がクライアントからサーバに来て、ディスクからファイルを読み込み、その内容を要求元のクライアントに返す(単純なHTTP要求と同じように)と想像してください。そのような要求を処理するために、最終的にファイルを開くために`open()`システムコールを発行し、続いてファイルを読むために一連の`read()`を呼び出す必要があります。ファイルがメモリに読み込まれると、サーバーはおそらく結果をクライアントに送信し始めます。

`open()`と`read()`の両方の呼び出しでストレージシステムにI/O要求が発行されます(必要なメタデータやデータがすでにメモリにない場合)。スレッドベースのサーバーでは、これは問題ではありません。I/O要求を発行しているスレッドが中断している間(I/Oが完了するのを待っている間)、他のスレッドが実行できるため、サーバーは処理を進めることができます。実際、このようなI/Oと他の計算の自然なオーバーラップは、スレッドベースのプログラミングを非常に自然かつ簡単にするものです。

しかし、イベントベースのアプローチでは、実行する他のスレッドはありません。メインイベントループだけです。そして、これは、イベントハンドラがブロックする呼び出しを発行した場合、サーバー全体がその処理を行うことを意味します。呼び出しが完了するまでブロックします。イベントループがブロックされると、システムはアイドル状態になり、リソースを浪費する可能性があります。したがって、イベントベースのシステムでは、ブロッキングコールは許可されていないため、ルールに従わなければなりません。

## 33.6 A Solution: Asynchronous I/O
この制限を克服するために、多くの最新のオペレーティングシステムでは、一般に非同期I/Oと呼ばれるディスクシステムにI/O要求を発行する新しい方法が導入されています。これらのインタフェースを使用すると、I/Oが完了する前にアプリケーションがI/O要求を発行し、直ちに呼び出し元に制御を戻すことができます。追加のインターフェースにより、アプリケーションは様々なI/Oが完了したかどうかを判断することができます。

たとえば、Macで提供されているインターフェース(他のシステムにも同様のAPIがあります)を調べてみましょう。APIは、共通の用語で基本構造、構造体aiocbまたはAIO制御ブロックを中心に展開されています。構造の簡略化されたバージョンは以下のようになります(詳細は、マニュアルページを参照してください)。


![](../33/img/fig33_1_1.PNG)

非同期読み取りをファイルに発行するには、アプリケーションは最初にこの構造体に関連する情報を書き込む必要があります。読み込むファイルのファイル記述子(aio_fildes)、ファイル内のオフセット(aio_offset)、およびファイルの長さの要求(aio_nbytes)、最後に、読み取り結果をコピーする対象のメモリ位置(aio_buf)を指定します。

この構造体が埋め込まれた後、アプリケーションは非同期呼び出しを発行してファイルを読み取る必要があります。Macでは、このAPIは単に非同期読み取りAPIです。
```c
int aio_read(struct aiocb *aiocbp);
```
このコールはI/Oを発行しようとします。成功した場合はすぐに戻り、アプリケーション(つまり、イベントベースのサーバー)はその作業を続行できます。  
しかし、解決しなければならないパズルの最後の部分が1つあります。I/Oが完了し、バッファ(aio_bufが指す)が要求されたデータをその中に持つようになったら、どうすればわかりますか？1つの最後のAPIが必要です。Macでは、`aio_error()`と呼ばれます(やや混乱します)。APIは次のようになります。
```c
int aio_error(const struct aiocb *aiocbp);
```
このシステムコールは、aiocbpによって参照された要求が完了したかどうかをチェックします。存在する場合、ルーチンは成功を返します(ゼロで示されます)。そうでなければ、EINPROGRESSが返されます。したがって、すべての未処理の非同期I/Oに対して、アプリケーションは`aio_error()`の呼び出しを介してシステムを定期的にポーリングして、前記I/Oがまだ完了しているかどうかを判断できます。あなたが気付いたことの1つは、I/Oが完了したかどうかを確認することは痛いということです。特定の時点で数十回または数百回のI/Oが発行された場合、各プログラムを繰り返しチェックするか、最初に少し待つか、または...？

この問題を解決するために、一部のシステムでは割り込みに基づく手法が提供されています。この方法では、非同期I/Oが完了したときにUNIX信号を使用してアプリケーションに通知するので、システムに繰り返し尋ねる必要がなくなります。このポーリングと割り込みの問題は、I/Oデバイスの章で見られるように(または既に見ているように)デバイスでも見られます。

>> ASIDE: UNIX SIGNALS  
>> 近代的なUNIXのすべての変種には、シグナルと呼ばれる巨大で魅力的なインフラが存在します。シグナルは最も単純な方法で、プロセスと通信する手段を提供します。具体的には、信号をアプリケーションに配信することができます。そうすることにより、シグナルハンドラ、すなわち、その信号を処理するためのアプリケーション内のいくつかのコードを実行するために、アプリケーションが何をしているのかを停止させます。終了すると、プロセスは以前の動作を再開します。  
各信号には、HUP(ハングアップ)、INT(割り込み)、SEGV(セグメンテーション違反)などの名前があります。詳細については、マニュアルページを参照してください。興味深いことに、時にはシグナルを行うのはカーネル自体です。例えば、あなたのプログラムがセグメンテーション違反に遭遇すると、OSはそれにSIGSEGVを送ります(シグナル名の前にSIGが付いていることが一般的です)。あなたのプログラムがそのシグナルを捕捉するように設定されていれば、この誤ったプログラムの振る舞い(デバッグに役立つ可能性がある)に応答して実際にコードを実行することができます。シグナルを処理するように構成されていないプロセスにシグナルが送信されると、いくつかのデフォルト動作が成立します。SEGVの場合、プロセスは強制終了されます。無限ループに入るシンプルなプログラムですが、最初にSIGHUPを捕捉するシグナルハンドラを設定しています：  
![](../33/img/fig33_1_2.PNG)  
killコマンドラインツールでシグナルを送ることができます(これは奇妙で攻撃的な名前です)。そうすることで、プログラム中のmain whileループが中断され、ハンドラコード`handle()`が実行されます。  
![](../33/img/fig33_1_3.PNG)  
シグナルについて学ぶにはさらに多くのことがありますので、単一のページではなく、1つの章で十分ではありません。いつものように、StevensとRagoという素晴らしい出典があります[SR05]。興味があればもっと読んでください。

非同期I/Oのないシステムでは、純粋なイベントベースのアプローチは実装できません。しかし、賢明な研究者は、その場所でかなりうまく機能する方法を導いてきました。例えば、Pai et al。[PDZ99]は、ネットワークパケットを処理するためにイベントが使用され、未処理のI/Oを管理するためにスレッドプールが使用されるハイブリッドアプローチを説明しています。詳細は論文を参照してください。

## 33.7 Another Problem: State Management
イベントベースのアプローチのもう1つの問題は、そのようなコードは、一般に従来のスレッドベースのコードよりも書くのがより複雑であるということです。理由は次のとおりです。イベントハンドラが非同期I/Oを発行すると、I/Oが最後に完了したときに使用する次のイベントハンドラのプログラム状態をパッケージ化する必要があります。スレッドベースの場合は。プログラムが必要とする状態がスレッドのスタック上にあるので、この追加の作業は必要ありません。Adya et al。これは、この作業マニュアルのスタック管理と呼ばれ、イベントベースのプログラミング[A+02]の基本です。

この点をより具体的にするために、スレッドベースのサーバがファイルディスクリプタ(fd)から読み込み、完了したらファイルから読み込んだデータをネットワークソケットディスクリプタ(sd)に書き込むという単純な例を見てみましょう。エラーチェックを無視したコードは次のようになります。
```c
int rc = read(fd, buffer, size);
rc = write(sd, buffer, size);
```
ご覧のように、マルチスレッドプログラムでは、この種類の作業を行うのは簡単です。最終的に`read()`が返ってくると、コードはその情報がスレッドのスタック(変数sd内)にあるため、書き込むソケットをすぐに知ることができます。  
イベントベースのシステムでは、それほど簡単ではありません。同じタスクを実行するには、先に説明したAIO呼び出しを使用して、まず非同期に読み取りを発行します。次に、`aio_error()`呼び出しを使用して読み取りの完了を定期的に確認するとします。その呼び出しによって読み取りが完了したことが通知されると、イベントベースのサーバーはどのように処理すべきかを知っていますか？

この解決策は、Adya et alに詳細が書かれています。[A + 02]は、継続[FHK84]と呼ばれる古いプログラミング言語の構造を使用することです。複雑に思えますが、アイデアは単純です。基本的に、このイベントの処理を終了するために必要な情報をいくつかのデータ構造に記録します。イベントが発生したとき(すなわち、ディスクI/Oが完了したとき)に、必要な情報を調べてイベントを処理します。この特定の場合、解決策は、ソケットディスクリプタ(sd)を、ファイルディスクリプタ(fd)によってインデックス付けされた何らかの種類のデータ構造(例えば、ハッシュテーブル)に記録することです。ディスクI/Oが完了すると、イベントハンドラはファイルディスクリプタを使用して継続を検索し、ソケットディスクリプタの値を呼び出し側に返します。この時点で(最後に)、サーバーはデータをソケットに書き込むために最後の作業を行うことができます。

## 33.8 What Is Still Difficult With Events
私たちが言及すべきイベントベースのアプローチには他にもいくつかの困難があります。たとえば、システムが単一のCPUから複数のCPUに移行したとき、イベントベースのアプローチの単純さのいくつかは消えました。具体的には、複数のCPUを使用するには、イベント・サーバーが複数のイベント・ハンドラーを並行して実行する必要があります。そうするときには、通常の同期問題(例えば、クリティカルセクション)が発生し、通常の解決法(例えば、ロック)が採用されなければならない。したがって、現代のマルチコアシステムでは、ロックなしの簡単なイベント処理はもはや不可能です。

イベントベースのアプローチのもう1つの問題は、ページングなどの特定の種類のシステムアクティビティとうまく統合できないことです。たとえば、イベント・ハンドラ・ページに障害が発生すると、そのイベント・ハンドラ・ページはブロックされ、ページ・フォルトが完了するまでサーバーは処理を進めません。サーバーが明示的なブロッキングを回避するように構成されていても、ページフォールトによるこの種類の暗黙のブロッキングは回避しにくいため、一般的な場合に大きなパフォーマンス上の問題が発生する可能性があります。

第3の問題は、さまざまなルーチンの正確なセマンティクスが変更されるため、イベントベースのコードを時間外管理するのが難しいことです[A + 02]。たとえば、ルーチンが非ブロッキングからブロッキングに変更された場合、そのルーチンを呼び出すイベントハンドラも、2つの部分に分けることによって、その新しい性質に適応するように変更する必要があります。ブロッキングはイベントベースのサーバーにとって非常に悲惨であるため、プログラマーは、各イベントが使用するAPIのセマンティクスにおけるそのような変更を常に把握している必要があります。

最後に、ほとんどのプラットフォームで非同期ディスクI/Oが可能になりましたが、そこに到達するまでには長い時間がかかりましたが、シンプルで均一な方法で非同期ネットワークI/Oと決して統合することは決してありません。たとえば、`select()`インタフェースを使用してすべての未処理I/Oを管理するのは簡単ですが、通常はネットワーキング用の`select()`とディスクI/OのAIO呼び出しの組み合わせが必要です。

## 33.9 Summary
私たちは、イベントに基づいて異なるスタイルの同時実行性について骨組みを紹介しました。イベントベースのサーバは、アプリケーション自体へのスケジューリングを制御するが、現代のシステム(例えば、ページング)の他の側面との統合の複雑さおよび困難性をいくらかのコストを払うことで実行しています。これらの課題のために、単一のアプローチが最良のものとして浮上したわけではありません。したがって、スレッドとイベントの両方は、今後数年間、同じ同時実行性問題に対する2つの異なるアプローチとして存続する可能性があります。いくつかの研究論文(例：[A + 02、PDZ99、vB + 03、WCB01])を読んだり、イベントベースのコードを書いて詳細を調べることができます。

## 参考文献
[A+02] “Cooperative Task Management Without Manual Stack Management”  
Atul Adya, Jon Howell, Marvin Theimer, William J. Bolosky, John R. Douceur  
USENIX ATC ’02, Monterey, CA, June 2002  
This gem of a paper is the first to clearly articulate some of the difficulties of event-based concurrency, and suggests some simple solutions, as well explores the even crazier idea of combining the two types of concurrency management into a single application!  

[FHK84] “Programming With Continuations”  
Daniel P. Friedman, Christopher T. Haynes, Eugene E. Kohlbecker  
In Program Transformation and Programming Environments, Springer Verlag, 1984  
The classic reference to this old idea from the world of programming languages. Now increasingly popular in some modern languages.

[N13] “Node.js Documentation”  
By the folks who build node.js  
Available: http://nodejs.org/api/  
One of the many cool new frameworks that help you readily build web services and applications. Every modern systems hacker should be proficient in frameworks such as this one (and likely, more than one). Spend the time and do some development in one of these worlds and become an expert.

[O96] “Why Threads Are A Bad Idea (for most purposes)”  
John Ousterhout  
Invited Talk at USENIX ’96, San Diego, CA, January 1996  
A great talk about how threads aren’t a great match for GUI-based applications (but the ideas are more general). Ousterhout formed many of these opinions while he was developing Tcl/Tk, a cool scripting language and toolkit that made it 100x easier to develop GUI-based applications than the state of the art at the time. While the Tk GUI toolkit lives on (in Python for example), Tcl seems to be slowly dying (unfortunately).

[PDZ99] “Flash: An Efficient and Portable Web Server”  
Vivek S. Pai, Peter Druschel, Willy Zwaenepoel  
USENIX ’99, Monterey, CA, June 1999  
A pioneering paper on how to structure web servers in the then-burgeoning Internet era. Read it to understand the basics as well as to see the authors’ ideas on how to build hybrids when support for asynchronous I/O is lacking.

[SR05] “Advanced Programming in the UNIX Environment”  
W. Richard Stevens and Stephen A. Rago  
Addison-Wesley, 2005  
Once again, we refer to the classic must-have-on-your-bookshelf book of UNIX systems programming. If there is some detail you need to know, it is in here.

[vB+03] “Capriccio: Scalable Threads for Internet Services”  
Rob von Behren, Jeremy Condit, Feng Zhou, George C. Necula, Eric Brewer  
SOSP ’03, Lake George, New York, October 2003  
A paper about how to make threads work at extreme scale; a counter to all the event-based work ongoing at the time.

[WCB01] “SEDA: An Architecture for Well-Conditioned, Scalable Internet Services”  
Matt Welsh, David Culler, and Eric Brewer  
SOSP ’01, Banff, Canada, October 2001  
A nice twist on event-based serving that combines threads, queues, and event-based hanlding into one streamlined whole. Some of these ideas have found their way into the infrastructures of companies such as Google, Amazon, and elsewhere.

\newpage

\part{Persistence}
# 36 I/O Devices
本書のこの部分(永続性)の主な内容を掘り下げる前に、入出力(I/O)デバイスの概念を紹介し、オペレーティングシステムがそのようなエンティティとどのように対話するかを示します。もちろん、I/Oはコンピュータシステムにとって非常に重要です。入力なしのプログラムを想像してください(毎回同じ結果を出します)。今は出力のないプログラムを想像してください(何が目的なのですか？)。明らかに、コンピュータシステムが面白いためには、入力と出力の両方が必要です。

>> CRUX: HOW TO INTEGRATE I/O INTO SYSTEMS  
>> I/Oをシステムにどのように組み込むべきですか？一般的な仕組みは何ですか？どのように効率的にすることができますか？

## 36.1 System Architecture
議論を始めるには、典型的なシステムの構造を見てみましょう(図36.1)。画像は、ある種のメモリバスまたは相互接続を介してシステムのメインメモリに接続された単一のCPUを示しています。デバイスの中には、一般的なI/Oバスを介してシステムに接続されているものがあります。多くの現代システムでは、PCI(またはその多くの派生品)の1つです。グラフィックスやその他の高性能I/Oデバイスがここにあります。最後に、さらに低いものは、SCSI、SATA、またはUSBなどの周辺バスと呼ばれるものの1つ以上です。これらは、ディスク、マウス、およびその他の同様のコンポーネントを含む、最も遅いデバイスをシステムに接続します。

![](../36/img/fig36_1.PNG)

あなたが求めるかもしれない1つの質問は、なぜこのような階層構造が必要なのかということです。単純に言えば、物理学とコストです。バスが速ければ速いほど短くなければなりません。したがって、高性能メモリバスは、デバイスなどをプラグインする余地があまりありません。さらに、高性能のためにバスを設計することはかなりコストがかかります。したがって、システム設計者は、グラフィックカードなどの高性能を要求するコンポーネントがCPUに近いほど、この階層的アプローチを採用しています。性能の低いコンポーネントはさらに遠くにあります。周辺バスにディスクやその他の低速デバイスを配置する利点は多岐にわたっています。特に、多数のデバイスを配置することができます。

## 36.2 A Canonical Device
標準のデバイス(実際のものではない)を見て、このデバイスを使用してデバイスの相互作用を効率的にするために必要な機械のいくつかの理解を促進しましょう。図36.2から、デバイスには2つの重要なコンポーネントがあることがわかります。1つ目は、システムの残りの部分に提示するハードウェアインターフェイスです。ソフトウェアのように、ハードウェアは、システムソフトウェアがその動作を制御することを可能にする何らかの種類のインタフェースも提示しなければいけません。したがって、すべてのデバイスは、典型的な相互作用のための特定のインタフェースおよびプロトコルがあります。

任意のデバイスの第2の部分は、その内部構造です。デバイスのこの部分は実装固有のものであり、デバイスがシステムに提示する抽象化を実装する責任があります。非常にシンプルなデバイスは、その機能を実装するために1つまたはいくつかのハードウェアチップを持ちます。より複雑なデバイスには、シンプルなCPU、汎用メモリ、および他のデバイス固有のチップが含まれ、仕事を得て、おわらせます。例えば、最新のRAIDコントローラは、その機能を実現するために、数十万行のファームウェア(すなわち、ハードウェアデバイス内のソフトウェア)から構成されています。

![](../36/img/fig36_2.PNG)

## 36.3 The Canonical Protocol
上記の図では、(簡略化された)デバイスインタフェースは3つのレジスタで構成されています。ステータスレジスタは、デバイスの現在の状態を見るために読み出すことができます。コマンドレジスタは特定のタスクを実行するようにデバイスに指示します。データレジスタはデバイスにデータを渡すか、またはデバイスからデータを取得します。これらのレジスタを読み書きすることにより、オペレーティングシステムはデバイスの動作を制御できます。ここでは、デバイスがそのために何かを行うために、OSとデバイスが持つ可能性がある典型的な相互作用について説明します。プロトコルは次のとおりです。  
![](../36/img/fig36_2_1.PNG)  
プロトコルには4つのステップがあります。最初に、OSは、デバイスがステータスレジスタを繰り返し読み出すことによってコマンドを受信する準備ができるまで待機します。このポーリングをデバイスと呼びます(基本的に、何が起こっているのかを尋ねるだけです)。第2に、OSはいくつかのデータをデータレジスタに送る。これがディスクの場合、たとえばディスクブロック(たとえば4KB)をデバイスに転送するために複数の書き込みが行われる必要があると想像することができます。メインCPUが(このプロトコル例のように)データ移動に関与する場合、これをプログラムI/O(PIO)と呼びます。第3に、OSはコマンドレジスタにコマンドを書き込む。そうすることで、暗黙的に、データが存在し、コマンドで作業を開始する必要があることをデバイスに知らせることができます。最後に、OSはデバイスが終了するのを待って、ループでポーリングして終了するかどうかを確認します(成功または失敗を示すエラーコードが表示されることがあります)。

この基本的なプロトコルは、単純で働きやすいという肯定的な側面を持っています。しかし、いくつかの非効率性と不便さがあります。プロトコルで最初に気づく問題は、ポーリングが非効率的であるように見えることです。具体的には、デバイスがそのアクティビティを完了するのを待つだけでCPU時間を大量に無駄する代わりに、別の準備完了プロセスに切り替え(潜在的に遅い)、CPUをより有効に利用しています。

## 36.4 Lowering CPU Overhead With Interrupts
この相互作用を改善するために何年も前に多くのエンジニアが出てきた発明は、すでに見てきたことです。デバイスを繰り返しポーリングする代わりに、OSは要求を発行し、呼び出しプロセスをスリープ状態にし、コンテキストを別のタスクに切り替えることができます。デバイスが最終的に動作を終了すると、ハードウェア割り込みが発生し、CPUが所定の割り込みサービスルーチン(ISR)またはより単純に割り込みハンドラでOSにジャンプします。ハンドラは、要求を終了する(たとえば、データおよびおそらくはデバイスからのエラーコードを読み取る)オペレーティングシステムコードの一部に過ぎず、I/Oを待っているプロセスを復帰させ、必要に応じて処理をすすめることができます。

したがって、割り込みは、計算とI/Oの重複を可能にします。これは、使用率を向上させるための鍵です。このタイムラインは問題を示しています  
![](../36/img/fig36_2_2.PNG)  
この図では、プロセス1はCPUで一定時間実行され(CPU行で1が繰り返されます)、ディスクにI/O要求を発行してデータを読み取ります。割り込みがなければ、システムは単に回転し、I/Oが完了するまで(pで示される)、デバイスの状態を繰り返しポーリングします。ディスクは要求を処理し、最後にプロセス1を再度実行できます。代わりに、割り込みを利用して重複を許可すると、OSはディスクを待っている間に何か他のことをすることができます。  
![](../36/img/fig36_2_3.PNG)  
この例では、ディスクサービスプロセス1の要求中に、OSはCPU上でプロセス2を実行します。ディスク要求が終了すると、割り込みが発生し、OSはプロセス1を起動して再度実行します。したがって、CPUとディスクの両方は、中程度の時間に適切に利用されます。

割り込みを使用することは常に最適な解決方法ではないことに注意してください。たとえば、タスクを非常に迅速に実行するデバイスを想像してください。最初のポーリングでは通常、タスクを実行するデバイスが見つけられます。この場合に割り込みを使用すると、実際にはシステムの速度が遅くなります。別のプロセスに切り替え、割り込みを処理し、発行プロセスに戻すのはコストがかかります。したがって、デバイスが高速であれば、ポーリングするのが最善の方法です。遅い場合、オーバーラップを許す割り込みが最適です。デバイスの速度がわかっていないか、時には高速で時には遅い場合は、しばらくポーリングするハイブリッドを使用し、デバイスがまだ終了していない場合は割り込みを使用するのが最善の方法です。この2段階アプローチは、両方の世界のベストを達成するかもしれません。

>> TIP: INTERRUPTS NOT ALWAYS BETTER THAN PIO  
>> 割り込みは計算とI/Oのオーバーラップを可能にしますが、遅いデバイスには本当に意味があります。それ以外の場合は、割込み処理とコンテキスト切り替えのコストが割込みの利点を上回る可能性があります。また、割り込みfloodがシステムに過負荷をかけ、それをライブロックに導く場合もあります[MR96]。そのような場合、ポーリングはOSのスケジューリングにおいてより多くの制御を提供し、したがって再び有用です。

割り込みを使用しない別の理由は、ネットワーク[MR96]で発生します。膨大なストリームの着信パケットがそれぞれ割り込みを生成すると、OSはライブロックすることができます。つまり、割り込み処理のみを実行し、ユーザーレベルのプロセスを実行して実際に要求を処理することはできません。たとえば、スラッシュドット効果のために突然負荷が高くなるWebサーバーを想像してください。この場合、ポーリングを使用して、システムで発生していることをよりうまく制御し、Webサーバーがいくつかの要求にサービスを提供してから、デバイスに戻ってより多くのパケット到着を確認することをお勧めします。

別の割り込みベースの最適化が統合されています。このような設定では、割り込みを発生させる必要のあるデバイスは、最初にCPUに割り込みを送信する前に少し待っています。待機している間に、他の要求が間もなく完了し、複数の割り込みを1つの割り込みデリバリにまとめて割り込み処理のオーバーヘッドを減らすことができます。もちろん、待ち時間が長すぎると、システムの一般的なトレードオフであるリクエストの待ち時間が長くなります。Ahmad et al。 [A + 11]は素晴らしい要約です。

## 36.5 More Efficient Data Movement With DMA
残念ながら、私たちの注意を必要とする標準プロトコルのもう一つの側面があります。特に、プログラムされたI/O(PIO)を使用して大量のデータをデバイスに転送する場合、CPUはもう少し過酷な作業で再び過負荷になり、実行に多くの時間と労力を費やし、他のプロセスの実行にもっと費やすことができる努力を無駄にします。このタイムラインは問題を示しています  
![](../36/img/fig36_2_4.PNG)  
タイムラインでは、プロセス1が実行されていて、ディスクにデータを書きたいと考えています。その後、I/Oを開始します。このI/Oは、一度に1ワードずつメモリからデバイスに明示的にデータをコピーする必要があります(図のc)。コピーが完了すると、I/Oはディスク上で開始され、CPUは最終的に他の用途に使用されます。

>> THE CRUX: HOW TO LOWER PIO OVERHEADS  
>> PIOを使用すると、CPUは手作業でデバイス間でデータを移動するのに時間がかかりすぎます。どのようにしてこの作業をオフロードし、CPUをより効果的に利用できるようにするか？

この問題に対する解決策は、ダイレクトメモリアクセス(DMA)と呼ばれるものです。DMAエンジンは基本的にシステム内の非常に特殊なデバイスであり、多くのCPUの介入なしにデバイスとメインメモリ間の転送を調整することができます。

DMAは次のように動作します。たとえば、デバイスにデータを転送するには、OSはメモリにデータがどこにあるのか、どのくらいのデータをコピーするのか、どのデバイスにデータを送るのかを伝えることでDMAエンジンをプログラムします。その時点で、OSは転送を終え、他の作業を進めることができます。DMAが完了すると、DMAコントローラは割り込みを発生させ、したがってOSは転送が完了したことを知ります。改訂されたタイムラインでは以下のようになります。  
![](../36/img/fig36_2_5.PNG)  
タイムラインから、データのコピーがDMAコントローラによって処理されることがわかります。その間にCPUはフリーであるため、OSはプロセス2を実行することを選択して別の処理を行うことができます。プロセス1はプロセス1が再び実行される前に、より多くのCPUを使用します。

## 36.6 Methods Of Device Interaction
I/Oを実行する際の効率の問題があるので、デバイスを最新のシステムに組み込むために処理する必要があるいくつかの問題があります。これまでに気がついた問題の1つは、OSが実際にデバイスとどのように通信しているかについては何も言いませんでした。

>> THE CRUX: HOW TO COMMUNICATE WITH DEVICES  
ハードウェアとデバイスとの通信方法明示的な指示があるべきか？それともそれ以外の方法がありますか？

時間の経過と共に、デバイス通信の2つの主要な方法が開発されています。最初の、(IBMのメインフレームで長年使用されている)最も古い方法は、明示的なI/O命令を持つことです。これらの命令は、OSが特定のデバイスレジスタにデータを送信する方法を指定し、したがって上述のプロトコルの構築を可能にします。

たとえば、x86では、inおよびout命令を使用してデバイスと通信することができます。たとえば、デバイスにデータを送信するには、呼び出し元はデータが入っているレジスタとデバイスに名前を付ける特定のポートを指定します。命令を実行すると、目的の動作になります。

そのような指示は通常特権が与えられます。OSはデバイスを制御するため、OSだけは直接通信できるエンティティとして許可されています。プログラムがディスクを読み書きできるかどうかを想像してみましょう。例えば、すべてのカオスはいつものように、どんなユーザプログラムもそのような抜け穴を使ってマシンを完全に制御することができます。

デバイスと対話する第2の方法は、メモリマップI/Oと呼ばれます。このアプローチでは、ハードウェアはデバイスのレジスタをあたかもメモリの場所のように使用可能にします。特定のレジスタにアクセスするために、OSはアドレスをロード(読み込み)または格納(書き込み)します。ハードウェアはメインメモリではなくデバイスにロード/ストアをルーティングします。あるアプローチや他のアプローチに大きな利点はありません。メモリマップされたアプローチは、それをサポートするための新しい命令は必要ないという点で素晴らしいですが、どちらのアプローチも今日でもまだ使用されています。

## 36.7 Fitting Into The OS: The Device Driver
最終的に私たちが議論する問題の1つは、それぞれ固有のインタフェースを持つデバイスをOSに組み込む方法です。できるだけ一般的なものにしたいと考えています。 たとえば、ファイルシステムを考えてみましょう。私たちは、SCSIディスク、IDEディスク、USBキーチェーンドライブなどの上で動作するファイルシステムを構築したいと考えています。また、ファイルシステムがこれらの差分があるタイプのドライブに対してどのように読み書き要求を発行するかの詳細すべてを比較的知らないことを望んでいます。

>> THE CRUX: HOW TO BUILD A DEVICE-NEUTRAL OS  
>> どのようにしてOSのデバイスの中立性を最大限に保ち、主要なOSサブシステムからのデバイスの相互作用の詳細を隠すことができますか？

この問題は、古くからの抽象化技術によって解決されています。最低レベルでは、OSの一部のソフトウェアは、デバイスがどのように動作するかを詳細に把握している必要があります。このソフトウェアはデバイスドライバと呼ばれ、デバイスの相互作用の詳細はすべてカプセル化されています。

この抽象化が、Linuxファイルシステムソフトウェアスタックを調べることによって、OSの設計と実装にどのように役立つかを見てみましょう。図36.3は、Linuxソフトウェアの組織の概略図です。ダイアグラムから分かるように、ファイルシステム(もちろん、上記のアプリケーション)は、使用しているディスククラスの詳細を完全に無視しています。ブロック読み出しおよび書き込み要求を汎用ブロックレイヤーに発行するだけで、適切なデバイスドライバにルーティングされ、特定の要求を発行する詳細が処理されます。簡略化されていますが、この図は、そのような詳細がほとんどのOSからどのように隠れるかを示しています。

![](../36/img/fig36_3.PNG)  

このようなカプセル化は、同様にその欠点を有する可能性があることを覚えておいてください。例えば、多くの特殊な機能を備えているが、残りのカーネルに汎用インターフェースを提示しなければならないデバイスがある場合、それらの特殊機能は使用されなくなります。このような状況は、例えば、SCSIデバイスを搭載したLinuxでは非常に多くのエラー報告があります。他のブロックデバイス(例えば、ATA/IDE)はずっと簡単なエラー処理をもっているので、これまでに受け取ったより高いレベルのソフトウェアの全ては一般的なEIO(一般的なIOエラー)エラーコードです。したがって、SCSIが提供している可能性のあるエラーの詳細は、ファイルシステム(高いレベルのソフトウェア)[G08]では失われます。

興味深いことに、あなたのシステムに接続する可能性のあるデバイスにはデバイスドライバが必要であるため、時間の経過とともにカーネルコードの膨大な割合を占めています。Linuxカーネルの研究では、OSコードの70％以上がデバイスドライバ[C01]に存在することが明らかになりました。Windowsベースのシステムでも、かなり高い可能性で同じくらいあります。したがって、OSに何百万行ものコードが含まれていると言うと、実際にはOSには何百万行ものデバイスドライバコードが含まれています。当然のことながら、任意のインストールに対して、そのコードの大部分はアクティブではないです(すなわち、一度にいくつかのデバイスのみがシステムに接続される)。ドライバーは(フルタイムのカーネル開発者の代わりに)「アマチュア」によって書かれていることが多いため、より多くのバグを持つ傾向があり、カーネルクラッシュの主な原因となります[S03]。

## 36.8 Case Study: A Simple IDE Disk Driver
ここでもう少し詳しく調べるために、実際のデバイスを簡単に見てみましょう。IDEディスクドライブ[L94]です。この参考文献[W10]に記載されているプロトコルを要約します。実際のIDEドライバ[CK + 08]の簡単な例についてはxv6ソースコードを見ていきます。

IDEディスクは、コントロール、コマンドブロック、ステータス、およびエラーの4種類のレジスタで構成されるシンプルなインターフェイスをシステムに提供します。これらのレジスタは、I/O命令の入出力(x86では)を使用して、特定の"I/Oアドレス"(以下の0x3F6など)を読み書きすることで利用できます。

![](../36/img/fig36_4.PNG)

デバイスと相互作用するための基本的なプロトコルは、既に初期化されているものとします。

- ドライブの準備が整うのを待ちます。ドライブがREADYでBUSYになるまでステータスレジスタ(0x1F7)を読み込みます。  
- コマンドレジスタにパラメータを書き込む。アクセスするセクタのセクタ数、論理ブロックアドレス(LBA)、およびコマンド番号(0x1F2-0x1F6)にドライブ番号(IDEが2つのドライブのみを許可するため、マスタ= 0x00またはスレーブ= 0x10)を書き込みます。  
- I/Oを開始します。コマンドレジスタへの読出し/書込みを発行することによって、コマンドレジスタ(0x1F7)にREAD-WRITEコマンドを書き込みます。  
- データ転送(書き込み用)：ドライブステータスがREADYおよびDRQ(データのドライブ要求)になるまで待ちます。データポートにデータを書き込みます。  
- 割り込みを処理します。最も単純なケースでは、転送されるセクタごとに割り込みを処理します。より複雑なアプローチはバッチ処理と転送全体が完了したときに最終的な割り込みを可能にします。  
- エラー処理。各操作の後、ステータスレジスタを読み出します。ERRORビットがオンの場合は、エラー・レジスタを参照してください。

このプロトコルの大部分は、xv6 IDEドライバ(図36.5)にあります。初期化後、4つの主な機能が動作します。最初のものは`ide_rw()`で、要求がキューに入れられているかどうかを確認したり、`ide_start_request()`を使ってディスクに直接発行したりします。どちらの場合も、ルーチンは要求が完了するのを待つため、呼び出しプロセスがスリープ状態になります。2番目は`ide_start_request()`で、要求(ディスクの場合はデータ)をディスクに送信するために使用されます。インおよびアウトx86命令は、それぞれデバイスレジスタを読み書きするために呼び出されます。開始要求ルーチンは、3番目の関数`ide_wait_ready()`を使用して、要求を発行する前にドライブが準備状態になっていることを確認します。最後に、割り込みが発生すると`ide_intr()`が呼び出されます。(要求が書き込みではなく読み込みである場合)デバイスからデータを読み込み、I/Oが完了するのを待つプロセスを起動し、(I/Oキューにさらに要求がある場合)、`ide_start_request()`を介して次のI/Oに移ります。
![](../36/img/fig36_5.PNG)

## 36.9 Historical Notes
終了する前に、これらの基本的なアイデアの起源に関する簡単な歴史的な注記を掲載します。あなたがもっと学ぶことに興味があるなら、Smothermanの優れた要約[S08]を読んでください。

割り込みは古代の考えであり、最も初期のマシンに存在します。例えば、1950年代初期のUNIVACは、この機能が利用可能であったのかどうかは明確ではありませんが、何らかの形の割り込みベクタリングを持っていました[S08]。残念なことに、揺籃期(物事が張ってする初期の段階)であっても、私たちはコンピューティングの歴史の始まりを失い始めています。

DMAの考え方をどのマシンが最初に導入したのかについていくつかの議論があります。例えば、Knuthらは、DYSEAC(その時点でトレーラーで運ばれることができる「モバイル」マシン)を指していますが、他の人はIBM SAGEが最初の[S08]であったかもしれないと考えています。どちらの方法でも、50年代半ばまでに、メモリと直接通信し、終了時にCPUを中断するI/Oデバイスを持つシステムが存在しました。

この発明の歴史は現実、または時にはあいまいに機械と結びついているので、ここまで歴史は辿るのが難しいです。たとえば、リンカーン・ラボのTX-2マシンが最初にベクトル化された割込みであると考える人もいますが(S08)、これはほとんどわかりません。

アイデアは比較的分かりやすいので、遅いI/Oを待っている間にCPUに何か他のことをさせるというアイデアは自然に生まれたものでしょう。おそらく、"誰が先に？"というのは関係ありません。確かに明らかなのは、人々がこれらの初期のマシンを構築すると、I/Oサポートが必要だったということです。割り込み、DMA、および関連するアイデアは、すべて高速のCPUと低速デバイスの性質の自然な結果です。あなたがその時にそこにいたなら、あなたは似た考えを持っていたかもしれません。

## 36.10 Summary
これで、OSがデバイスとどのようにやりとりするかについて、基本的に理解しているはずです。デバイスの効率を助けるために、割り込みとDMAの2つの手法が導入されており、デバイス・レジスタ、明示的なI/O命令、およびメモリ・マップされたI/Oにアクセスする2つの方法の詳細が説明されています。最後に、デバイスドライバの概念が提示され、OS自体がどのように低レベルの詳細をカプセル化して、残りのOSをデバイス中立的な方法で簡単に構築できるかを示しています。

## 参考文献
[A+11] “vIC: Interrupt Coalescing for Virtual Machine Storage Device IO”  
Irfan Ahmad, Ajay Gulati, Ali Mashtizadeh  
USENIX ’11  
A terrific survey of interrupt coalescing in traditional and virtualized environments.  

[C01] “An Empirical Study of Operating System Errors”  
Andy Chou, Junfeng Yang, Benjamin Chelf, Seth Hallem, Dawson Engler  
SOSP ’01  
One of the first papers to systematically explore how many bugs are in modern operating systems. Among other neat findings, the authors show that device drivers have something like seven times more bugs than mainline kernel code.  

[CK+08] “The xv6 Operating System”  
Russ Cox, Frans Kaashoek, Robert Morris, Nickolai Zeldovich  
From: http://pdos.csail.mit.edu/6.828/2008/index.html  
See ide.c for the IDE device driver, with a few more details therein.  

[D07] “What Every Programmer Should Know About Memory”  
Ulrich Drepper  
November, 2007  
Available: http://www.akkadia.org/drepper/cpumemory.pdf  
A fantastic read about modern memory systems, starting at DRAM and going all the way up to virtualization and cache-optimized algorithms.

[G08] “EIO: Error-handling is Occasionally Correct”  
Haryadi Gunawi, Cindy Rubio-Gonzalez, Andrea Arpaci-Dusseau, Remzi Arpaci-Dusseau, Ben Liblit  
FAST ’08, San Jose, CA, February 2008  
Our own work on building a tool to find code in Linux file systems that does not handle error return properly. We found hundreds and hundreds of bugs, many of which have now been fixed.

[L94] “AT Attachment Interface for Disk Drives”  
Lawrence J. Lamers, X3T10 Technical Editor  
Available: ftp://ftp.t10.org/t13/project/d0791r4c-ATA-1.pdf  
Reference number: ANSI X3.221 - 1994 A rather dry document about device interfaces. Read it at your own peril.

[MR96] “Eliminating Receive Livelock in an Interrupt-driven Kernel”  
Jeffrey Mogul and K. K. Ramakrishnan  
USENIX ’96, San Diego, CA, January 1996  
Mogul and colleagues did a great deal of pioneering work on web server network performance. This paper is but one example.

[S08] “Interrupts”  
Mark Smotherman, as of July ’08  
Available: http://people.cs.clemson.edu/˜mark/interrupts.html  
A treasure trove of information on the history of interrupts, DMA, and related early ideas in computing

[S03] “Improving the Reliability of Commodity Operating Systems”  
Michael M. Swift, Brian N. Bershad, and Henry M. Levy  
SOSP ’03  
Swift’s work revived interest in a more microkernel-like approach to operating systems; minimally, it finally gave some good reasons why address-space based protection could be useful in a modern OS.

[W10] “Hard Disk Driver”  
Washington State Course Homepage  
Available: http://eecs.wsu.edu/˜cs460/cs560/HDdriver.html  
A nice summary of a simple IDE disk drive’s interface and how to build a device driver for it.  

\newpage

# 37 Hard Disk Drives
最後の章では、I/Oデバイスの一般的な概念を紹介し、OSがそのような獣とどのように相互作用するかを示しました。この章では、特に1つのデバイス、ハードディスクドライブについて詳しく説明します。これらのドライブは、何十年もの間、コンピュータシステムにおける永続的なデータストレージの主要な形態であり、ファイルシステム技術の開発の多くは、その動作を前提としています。したがって、それを管理するファイルシステムソフトウェアを構築する前に、ディスク操作の詳細を理解することは価値があります。これらの詳細の多くは、RuemmlerとWilkes [RW92]とAnderson、Dykes、Riedel [ADR03]の優れた論文で利用できます。

>> CRUX: HOW TO STORE AND ACCESS DATA ON DISK  
>> 最新のハードディスクドライブはどのようにデータを保存しますか？インターフェイスとは何ですか？データは実際にどのように割り当てられ、アクセスされますか？ディスクスケジューリングはどのようにパフォーマンスを改善しますか？

# 37.1 The Interface
現代のディスクドライブとのインターフェースを理解することから始めましょう。現代のすべてのドライブの基本的なインターフェースは簡単です。ドライブは、多数のセクタ(512バイトごとのブロック)で構成され、それぞれのセクタは読み書き可能です。セクタは、nセクタのディスク上で0からn-1まで番号が付けられます。したがって、ディスクをセクタの配列として見ることができます。0〜n-1はドライブのアドレス空間です。

マルチセクタ操作が可能です。確かに、多くのファイルシステムは一度に4KBを読み書きします。しかし、ディスクを更新するとき、ドライブ製造業者が行う唯一の保証は、単一の512バイト書き込みがアトミックである(すなわち、その全体が完了するか、または完了しない)ことである。したがって、不意の電力損失が発生した場合、より大きな書込みの一部のみが完了することがある(時には、torn write(破れた書込み)と呼ばれる)。

ディスクドライブのほとんどのクライアントはいくつかの前提がありますが、それはインターフェイスに直接指定されていません。SchlosserとGangerはこれをディスクドライブの「未書き込み契約」と呼んでいます[SG04]。具体的には、通常、ドライブのアドレス空間内で互いに隣接する2つのブロックにアクセスすることは、離れている2つのブロックにアクセスするよりも高速であると想定することができます。連続したチャンク(すなわち、順序読み込みまたは書き込み)内のブロックにアクセスすることが最速のアクセスモードであり、通常はそれ以上のランダムアクセスパターンよりもはるかに高速であると通常仮定してもよいです。

# 37.2 Basic Geometry
現代のディスクのいくつかのコンポーネントを理解してみましょう。まずは、磁気的な変化を誘発することによってデータが永続的に保存される円形の硬い表面であるプラッターから始めます。ディスクには1つまたは複数のプラッタがあります。各プラッターは2面を持ち、それぞれの面はサーフェスと呼ばれます。これらのプラッタは、通常、アルミニウムなどの硬い材料で作られ、薄い磁性層でコーティングされているため、ドライブの電源を切ってもドライブがビットを永続的に保存できます。

プラッタはスピンドルの周りに一緒に拘束されています。スピンドルは、一定の速度で(ドライブの電源がオンの状態で)プラッタを回転させるモータに接続されています。回転速度はRPM(回転数/分)で測定されることが多く、典型的な現代的な値は7,200 RPM〜15,000 RPMの範囲です。1回転の時間に興味があることが多いことに注意してください。たとえば、10,000 RPMで回転するドライブは、1回転に約6ミリ秒(6 ms)かかることを意味します。

データはセクタの同心円内の各サーフェスでエンコードされます。そのような同心円を1つのトラックと呼びます。単一のサーフェスには、数千、数千のトラックが含まれています。これらのトラックはしっかりと詰め込まれており、人間の髪の毛の幅に収まる数百のトラックがあります。

表面から読み書きするためには、ディスク上の磁気パターンを感知(すなわち、読み取る)するか、またはそれらに変化を誘発(すなわち書き込む)することができる機構が必要です。この読み書きのプロセスは、ディスクヘッドによって解決されます。ドライブの表面に1つのそのようなヘッドがあります。ディスクヘッドは単一のディスクアームに取り付けられ、ディスクアームは表面を横切って移動し、ヘッドを望んだトラック上に位置決めします。

# 37.3 A Simple Disk Drive
一度に1つのモデルを1つのトラックにまとめることで、ディスクの仕組みを理解してみましょう。1つのトラックを持つシンプルなディスクがあるとします(図37.1)。このトラックには12のセクタがあり、それぞれのセクタのサイズは512バイト(通常のセクタサイズ、リコール)であり、したがって0〜11の数字で表されます。ここにある単一のプラッタは、モータが取り付けられたスピンドルを中心に回転します。もちろん、トラック自体は興味深いものではありません。これらのセクタを読み書きできるようにしたいので、今見ているようにディスクアームにディスクヘッドが必要です(図37.2)。図では、アームの端部に取り付けられたディスクヘッドがセクター6の上に配置され、表面が反時計回りに回転しています。

![](../37/img/fig37_1.PNG)

![](../37/img/fig37_2.PNG)

### Single-track Latency: The Rotational Delay
単純なonetrackディスク上でリクエストがどのように処理されるかを理解するために、今ブロック0の読み込み要求を受け取ったとします。ディスクサービスはどのようにこの要求を処理する必要がありますか？

私たちのシンプルなディスクでは、ディスクはそれほど多くする必要はありません。特に、ディスクヘッドの下で望んだセクタが回転するのを待たなければならない。この待ち時間は、現代のドライブではよく起こりますが、I/Oサービス時間の重要な要素です。rotational delay(回転遅延)(ときどき回転遅延、それは奇妙に聞こえるかもしれませんが)という特殊な名前があります。この例では、完全回転遅延がRである場合、ディスクは、読み書きヘッドがセクタ0で待っていた場合(もしセクタ6で開始すると)、平均でR/2の回転遅延を生じます。この単一のトラックに対する最悪の場合の要求は、セクタ5へのものであり、そのような要求に対応するためにほぼ完全回転遅延を引き起こします。

### Multiple Tracks: Seek Time
これまでのところ、私たちのディスクには単一のトラックしかありません。これはあまり現実的ではありません。現代のディスクには数百万ものものがあります。このようにして、少しだけ現実的なディスク面を見てみましょう。このディスク面は3つのトラックで構成されています(図37.3)

![](../37/img/fig37_3.PNG)

図では、ヘッドは現在、最も内側のトラック(セクタ24〜35を含む)上に配置されている。次のトラックは次のセクタセット(12〜23)を含み、最外トラックは第1のセクタ(0〜11)を含みます。

ドライブがどのセクタにアクセスする可能性があるかを理解するために、セクタ11への読み込みなど、離れたセクタへの要求で何が起きるかを追跡します。この読み込みを処理するには、まずドライブをディスクアームを正しいトラック(この場合、最外側のトラック)に移動させるプロセスを行います。このプロセスシーク(seek)と呼びます。シークは回転と一緒に、最もコストのかかるディスク操作の1つです。

シークは注目すべきですが、多くのフェーズがあります。最初にディスクアームが動くように加速フェーズを行います。腕が最高速度で動いているときに惰性走行し、次に腕が減速すると減速します。最終的にはヘッドは正しいトラック上に慎重に配置されるとsettling(沈む)します。ドライブが正しいトラックを見つけることが確実でなければならないので、settling time(沈む時間)は非常に重要であり、例えば、0.5~2msとなることが多いです。

シークの後、ディスクアームはヘッドを正しいトラックの上に配置します。シークの描写は図37.3にあります。

見てわかるように、シーク中にアームが所望のトラックに移動し、プラッタが回転しています。この場合は約3セクタである。したがって、セクタ9はディスクヘッドの下をちょうど通過しようとしており、転送を完了するために短い回転遅延に耐える必要があります。

セクタ11がディスクヘッドの下を通過すると、転送と呼ばれるI/Oの最終フェーズが実行され、データの読み書きが行われます。したがって、I/O時間の完全な図が得られます。最初にシークし、次に回転遅延を待ってから最後に転送します。

### Some Other Details
あまり時間を費やすことはありませんが、ハードドライブの動作に関する他の面白い詳細があります。多くのドライブでは、トラックの境界を越えても順次読み込みが適切に処理できるように、何らかのトラックスキューを使用しています。私たちの単純な例のディスクでは、これは図37.4のように見えるかもしれません。

![](../37/img/fig37_4.PNG)

あるトラックから別のトラックに切り替えると、ディスクはヘッドを再配置する時間が必要となるため(隣接するトラックまで)、セクタはこのように歪んでいることがよくあります。このようなスキュー(歪み)がなければ、ヘッドは次のトラックに移動されるが、望んだセクタの次のブロックはヘッドの下で既に回転しているため、ドライブは次のブロックにアクセスするためにほぼ全回転遅延を待たなければいけません。

もう一つの現実は、アウタートラックは、ジオメトリの結果であるインナートラックより多くのセクタを持つ傾向があります。そこには単に余裕があります。これらのトラックは、ディスクが複数のゾーンに編成され、ゾーンがサーフェス上のトラックの連続したセットであるマルチゾーンのディスクドライブとしてよく使用されます。各ゾーンは1トラックあたりのセクタ数が同じであり、外側ゾーンは内側ゾーンよりも多くのセクタを持ちます。

最後に、最近のディスクドライブの重要な部分は、歴史的な理由からトラックバッファと呼ばれるキャッシュです。このキャッシュは、ドライブがディスクから読み書きするデータを保持するために使用できるわずかな量のメモリ(通常は約8または16 MB)です。たとえば、ディスクからセクタを読み取る場合、ドライブはそのトラックのすべてのセクタを読み込み、そのセクタをそのメモリにキャッシュすることになります。これにより、ドライブは、同じトラックへの後続の要求にすばやく応答することができます。

書き込みの場合、ドライブには選択肢があります。データがメモリに格納されたとき、または書き込みが実際にディスクに書き込まれた後に、書き込みが完了したことを確認する必要がありますか？前者はライトバックキャッシング(または時には即時報告)と呼ばれ、後者はライトスルーと呼ばれています。ライトバックキャッシングを行うと、ドライブが「高速」に見えることがありますが、危険です。ファイルシステムまたはアプリケーションで、正しい順序でデータをディスクに書き込む必要がある場合は、ライトバックキャッシュで問題が発生する可能性があります(詳細は、ファイルシステムのジャーナリングに関する章を参照してください)。

>> ASIDE: DIMENSIONAL ANALYSIS  
>> 化学の授業では、単位を単純に設定し打ち消しあい、結果として何か答えをだしました。どのようにしたかを覚えていますか？その化学的な魔法は、高次元ファクター解析の名で知られており、それはコンピュータシステム分析にも有用であることが判明しています。  
次元分析がどのように機能するのか、なぜそれが有用なのかを見てみましょう。この場合、ディスクの1回転にかかる時間(ミリ秒)を把握しておく必要があると仮定します。残念ながら、ディスクのRPM、または1分あたりの回転のみが与えられます。10K RPMディスク(つまり、1分間に10,000回回転するディスク)を想定しているとします。1回転あたりの時間をミリ秒単位で取得できるようにしますか？  
これを行うには、まず、目的のユニットを左に置きます。この場合、1回転あたりの時間(ミリ秒)を取得したいので、正確には(Time(ms))/(1 Rotation)を書きます。私たちが知っているすべてを書き、できるだけユニットを取り消してください。最初に、(1分)/(10,000回転)(回転を下にし、それが左にあるように)、分を(60秒/1分)秒に変換し、最後に(1000 ms)/(1秒)でミリ秒単位の秒数に変換します。最終的な結果は以下の通りです(ユニットは素早くキャンセルされています)。  
![](../37/img/fig37_4_1.PNG)  
この例からわかるように、次元分析は、直感的に見えるものを単純で反復可能なプロセスにします。上記のRPM計算以外にも、定期的にI/O分析を行うことができます。たとえば、ディスクの転送速度(100 MB/秒など)が与えられることがよくあります。次に、512 KBブロック(ミリ秒単位)の転送にはどのくらいの時間がかかりますか？次元分析を使用すると簡単です。  
![](../37/img/fig37_4_2.PNG)  

## 37.4 I/O Time: Doing The Math
ディスクの抽象モデルが完成したので、ディスクのパフォーマンスをよりよく理解するために、少し分析することができます。特に、I/O時間を3つの主要コンポーネントの合計として表すことができるようになりました。  
![](../37/img/fig37_4_3.PNG)  
ドライブ間の比較のためにしばしばより容易に使用されるI/O(R_I/O)の割合(以下で説明する)は、時間から容易に計算されることを覚えておいてください。単純に転送のサイズをそれにかかる時間で割ってください：  
![](../37/img/fig37_5.PNG)  
I/O時間をより良く感じるには、次の計算を実行してみましょう。私たちが興味を持っている2つの仕事量があると仮定します。最初のものは、ランダム仕事量として知られ、ディスク上のランダムな場所に小さな(たとえば4KB)読み取りを発行します。ランダム仕事量は、データベース管理システムを含む多くの重要なアプリケーションで一般的です。順次仕事量と呼ばれる第2のものは、ジャンプすることなく、ディスクから連続して多数のセクタを読み取るだけです。順次アクセスパターンは非常に一般的であり、したがって同様に重要です。

ランダム仕事量と順次仕事量のパフォーマンスの違いを理解するには、まずディスクドライブについていくつかの仮定を立てる必要があります。Seagateの最新のディスクをいくつか見てみましょう。Cheetah 15K.5 [S09b]と呼ばれる最初のものは、高性能SCSIドライブです。2番目のバラクーダ[S09a]は、容量のために作られたドライブです。両方の詳細は図37.5にあります。

![](../37/img/fig37_5_1.PNG)

ご覧のように、ドライブは全く異なる特性を持ち、多くの点でディスクドライブ市場の2つの重要なコンポーネントを素早く要約しています。1つは、ドライブができるだけ速く回転し、シーク時間が短く、データをすばやく転送できるように設計された「高性能」ドライブ市場です。2番目は「容量」市場で、1バイトあたりのコストが最も重要な側面です。したがって、ドライブはより低速ですが、使用可能なスペースにできるだけ多くのビットをパックします。

これらの数字から、上に概説した2つの仕事量でドライブがどれくらいうまくいくかを計算することができます。ランダムな仕事量を見てみましょう。ディスク上のランダムな位置で各4 KBの読み取りが発生すると仮定して、このような読み取りがどれぐらいかかるかを計算することができます。  
![](../37/img/fig37_5_2.PNG)  

>> TIP: USE DISKS SEQUENTIALLY  
>> 可能な限り、シーケンシャルな方法でディスクとの間でデータを転送します。シーケンシャルが不可能な場合は、少なくとも大量のチャンクでデータを転送することを検討してください。I/Oがランダムではない場合、I/Oパフォーマンスは劇的に低下します。また、ユーザーは苦しみます。また、不注意なランダムI/Oで何が苦しんでいるのかを知ることで、苦しみます。

平均シーク時間(4ミリ秒)は、製造元によって報告された平均時間とみなされます。表面の一方の端から他方への完全なシークは2〜3倍長くかかる可能性があることに注意してください。平均回転遅延はRPMから直接計算されます。15000 RPMは250 RPS(1秒あたりの回転数)に等しい。従って、各回転は4msを要する。平均して、ディスクは半回転するため、平均時間は2msです。最後に、転送時間は、ピーク転送レートでの転送サイズにすぎません。ここではそれほど小さくはありません(30マイクロ秒です、1ミリ秒は1000マイクロ秒です)。

したがって、上記の方程式から、チーターのT_I/Oはおおよそ6ミリ秒に等しい。I/Oの速度を計算するには、転送のサイズを平均時間で除算するだけで、約0.66 MB/sのランダムな仕事量でチーターのR_I/Oに到着します。バラクーダと同じ計算では、T_I/Oは約13.2ミリ秒、2倍以上の遅さ、したがって約0.31 MB/sの速度をもたらします。

次に、順次仕事量を見てみましょう。ここでは、非常に長い転送の前に単一のシークとローテーションがあると仮定できます。簡単にするために、転送のサイズが100 MBであると仮定します。したがって、BarracudaとCheetahのT_I/Oはそれぞれ約800 msと950 msです。従って、I/Oのレートは、それぞれ125 MB/s及び105 MB/sのピーク転送速度に非常に近い。図37.6にこれらの数値をまとめました。

![](../37/img/fig37_6.PNG)

図は私たちに多くの重要なことを示しています。まず第一に、最も重要なのは、ランダムとシーケンシャルの仕事量の間には、チーターの場合はほぼ200倍、バラクーダの場合は300倍以上の差があることです。そして、我々はコンピューティングの歴史の中で最も明白なデザインのヒントに到達します。

もう1つ、より微妙な点であるハイエンドの「パフォーマンス」ドライブとローエンドの「容量」ドライブのパフォーマンスには大きな違いがあります。この理由(およびその他)のために、人々は後者をできるだけ安く手に入れようとしながら、よく前者のようなパフォーマンスを望んでいます。

>> ASIDE: COMPUTING THE “AVERAGE” SEEK  
>> 多くの書籍や論文では、平均シーク時間の約3分の1のディスクシーク時間が表示されます。これはどこから来たのですか？  
それは、時間ではなく平均シーク距離に基づく単純な計算から生じることが分かります。ディスクを0からNまでのトラックのセットとして想像してください。したがって、任意の2つのトラックxとyの間のシーク距離は、| x - y |の差の絶対値として計算されます。  
平均シーク距離を計算するには、まずシーク距離をすべて加算するだけです。  
![](../37/img/fig37_6_1.PNG)  
次に、これを可能なシークの数N^2で割ります。合計を計算するには、整数型を使用します  
![](../37/img/fig37_6_2.PNG)  
内部積分を計算するには、絶対値を分解しましょう  
![](../37/img/fig37_6_3.PNG)  
これを解決すると、![](../37/img/fig37_6_6.PNG)に簡略化でき、![](../37/img/fig37_6_7.PNG)が生成されます。今度は、外積分を計算しなければなりません  
![](../37/img/fig37_6_4.PNG)  
which results in:  
![](../37/img/fig37_6_5.PNG)  
平均シーク距離を計算するには、シークの総数(N^2)で除算する必要があります：![](../37/img/fig37_6_8.PNG)。したがって、ディスク上の平均シーク距離は、すべての可能なシークにわたって、全距離の3分の1でです。そして今、平均的なシークが完全なシークの3分の1であると聞くと、それはどこから来たのかを知るでしょう。

## 37.5 Disk Scheduling
I / Oのコストが高いため、OSは歴史的にディスクに発行されたI/Oの順序を決定する役割を果たしました。より具体的には、ディスクスケジューラは、一連のI/O要求があれば、その要求を検査し、次にスケジューリングするものを決定します[SCO90、JW91]。

ジョブスケジューリングとは異なり、各ジョブの長さは通常不明ですが、ディスクスケジューリングでは、「ジョブ」(ディスク要求)の所要時間を推測することができます。要求のシークと可能な回転遅延を見積もることにより、ディスクスケジューラは、各要求がどれくらいの時間かかるかを知ることができ、最初にサービスする時間が最も短いものを選びます(貪欲に)。したがって、ディスクスケジューラは、その操作においてSJF(shortest job first)の原則に従おうとします。

### SSTF: Shortest Seek Time First
早期ディスクスケジューリングアプローチの1つは、最短シークタイムファースト(SSTF)(最短シークファーストまたはSSFとも呼ばれる)として知られている。SSTFは、I/O要求のキューをトラックごとに順序付けし、最寄りのトラックで要求をピックして最初に完了します。例えば、ヘッドの現在位置が内側トラック上にあり、セクタ21(ミドルトラック)と2(外側トラック)に対する要求があると仮定すると、最初に21へリクエストを発行し、その完了を待ってから2を要求するように発行します(図37.7)。

![](../37/img/fig37_7.PNG)

SSTFはこの例ではうまく機能し、最初はミドルトラック、次にアウタートラックを探します。しかし、SSTFは万能ではないです。まず、ドライブのジオメトリはホストOSで使用できません。むしろ、それはブロックの配列を見ます。幸いにも、この問題はかなり簡単に修正されています。OSは、SSTFの代わりに、次の最も近いブロックアドレスを持つ要求をスケジューリングするnearest-block-first(NBF)を実装するだけで済みます。

第2の問題はより根本的です。上記の例では、ヘッドが現在位置している内側のトラックへの要求が安定しているとします。他のトラックへのリクエストは、純粋なSSTFアプローチによって完全に無視されます。したがって、問題の要点は次のとおりです。

>> CRUX: HOW TO HANDLE DISK STARVATION  
>> どのようにしてSSTFのような、しかし、飢餓を避けるようなスケジューリングを実装できますか？

### Elevator (a.k.a. SCAN or C-SCAN)
このクエリーへの答えは、しばらく前に開発されました(たとえば[CKR72]を参照)。これは比較的簡単です。もともとSCANと呼ばれていたこのアルゴリズムは、トラックを横切って順番に要求を処理するディスクを横切って前後に移動するだけです。ディスクを横切る単一のパス(外側から内側トラック、または内側から外側)をスイープと呼ぶことにしましょう。したがって、ディスクのこのスイープですでに処理されているトラックのブロックに対する要求が発生すると、すぐには処理されず、次のスイープまで(他の方向の)キューに入れられます。

SCANには多数の変種があり、そのすべてがほぼ同じことをしています。例えば、Coffman et al。スイープを実行しているときに処理されるキューをフリーズするF-SCANが導入されました[CKR72]。このアクションによって、スイープ中に入った要求がキューに入れられ、後で処理されます。そうすることで、遅く到着する(しかしより近い)要求のサービスを遅らせることによって、遠く離れた要求の枯渇を回避することができます。

C-SCANはCircular SCANの略語でもあります。ディスク上の両方向にスイープするのではなく、アルゴリズムは外側から内側へのみスイープし、外側のトラックでリセットして再び開始します。これは、純粋なバック・アンド・フォワード・スキャンは中間トラックに有利です、すなわち外側トラックを処理した後、SCANが再び外側トラックに戻る前に2回真ん中を通過するので、内側トラックおよび外側トラックに対して少し公平である。

SCANアルゴリズム(およびその従兄弟)は、エレベータアルゴリズムと呼ばれることがあります。エレベータは、エレベータのように動作します。エレベータは、上または下に移動し、どちらのフロアが近いかというベースでは動きません。ここで、フロアのどちらが近いかというアルゴリズムで動いていると想像してみてください。あなたがもし10階からエレベータに載って1階へ行っているとき、途中で誰かが3階から乗って4階へ行くためにボタンを押したとき、エレベーターは現時点で1階より4階が近いので、エレベーターが4階にあがります。どれほど迷惑なことでしょう。ご覧のように、エレベーターのアルゴリズムは実際に使用すると、さきほどのような迷惑な行為を防ぎます。ディスクでは、単に飢餓を防ぎます。

残念なことに、SCANとその従兄弟は最良のスケジューリング技術ではありません。特に、SCAN(またはSSTF even)は、実際にはSJFの原理に厳密に準拠していません。特に、それらは回転を無視します。

>> CRUX: HOW TO ACCOUNT FOR DISK ROTATION COSTS  
シークと回転の両方を考慮に入れて、SJFにさらに近似したアルゴリズムを実装するにはどうすればよいですか？

### SPTF: Shortest Positioning Time First
shortest positioning time first(最短の位置時間)、問題の解決策であるSPTFスケジューリング(時にはshortest access time first(最短アクセス時間)とも呼ばれる)を検討する前に、問題をより詳細に理解してください。図37.8に例を示します。

![](../37/img/fig37_8.PNG)

この例では、ヘッドは、現在、内側トラック上のセクタ30上に配置されています。したがって、スケジューラは、次の要求に対してセクタ16(中間トラック上)またはセクタ8(外側トラック上)をスケジューリングすべきかどうかを決定しなければいけません。そのため、どのサービスを次にすればよいでしょうか？

答えは、もちろん、「それは依存している」です。エンジニアリングでは、「それは依存している」ことがほとんど常に答えです。トレードオフはエンジニアの生活の一部です。そのような格言は、ピンチでも良いです。たとえば、あなたの上司の質問に対する答えがわからないときは、この宝石を試してみてください。しかし、なぜそれが依存するのかを知ることは、ほとんどいつもより良いことです。それはここで議論するものです。

ここに依存するのは、回転と比較したシークの相対的な時間です。この例では、シーク時間が回転遅延よりもはるかに高い場合、SSTF(およびバリアント)は正常です。しかし、シークがローテーションよりかなり速いと想像してください。次に、この例では、外側のトラックでサービス要求8を求めるよりも、サービス16の中間トラックをシークするのがより短くなります。これは、ディスクヘッドの下を通過する前に全周にわたって回転しなければいけないからです。

現代のドライブでは、上記のように、シークとローテーションの両方がほぼ同等です(もちろん、正確な要求にもよるが)。したがって、SPTFは有用であり、パフォーマンスを向上させます。しかし、OSで実装することはさらに難しく、ディスク境界がどこにあるか、またはディスクヘッドが現在(回転の意味で)どこにあるかはよく分かっていません。したがって、SPTFは通常、以下で説明するドライブ内で実行されます。

>> TIP: IT ALWAYS DEPENDS (LIVNY’S LAW)  
>> 私たちの同僚であるMiron Livnyはいつも言っているように、ほとんどの質問は「それは依存している」と答えることができます。しかし、あまりにも多くの質問にこのように答えると、人々はあなたに質問をするのをやめます。たとえば、誰かが「昼食に行きたいですか？」と尋ねると、あなたは「それは、あなたがくるのかによる」というでしょう。

### Other Scheduling Issues
この基本的なディスク操作、スケジューリング、および関連するトピックのこの簡単な説明では、他にも多くの問題がありますが、議論しませんでした。そのような問題の1つは、最新のシステムでディスクスケジューリングを実行する場所はどこですか？ということです。古いシステムでは、オペレーティングシステムはすべてスケジューリングを行いました。保留中の要求のセットを調べた後、OSは最良のものを選択し、ディスクに発行します。その要求が完了すると、次の要求が選択されます。ディスクは単純でした。

現代のシステムでは、ディスクは複数の未解決の要求に対応でき、高度な内部スケジューラ自体を持っています(SPTFを正確に実装できる、ディスクコントローラ内では正確なヘッド位置を含む関連するすべての詳細が利用可能です)。したがって、OSスケジューラは、通常、いくつかの要求が最良であると思うもの(たとえば16)を選択し、すべてをディスクに発行します。ディスクは、詳細な内部情報のヘッド位置および詳細なトラックレイアウト情報を使用して、可能な限り最善の順序(SPTF)で前記要求を処理します。

ディスクスケジューラによって実行されるもう1つの重要な関連タスクは、I/Oマージです。たとえば、図37.8のように、ブロック33、次に8、そして34を読み込む一連の要求を想像してみてください。この場合、スケジューラは、ブロック33および34の要求を単一の2ブロック要求にマージする必要があります。スケジューラが行うリオーダリングは、マージされた要求に対して実行されます。マージは、ディスクに送信される要求の数を減らし、オーバーヘッドを減らすため、OSレベルで特に重要です。

現代のスケジューラが直面する最後の問題の1つは、システムがディスクへのI/Oを発行するまでにどれくらいの時間待つべきかということです。ディスクが一度I/Oがあっても、すぐにドライブに要求を発行する必要があると考えるかもしれません。このアプローチは、work-conserving(作業を保存する)と呼ばれます。これは、処理する必要がある場合、ディスクは決してアイドル状態にならないためです。しかし、先行ディスクスケジューリングに関する研究[ID01]では、非作業保存アプローチと呼ばれるもので、待つためのビットがある方が良いことが示されています。

待つことによって、新しい"より良い"要求がディスクに到着する可能性があるため、全体的な効率が向上します。もちろん、いつ待つか、どのくらい長くするかを決めるのは難しいことです。詳細については研究論文を参照するか、Linuxカーネルの実装をチェックして、そのようなアイディアがどのように実践されているかを見てみましょう(あなたが野心的な場合)

## 37.6 Summary
ディスクの仕組みの概要を示しました。要約は実際には詳細な機能モデルです。それは実際のドライブ設計に入る驚くべき物理学、エレクトロニクス、物質科学を記述するものではありません。しかし、その性質の詳細についてもっと興味を持っている人には、別のメジャー(またはマイナーな)な分野を提案します。これらの別のメジャーの分野のモデルに興味を持っているのであればそれは良かったです。このモデルを使用して、これらの素晴らしいデバイスの上にさらに興味深いシステムを構築することができます。

## 参考文献
[ADR03] “More Than an Interface: SCSI vs. ATA”  
Dave Anderson, Jim Dykes, Erik Riedel  
FAST ’03, 2003  
One of the best recent-ish references on how modern disk drives really work; a must read for anyone interested in knowing more.

[CKR72] “Analysis of Scanning Policies for Reducing Disk Seek Times”  
E.G. Coffman, L.A. Klimko, B. Ryan  
SIAM Journal of Computing, September 1972, Vol 1. No 3.  
Some of the early work in the field of disk scheduling.  

[ID01] “Anticipatory Scheduling: A Disk-scheduling Framework To Overcome Deceptive Idleness In Synchronous I/O”  
Sitaram Iyer, Peter Druschel  
SOSP ’01, October 2001  
A cool paper showing how waiting can improve disk scheduling: better requests may be on their way!  

[JW91] “Disk Scheduling Algorithms Based On Rotational Position”  
D. Jacobson, J. Wilkes  
Technical Report HPL-CSP-91-7rev1, Hewlett-Packard (February 1991)  
A more modern take on disk scheduling. It remains a technical report (and not a published paper) because the authors were scooped by Seltzer et al. [SCO90].

[RW92] “An Introduction to Disk Drive Modeling”  
C. Ruemmler, J. Wilkes  
IEEE Computer, 27:3, pp. 17-28, March 1994  
A terrific introduction to the basics of disk operation. Some pieces are out of date, but most of the basics remain.

[SCO90] “Disk Scheduling Revisited”  
Margo Seltzer, Peter Chen, John Ousterhout  
USENIX 1990  
A paper that talks about how rotation matters too in the world of disk scheduling.  

[SG04] “MEMS-based storage devices and standard disk interfaces:A square peg in a round hole?”  
Steven W. Schlosser, Gregory R. Ganger  
FAST ’04, pp. 87-100, 2004  
While the MEMS aspect of this paper hasn’t yet made an impact, the discussion of the contract between file systems and disks is wonderful and a lasting contribution.

[S09a] “Barracuda ES.2 data sheet”  
http://www.seagate.com/docs/pdf/datasheet/disc/ds_barracuda_es.pdf  
A data sheet; read at your own risk. Risk of what? Boredom.  

[S09b] “Cheetah 15K.5”  
http://www.seagate.com/docs/pdf/datasheet/disc/ds-cheetah-15k-5-us.pdf  
See above commentary on data sheets.  

\newpage

# 38 Redundant Arrays of Inexpensive Disks (RAIDs)
ディスクを使用する場合、ディスクを高速化したい場合があります。I/O操作が遅いため、システム全体のボトルネックになる可能性があります。ディスクを使用する場合、ディスクを大きくしたい場合があります。多くのデータをオンラインに置かなければ、ローカルディスクがデータでいっぱいになってしまいます。ディスクを使用する場合、ディスクをより信頼できるものにすることを望みます。ディスクに障害が発生した場合、データがバックアップされていなければ、その重要なデータはすべて失われます。

>> CRUX: HOW TO MAKE A LARGE, FAST, RELIABLE DISK  
>> 大規模で高速で信頼性の高いストレージシステムを構築するにはどうすればよいですか？重要な技術は何ですか？異なるアプローチ間のトレードオフは何ですか？

この章では、RAID [P + 88]というよりよく知られている安価なディスクの冗長配列を紹介します。これは、複数のディスクを一度に使用して、より速く、より大きく、より信頼性の高いディスクシステムを構築するテクニックです。この用語は、1980年代後半に米国バークレーの研究者グループによって導入されました。(David PattersonとRandy Katz教授、そしてGarth Gibson教授が率いる)くの異なる研究者が複数のディスクを使用してより優れたストレージシステムを構築するという基本的な考え方に同時に到達したのはこの頃でした[BG88、K86、K88、PB86、SG86]。

外部的には、RAIDはディスクのように見えます。つまり、読み書きできるブロックのグループです。内部的には、RAIDは複数のディスク、メモリ(揮発性と非揮発性の両方)、システムを管理する1つ以上のプロセッサで構成される複雑な獣です。ハードウェアRAIDは、ディスクグループを管理するタスクに特化したコンピュータシステムに非常によく似ています。

RAIDは、1つのディスクに比べて多くの利点を提供します。1つの利点はパフォーマンスです。並列に複数のディスクを使用すると、I/O時間が大幅に短縮されます。もう1つの利点は容量です。大きなデータセットには大きなディスクが必要です。最後に、RAIDは信頼性を向上させることができます。複数のディスクにデータを分散すると(RAID技術なし)、データは単一ディスクが壊れると消失するという脆弱に繋がります。何らかの形の冗長性では、RAIDはディスクの損失を許容し、何も問題がないかのように動作し続けることができます。

>> TIP: TRANSPARENCY ENABLES DEPLOYMENT  
システムに新しい機能を追加する方法を検討するときは、システムの残りの部分に変更を加える必要がないように、そのような機能を透過的に追加できるかどうかを常に検討する必要があります。既存のソフトウェア(または過激なハードウェア変更)の完全な書き換えを要求することは、アイデアのすばらしい可能性がなくなってしまいます。RAIDは完璧な例であり、確かにその透過性は成功に貢献しました。管理者はSCSIディスクの代わりにSCSIベースのRAIDストレージ配列をインストールでき、残りのシステム(ホストコンピュータ、OSなど)は1ビットも変更する必要がありませんでした。この展開の問題を解決することにより、RAIDは最初からすばらしい成功を収めました。

驚いたことに、RAIDは、これらの利点を使用するシステムにこれらの利点を透過的に提供します。つまり、RAIDはホストシステムにとって大きなディスクのように見えます。透明性の美しさはもちろん、ディスクをRAIDに置き換えるだけで、ソフトウェアの1行を変更する必要はありません。オペレーティング・システムおよびクライアント・アプリケーションは変更せずに動作し続けます。このように、透過性はRAIDの展開性を大幅に向上させ、ユーザと管理者がソフトウェア互換性の心配なしにRAIDを使用できるようにします。

ここで、RAIDの重要な側面のいくつかについて説明します。インターフェイス、障害モデルから始めて、容量、信頼性、パフォーマンスの3つの重要な軸に沿ってRAID設計を評価する方法について説明します。次に、RAIDの設計と実装にとって重要なその他の多くの問題について説明します。

## 38.1 Interface And RAID Internals
上のファイルシステムでは、RAIDは大きくて、うまくいけば高速で、(うまくいけば)信頼できるディスクのように見えます。単一のディスクの場合と同様に、ファイルシステム(または他のクライアント)によって読み書きできるブロックの線形配列として表示されます。

ファイルシステムがRAIDに論理I/O要求を発行すると、RAIDは内部要求を完了するためにアクセスしたディスク(またはディスク)を計算しなければいけません、そしてそうするために、1つまたは複数の物理I/Oを発行します。これらの物理I/Oの正確な性質は、RAIDレベルによって異なります(詳細は後述)。しかし、単純な例として、各ブロック(それぞれが別個のディスクにある)の2つのコピーを保持するRAIDを考えてみましょう。このようなミラー化されたRAIDシステムに書き込む場合、RAIDは発行される1つの論理I/Oごとに2つの物理I/Oを実行する必要があります。

RAIDシステムは、よくホストへの標準的な接続(例えば、SCSIまたはSATA)を有する別個のハードウェアボックスとして構築されます。しかし、内部的にRAIDはかなり複雑で、RAIDの動作を指示するファームウェアを実行するマイクロコントローラ、DRAMのような揮発性メモリが読み書きされたデータブロックをバッファリングし、場合によっては不揮発性メモリがバッファに安全な書き込みや特殊なロジックにも、パリティ計算を実行するかもしれません(いくつかのRAIDレベルで有効です、以下も参照してください)。高レベルでは、RAIDは特殊なコンピュータシステムです。プロセッサ、メモリ、およびディスクを備えています。ただし、アプリケーションを実行する代わりに、RAIDを動作させるように設計された専用ソフトウェアを実行します。

## 38.2 Fault Model
RAIDを理解し、さまざまなアプローチを比較するには、障害モデルを考慮する必要があります。RAIDは、特定の種類のディスク障害を検出してリカバリするように設計されています。したがって、どのような不具合が予期されるかを正確に知ることは、実際の設計に着手する上で重要です。

我々が想定する最初の故障モデルは非常に単純であり、fail stop fault model(フェイルストップフォールトモデル)と呼ばれています[S84]。このモデルでは、ディスクは正確に2つの状態のうちの1つになる可能性があります。二つの状態というのは作業中と故障です。作業ディスクを使用すると、すべてのブロックを読み書きすることができます。対照的に、ディスクに障害が発生した場合、ディスクは永久に失われたとみなされます。

フェールストップモデルの重要な側面の1つは、障害検出について想定していることです。具体的には、ディスクが故障した場合、これは容易に検出されると想定します。たとえば、RAID配列では、RAIDコントローラのハードウェア(またはソフトウェア)が、ディスクに障害が発生したときに直ちに監視することができます。

したがって、今のところ、ディスクの破損などのより複雑な「サイレント」エラーについては心配する必要はありません。また、別の方法で動作しているディスク(潜在的なセクタエラーと呼ばれることもあります)では、単一のブロックがアクセス不能になることを心配する必要はありません。私たちは後に、より複雑な(そして残念なことにより現実的な)ディスク障害を考えていきます。

## 38.3 How To Evaluate A RAID
すぐにわかるように、RAIDを構築するにはいくつかの異なるアプローチがあります。これらのアプローチのそれぞれは、その強みと弱みを理解するために、評価する価値のあるさまざまな特性を持っています。

具体的には、各RAID設計を3つの軸に沿って評価します。最初の軸は容量です。それぞれBブロックを持つN個のディスクのセットが与えられた場合、RAIDのクライアントはどれくらいの有用な容量を利用できますか？冗長性がなければ、答えはN・Bです。対照的に、各ブロックの2つのコピーを保持するシステム(ミラーリングと呼ばれる)があれば、(N・B)/2の容量が得られます。また、異なるスキーム(例えば、パリティベースのもの)がその間に入ることがあります。

評価の第2の軸は信頼性です。特定の設計で許容されるディスク障害の数はいくつですか？私たちのフォールト・モデルと一致して、ディスク全体が故障すると仮定します。後の章(データの完全性)では、より複雑な障害モードの処理方法について考えていきます。最後に、第3の軸はパフォーマンスです。パフォーマンスは、ディスク配列に提示される負荷に大きく依存するため、評価するのはやや難しいです。

したがって、パフォーマンスを評価する前に、最初に検討すべき典型的な仕事量のセットを提示します。RAIDレベル0(ストライピング)、RAIDレベル1(ミラーリング)、RAIDレベル4/5(パリティベースの冗長性)の3つの重要なRAID設計を検討します。これらのデザインの「レベル」という命名は、Berkeley [P + 88]のPatterson、Gibson、およびKatzの先駆的な仕事に由来します。

## 38.4 RAID Level 0: Striping
最初のRAIDレベルは、冗長性がないという点で、実際にはRAIDレベルではありません。ただし、RAIDレベル0、つまりストライピングはパフォーマンスと容量の優れた上限になり、理解を深めることができます。

ストライピングの最も単純な形式は、次のようにシステムのディスク全体にブロックをストライプします(ここでは4ディスク配列を仮定します)。  
![](../38/img/fig38_1.PNG)

図38.1から、ブロックの配列をラウンドロビン方式でディスクに分散させるという基本的な考え方が得られます。このアプローチは、連続したチャンクの配列に対して要求が行われたとき(たとえば、大規模なシーケンシャル読み取りの場合など)、配列から最も多くの並列性を抽出するように設計されています。同じ行のブロックをストライプと呼びます。したがって、ブロック0,1,2および3は、上の同じストライプにあります。

この例では、各ディスクに次のブロックに移動する前に、1つのブロック(それぞれサイズ4KB)が配置されていることを単純化しています。しかしながら、この配置は、必ずしもそうである必要はありません。たとえば、ブロックを図38.2のように配置することができます。  
![](../38/img/fig38_2.PNG)

この例では、各ディスクに2つの4KBブロックを配置してから、次のディスクに移動します。したがって、このRAIDアレイのチャンクサイズは8KBであり、したがってストライプは4チャンクまたは32KBのデータで構成されます。

>> ASIDE: THE RAID MAPPING PROBLEM  
>> RAIDの容量、信頼性、および性能の特性を調べる前に、まずマッピング問題と呼ばれるものを脇に示します。この問題はすべてのRAID配列で発生します。論理ブロックを読み書きすると、RAIDはアクセスする物理ディスクとオフセットを正確にどのように認識していますか？
これらのシンプルなRAIDレベルでは、論理ブロックを物理的な場所に正しくマップするために高度な知識は必要ありません。上記の最初のストライピングの例を考えてください(チャンクサイズ= 1ブロック= 4KB)。この場合、論理ブロックアドレスAが与えられれば、RAIDは簡単に2つの簡単な式で所望のディスクとオフセットを計算できます。  
```
Disk = A % number_of_disks
Offset = A / number_of_disks
```
>> これらはすべて整数演算であることに注意してください(たとえば、4/3 = 1ではなく1.33333 ...)。これらの方程式が簡単な例のためにどのように機能するかを見てみましょう。上記の最初のRAIDで、ブロック14の要求が到着したとします。4つのディスクがあると、ブロック14が(14％4 = 2)ディスク2であることを意味します。したがって、ブロック14は、3番目のディスク(0から始まるので)の4番目のブロック(0から始まるので)に存在しなければいけません。さまざまなチャンクサイズをサポートするためにこれらの方程式がどのように変更されるかについて考えることができます。それを試してみてください！それほど難しいことではありません。

### Chunk Sizes
チャンクサイズは主に配列のパフォーマンスに影響します。たとえば、チャンクサイズが小さいと、多くのファイルが多くのディスクにストライプ化され、1つのファイルに対する読み書きの並列性が向上します。ただし、要求全体の位置決め時間は、すべてのドライブの要求の位置決め時間の最大値によって決まるため、複数のディスクにまたがるブロックにアクセスするための位置決め時間が長くなります。

一方、大きなチャンクサイズは、このようなイントラファイルの並列性を低減し、従って、高いスループットを達成するために複数の並行要求(並行処理)に依存します。しかし、チャンクサイズが大きいと、位置決め時間が短縮されます。例えば、単一のファイルがチャンク内に収まり、単一のディスク上に置かれた場合、それにアクセスする際に発生する位置決め時間は、単一のディスクの位置決め時間と同じです。

したがって、「最良の」チャンク・サイズを決定することは、ディスク・システムに提示される仕事量に関する多くの知識を必要とするため、実行するのが難しいです。[CL95]。ここでは、1ブロック(4KB)のチャンクサイズを使用すると仮定します。ほとんどの配列はより大きいチャンクサイズ(たとえば64 KB)を使用しますが、以下で説明する問題については正確なチャンクサイズは関係ありません。したがって、わかりやすくするために単一のブロックを使用しています。

### Back To RAID-0 Analysis
ストライピングの容量、信頼性、パフォーマンスを評価しましょう。容量の観点からは、それは完璧です。サイズBブロックのN個のディスクがあれば、ストライピングはN・Bブロックの有用な容量を提供します。信頼性の観点からは、ストライピングも完璧ですが、悪いことにディスク障害が発生するとデータが失われます。最後に、パフォーマンスは優れています。ユーザーのI/O要求を処理するために、すべてのディスクが頻繁に並行して使用されます。

### Evaluating RAID Performance
RAIDパフォーマンスの分析では、2つの異なるパフォーマンスメトリックを考慮することができます。1つはsingle request latency(単一の要求の待ち時間)です。RAIDに対する単一のI/O要求の待ち時間を理解することは、単一の論理I/O操作中にどれだけの並列性が存在する可能性があるかを明らかにするのに役立ちます。第2は、steady state throughput of the RAID(RAIDの定常状態スループット)、すなわち多くの同時要求の総帯域幅です。RAIDは高性能環境で頻繁に使用されるため、定常状態の帯域幅は非常に重要なので、分析の主な焦点になります。

スループットをより詳細に理解するには、いくつかの重要な仕事量(仕事量)を提示する必要があります。ここでは、sequential(順次)とrandom(ランダム)の2種類の仕事量があると仮定します。シーケンシャル仕事量では、配列へのリクエストは大きな連続したチャンクになると想定します。例えば、ブロックxで始まりブロック(x + 1 MB)で終わる1 MBのデータにアクセスする要求(または一連の要求)は、連続しているとみなされます。シーケンシャル仕事量は多くの環境で共通しています(キーワードのために大きなファイルを検索することを考えると)、重要と考えられます。

ランダム仕事量の場合、各要求はかなり小さく、各要求はディスク上の別のランダムな場所にあると想定します。たとえば、要求のランダムなストリームは、最初に論理アドレス10で4KB、次に論理アドレス550,000、次に20,100などにアクセスすることができます。データベース管理システム(DBMS)上のなどのtransactional workloadsの重要な仕事量は、このタイプのアクセス・パターンを示します。したがって、重要な仕事量とみなされます。

もちろん、実際の仕事量はそれほど単純ではなく、しばしば、シーケンシャルとランダムに見えるコンポーネントの組み合わせと、その2つの間の動作を備えています。簡単にするために、これらの2つの可能性を考えます。

わかるように、シーケンシャルおよびランダムな仕事量は、ディスクとは大きく異なるパフォーマンス特性をもたらします。シーケンシャルアクセスでは、ディスクは最も効率的なモードで動作し、回転で待機している時間はほとんどなく、ほとんどの時間はデータを転送します。ランダムアクセスの場合、正反対です。ほとんどの時間は回転で待機しており、データの転送にはほとんど時間を費やされません。分析のこの違いを把握するために、ディスクはシーケンシャル仕事量でS MB /秒、ランダム仕事量でR MB /秒でデータを転送できると仮定します。一般に、SはRよりもはるかに大きい(すなわち、S >> R)。

この違いを理解するために、簡単な運動をしましょう。具体的には、次のディスク特性を考慮してSとRを計算しましょう。平均10MBのサイズの転送と平均10KBのランダム転送を仮定します。また、次のディスク特性を仮定します。  
![](../38/img/fig38_2_1.PNG)  
Sを計算するには、まず標準的な10 MB転送で時間が費やされるかどうかを調べる必要があります。まず、7ミリ秒のシークと3ミリ秒の回転を行います。最後に転送が開始されます。10 MB @ 50 MB/sの場合、転送に費やされる時間は1/5秒、つまり200ミリ秒になります。したがって、10 MB要求ごとに、要求を完了するのに210ミリ秒を費やします。Sを計算するには、次のように分割する必要があります。  
![](../38/img/fig38_2_2.PNG)  
わかるように、データ転送に時間がかかるため、Sはディスクのピーク帯域幅に非常に近くなります(シークおよびローテーションのコストは償却されます)。Rも同様に計算できます。シークと回転は同じです。転送に費やされた時間を計算します。転送時間は10KB @ 50MB / s、つまり0.195msです。  
![](../38/img/fig38_2_3.PNG)  
わかるように、Rは1 MB/sより小さく、S/Rはほぼ50です。

### Back To RAID-0 Analysis, Again
ストライピングのパフォーマンスを評価しましょう。上記のように、一般的には良いです。たとえば、レイテンシの観点からは、シングルブロック要求のレイテンシは、単一のディスクのレイテンシとほぼ同じでなければなりません。結局のところ、RAID-0は単にその要求をそのディスクの1つにリダイレクトします。

定常状態のスループットの観点からは、システムの全帯域幅を得ることが期待されます。したがって、スループットはN(ディスクの数)にS(単一ディスクの連続帯域幅)を掛けたものに等しくなります。多数のランダムI/Oに対して、すべてのディスクを再び使用することができ、N・R MB/sを得ることができます。以下に示すように、これらの値は計算するのが最も簡単で、他のRAIDレベルと比較して上限として機能します。

## 38.5 RAID Level 1: Mirroring
ストライピングを超える最初のRAIDレベルは、RAIDレベル1またはミラーリングと呼ばれます。ミラーリングされたシステムでは、システム内の各ブロックのコピーを複数作成するだけです。もちろん、それぞれのコピーは別々のディスクに置かなければなりません。これにより、ディスク障害を許容することができます。

典型的なミラーリングされたシステムでは、各論理ブロックについて、RAIDは2つの物理コピーを保持していると仮定します。次に例を示します。  
![](../38/img/fig38_3.PNG)  
この例では、ディスク0とディスク1の内容は同じで、ディスク2とディスク3も同じです。これらのミラーペアにデータがストライピングされます。実際、ディスクにブロックコピーを置く方法はいくつかあることに気が付いているかもしれません。上記の構成は一般的なもので、ミラーペア(RAID-1)、ストライプ(RAID-0)を使用するため、RAID-10または(RAID 1 + 0)と呼ばれることもあります。もう1つの一般的な構成は、RAID-01(またはRAID 0 + 1)です。この配列には、2つの大きなストライピング(RAID-0)配列と、その上にミラー(RAID-1)が含まれています。ここでは、上記のレイアウトを想定してミラーリングについて説明します。

ミラーリングされたアレイからブロックを読み取る場合、RAIDには選択肢があります。どちらのコピーも読み取ることができます。たとえば、論理ブロック5への読み取りがRAIDに対して発行された場合、ディスク2またはディスク3のいずれかから自由に読み取ることができます。ただし、ブロックを書き込む場合、そのような選択肢はありません。信頼性を維持するために両方のコピーのデータを更新します。ただし、これらの書き込みは並行して行うことができます。例えば、論理ブロック5への書き込みは、同時にディスク2および3に書き込むことができます。

### RAID-1 Analysis
RAID-1を評価してみましょう。容量の観点から、RAID-1は高価です。ミラーリングレベル= 2の場合、ピークの有効容量の半分しか得られません。BブロックのN個のディスクの場合、RAID-1の有効容量は(N・B)/ 2です。

信頼性の観点から、RAID-1はうまくいきます。いずれかのディスクの障害を許容することができます。また、RAID-1がこれよりも実際にはうまくいくかもしれないことに気がつくかもしれません。上の図では、ディスク0とディスク2の両方が故障したとします。このような状況では、データが失われることはありません！より一般的には、ミラーリングされたシステム(ミラーリングレベル2)は、特定のディスク障害を1回、失敗したディスクに応じて最大N / 2回の障害を許容することができます。実際には、私たちは一般的にこのようなことを偶然に残すことは良くないと思っています。したがって、ほとんどの人は、障害を処理するためにミラーリングが良いと考えています。

最後に、パフォーマンスを分析します。単一の読み取り要求の待ち時間の観点から、単一のディスク上の待ち時間と同じであることがわかります。RAID-1はすべてそのコピーをそのコピーの1つに転送します。書き込みは少し異なります。完了する前に2回の物理書き込みを完了する必要があります。これらの2つの書き込みは並行して行われるため、1回の書き込み時間とほぼ同じになります。ただし、論理書き込みは両方の物理書き込みが完了するまで待機する必要があるため、2つの要求の最悪の場合のシークと回転遅延が発生するため、平均して1つのディスクへの書き込みよりもわずかに高くなります。

>> ASIDE: THE RAID CONSISTENT-UPDATE PROBLEM  
>> RAID-1を分析する前に、一貫した更新の問題[DAA05]と呼ばれるマルチディスクRAIDシステムで発生する問題についてまず説明します。この問題は、単一の論理操作中に複数のディスクを更新する必要があるRAIDへの書き込みで発生します。この場合、ミラーリングされたディスク配列を検討しているものとします。  
書き込みがRAIDに発行されたとすると、RAIDはディスク0とディスク1の2つのディスクに書き込む必要があると判断します。次に、RAIDはディスク0に書き込みを発行しますが、RAIDがディスクに要求を発行する直前図1に示すように、電力損失(またはシステムクラッシュ)が発生したとします。このような不幸なケースでは、ディスク0への要求が完了したと仮定します(ただし、ディスク1へ明確に要求はされていないので、ディスク1への要求は発行されません)。  
不十分な電力損失の結果、ブロックの2つのコピーが矛盾していることがあります。ディスク0のコピーは新しいバージョンで、ディスク1のコピーは古いバージョンです。両方のディスクの状態が原子的に変化すること、すなわち、両方とも新しいバージョンとして終了するか、どちらも終了しないことである。  
この問題を解決する一般的な方法は、何らかの種類の先読みログを使用して、その前にRAIDが何をするかを最初に記録する(すなわち、2つのディスクを特定のデータで更新する)ことです。このアプローチをとることで、クラッシュが発生した場合に適切なことが起こることを保証することができます。保留中のすべてのトランザクションをRAIDにリカバリするrecovery procedure(リカバリ手順)を実行することで、2つのミラーリングされたコピー(RAID-1の場合)が同期していないことを確認できます。  
最後の注意：書き込みごとにディスクへのロギングが非常に高価であるため、ほとんどのRAIDハードウェアには、このタイプのロギングを実行する少量の不揮発性RAM(バッテリバックアップなど)が含まれています。 したがって、ディスクへの高コストのロギングを行うことなく、一貫性のあるアップデートが提供されます。

定常状態のスループットを分析するには、順次仕事量から始めましょう。シーケンシャルにディスクに書き込む場合、各論理書き込みでは2回の物理書き込みが必要です。たとえば、論理ブロック0(上の図)を書き込むと、RAIDはそれを内部的にディスク0とディスク1の両方に書き込みます。したがって、ミラーリングされたアレイへのシーケンシャル書き込み中に取得される最大帯域幅は(N / 2・S)、またはピーク帯域幅の半分である。

残念ながら、私たちはシーケンシャルリードの間に全く同じ性能を得ています。シーケンシャルな読み込みは、データの1つのコピーだけを読み込む必要があり、両方を読み込む必要がないため、よりうまくいくと考えるかもしれません。しかし、これがなぜあまり役に立たないのかを例を挙げて説明しましょう。ブロック0,1,2,3,4,5,6,7を読む必要があるとしましょう。ディスク0に0の読み取り、ディスク2に1の読み取り、ディスク1に2の読み取り、ディスク3に3の読み取りを発行します。ディスク0,2,1,3にそれぞれ4,5,6,7への読み取りを発行し続けます。すべてのディスクを利用しているため、アレイの全帯域幅を達成していると思うかもしれません。

ただし、必ずしもそうではないことを確認するには、1つのディスクが受け取る要求(たとえばディスク0)を考えてみてください。まず、ブロック0の要求を取得します。次に、ブロック4(ブロック2をスキップ)に対する要求を取得します。実際、各ディスクは1ブロックおきに要求を受け取ります。スキップされたブロック上で回転している間、クライアントに有用な帯域幅を提供していません。したがって、各ディスクはピーク帯域幅の半分しか配信しません。従って、シーケンシャルリードは、(N / 2・S)MB / sの帯域幅しか得られません。

ミラーリングされたRAIDの場合、ランダム読み取りが最適です。この場合、すべてのディスクに読み取りを配布して、可能な限りの帯域幅を確保できます。したがって、ランダム読み出しの場合、RAID-1はN・R MB/sで読み取ります。

最後に、期待どおりにランダム書き込みが実行されます。N / 2・R MB/sです。各論理書き込みは2つの物理書き込みに変わる必要があるため、すべてのディスクが使用されている間というのは、クライアントは利用可能な帯域幅の半分と認識します。論理ブロックxへの書き込みが2つの異なる物理ディスクへの2つの並列書き込みに変わったとしても、多くの小さな要求の帯域幅は、ストライピングで見たものの半分にしか達しません。すぐに利用可能な帯域幅の半分を得ることは、実際にはかなり良いことです。

## 38.6 RAID Level 4: Saving Space With Parity
ここでは、パリティと呼ばれるディスクアレイに冗長性を追加する別の方法を示します。 パリティベースのアプローチでは、容量を少なくして、ミラーリングされたシステムが支払う膨大なスペースペナルティを克服しようとします。コストはかかりません。しかし、パフォーマンスがかかります。  
![](../38/img/fig38_4.PNG)  
次に、5ディスクRAID-4システムの例を示します(図38.4)。データの各ストライプについて、そのストライプブロックの冗長情報を格納する単一のパリティブロックを追加しました。例えば、パリティブロックP1は、ブロック4,5,6、および7から計算した冗長な情報を持っています。

パリティを計算するには、ストライプからのブロックのいずれかの損失に耐えることができる数学関数を使用する必要があります。単純な関数XORがそのトリックをきちんとやります。与えられたビットの組について、それらのビットのすべてのXORは、ビットに1の偶数がある場合は0を返し、1の奇数がある場合は1を返します。例えば：  
![](../38/img/fig38_4_1.PNG)  

第1行(0,0,1,1)には2つの1(C2、C3)があり、したがってこれらの値のXORはすべて0(P)になります。同様に、第2の行には1つのC1(C1)しかないので、XORは1(P)でなければいけません。簡単な方法でこれを覚えておくことができます。つまり、任意の行の1の数が偶数(奇数でない)でなければなりません。パリティが正しいようにRAIDが維持しなければならない不変量です。

上記の例から、パリティ情報を使用して障害から回復する方法を推測することもできます。C2というラベルの付いた列がなくなったとします。どの値が列内になければならなかったかを理解するためには、その行の他のすべての値(XORされたパリティビットを含む)を読み込み、正しい答えを再構成するだけです。具体的には、列C2の最初の行の値が失われたとします(1)。その行の他の値(C0から0、C1から0、C3から1、パリティ列Pから0)を読み取ることによって、0、0、1、0の値を取得します。各行に1の偶数がある場合、欠落しているデータが何であるかを知っています。それは1です。これは、再構築がXORベースのパリティ方式でどのように機能するかです。再構成された値の計算方法にも注意してください。最初にパリティを計算したのと同じ方法で、データビットとパリティビットを一緒にXORします。

今、あなたは疑問に思っているかもしれません。これらのビットすべてを排他的論理和(XOR)といいますが、RAIDによって各ディスク上に4KB(またはそれ以上)のブロックが配置されています。パリティを計算するためにXORを複数のブロックに適用するにはどうすればよいですか？これは容易であることが判明しました。データブロックの各ビットにビット単位のXORを実行するだけです。各ビット単位のXORの結果をパリティブロックの対応するビットスロットに入れます。たとえば、サイズが4ビットのブロックがある場合(はい、これはまだ4KBブロックよりもかなり小さいですが、画像を取得する場合)、次のようになります。  
![](../38/img/fig38_4_2.PNG)  
図からわかるように、パリティは各ブロックの各ビットごとに計算され、結果はパリティブロックに配置されます。

### RAID-4 Analysis
RAID-4を分析してみましょう。容量の観点から、RAID-4は保護しているすべてのディスクグループのパリティ情報として1つのディスクを使用します。したがって、RAIDグループの私たちの有用な容量は(N - 1)・Bです。

信頼性も非常に理解しやすいです：RAID-4は1つのディスク障害を許容します。複数のディスクが失われた場合、失われたデータを再構築する方法はありません。

最後に、パフォーマンスがあります。今度は、定常状態のスループットを分析してみましょう。順次読み出しのパフォーマンスは、パリティ・ディスクを除くすべてのディスクを利用できるため、(N-1)・S MB/sのピーク実効帯域幅を提供します(簡単なケース)。

順次書き込みのパフォーマンスを理解するには、まずそれらがどのように行われているかを理解する必要があります。大量のデータをディスクに書き込む場合、RAID-4はフルストライプ書き込みと呼ばれる簡単な最適化を実行できます。たとえば、書き込み要求の一部としてブロック0,1,2,3がRAIDに送信されたとします(図38.5)。

![](../38/img/fig38_5.PNG)

この場合、RAIDはP0の新しい値を単純に計算し(ブロック0,1,2,3のXORを実行して)、上の5つのディスクにすべてのブロック(パリティブロックを含む)を並行して書き込みます(図の灰色で強調表示)。したがって、フルストライプ書き込みは、RAID-4がディスクに書き込むための最も効率的な方法です。

フルストライプ書き込みを理解したら、RAID-4でのシーケンシャル書き込みのパフォーマンスを計算するのは簡単です。実効帯域幅も(N - 1)・S MB/sです。パリティディスクは操作中に常に使用されていますが、クライアントはパフォーマンス上の利点を得られません。

さて、ランダムな読み込みのパフォーマンスを分析しましょう。上記の図からもわかるように、1ブロックのランダムな読み込みセットは、システムのデータディスク全体に分散されますが、パリティディスクには分散されません。従って、有効な性能は、(N - 1)・R MB/sです。

私たちが最後に保存したランダム書き込みは、RAID-4にとって最も興味深いケースです。上記の例でブロック1を上書きするとします。パリティブロックP0はストライプの正しいパリティ値を正確に反映しなくなります。この例では、P0も更新する必要があります。正しく更新するにはどうすればよいですか？

2つの方法があります。最初のものは、additive parity(加法パリティ)として知られています。新しいパリティブロックの値を計算するには、ストライプ内の他のすべてのデータブロックを並列に読み込み(例ではブロック0,2,3)、それらを新しいブロック(1)とXORします。結果が新しいパリティブロックになります。書き込みを完了するために、新しいデータと新しいパリティをそれぞれのディスクに同時に書き込むことができます。

この手法の問題点は、ディスクの数に応じて拡張するため、大規模なRAIDではパリティを計算するために多くの読み込みが必要になります。したがって、subtractive parity method(減法パリティ法)です。たとえば、このビット列(4データビット、1パリティ)を想像してみてください。  
![](../38/img/fig38_5_1.PNG)  
ビットC2をC2_newと呼ばれる新しい値で上書きしたいとしましょう。subtractive parity methodは3つのステップで動作します。まず、C2(C2_old = 1)と古いパリティ(P_old = 0)の古いデータを読み込みます。

次に、古いデータと新しいデータを比較します。それらが同じ場合(例えば、C2_new = C2_old)、パリティビットも同じままである(すなわち、P_new = P_old)ことがわかる。しかし、それらが異なる場合は、古いパリティビットを現在の状態の反対に、つまり(P_old == 1なら)、P_newを0に設定する必要があります。もし(P_old == 0)なら、P_newは1にセットされます。私たちはこの全体をXORできれいに表現することができます( \xor はXOR演算子です)：  
![](../38/img/fig38_5_2.PNG)  
ビットではなくブロックを扱うので、ブロック内のすべてのビット(たとえば、各ブロックの4096バイトに1バイトあたり8ビットを掛けたもの)にわたってこの計算を実行します。したがって、ほとんどの場合、新しいブロックは古いブロックとは異なるため、新しいパリティブロックも同様になります。

これで、加算パリティ方式をいつ使うのか、減算方式を使うのかを理解できるはずです。加算方式では減算方式よりも少ないI/Oを実行できるように、システムに必要なディスクの数を考える必要があります。クロスオーバーポイントは何ですか？

この性能分析のために、減算方式を使用していると仮定します。したがって、書き込みごとに、RAIDは4つの物理I/O(2回の読み出しと2回の書き込み)を実行する必要があります。RAIDに提出された多くの書き込みがあると想像してください。RAID-4は何台並行して実行できますか？理解を深めるために、RAID-4レイアウトをもう一度見てみましょう(図38.6)。  
![](../38/img/fig38_6.PNG)  
今度は、ブロック4とブロック13(図中の*でマークされている)とほぼ同時に、RAID-4に2回の小さな書き込みが行われたとします。

これらのディスクのデータはディスク0と1にあり、データの読み書きは並行して行われる可能性があります。発生する問題は、パリティディスクにあります。両方の要求は、4と13のパリティブロック1と3(+でマークされている)の関連パリティブロックを読み取らなければなりません。うまくいけば、この問題は明らかです。パリティディスクはこのタイプの仕事量の下でボトルネックです。私たちはこのように時々これをパリティベースのRAIDの小さな書き込み問題と呼んでいます。したがって、たとえデータディスクに並列にアクセスすることができたとしても、パリティディスクは並列化が実現するのを防ぎます。パリティディスクのため、システムへのすべての書き込みがシリアル化されます。

パリティディスクは論理I/Oごとに2つのI/O(1つの読み取り、1つの書き込み)を実行する必要があるためです。これらの2つのI/Oでパリティディスクのパフォーマンスを計算することで、RAID-4での小さなランダム書き込みのパフォーマンスを計算することができ、(R/2)MB/sを達成します。ランダムな小さな書き込みの下でRAID-4のスループットはひどいです。システムにディスクを追加しても改善されません。

私たちは、RAID-4のI/Oレイテンシを分析して結論づけます。あなたが今知っているように、単一の読み取り(障害がないと仮定)はただ1つのディスクにマップされているため、その待ち時間は1回のディスク要求の待ち時間と同等です。1回の書き込みのレイテンシには2回の読み取りと2回の書き込みが必要です。書き込みはできるだけ並行して行われるため、合計レイテンシは1つのディスクの約2倍です(いくつかの違いがありますが、両方の読み取りが完了するまで待ち、最悪の場合の位置決め時間を取得する必要があるためです。しかし、そのとき更新は探索コストを必要とせず、したがって、より優れた測位コストとなる可能性があります)。

## 38.7 RAID Level 5: Rotating Parity
小さな書き込み問題(少なくとも部分的に)に対処するために、Patterson、Gibson、およびKatzはRAID-5を導入しました。RAID-5は、ドライブ間でパリティブロックを回転させることを除いて、RAID-4とほぼ同じように機能します(図38.7)。  
![](../38/img/fig38_7.PNG)  
ご覧のとおり、RAID-4のパリティディスクのボトルネックを解消するために、各ストライプのパリティブロックがディスク間で回転しています。

### RAID-5 Analysis
RAID-5の分析の多くは、RAID-4と同じです。たとえば、2つのレベルの実効容量と耐障害性は同じです。シーケンシャルな読み書き性能も同様です。単一の要求(読み込みか書き込みか)のレイテンシもRAID-4と同じです。

私たちはすべてのディスクを利用できるようになったので、ランダムな読み込みパフォーマンスは少し良くなりました。最後に、ランダム書き込みパフォーマンスがRAID-4よりも大幅に向上し、要求間で並列処理が可能です。ブロック1への書き込みとブロック10への書き込みを想像してください。これは、ディスク1とディスク4(ブロック1とそのパリティ用)とディスク0とディスク2(ブロック10とそのパリティ用)に対する要求になります。したがって、彼らは並行して進めることができます。実際、多数のランダムな要求があれば、すべてのディスクを均等に使用することができると一般的に想定できます。そうであれば、小規模の書き込みの総帯域幅はN/4・R MB/sになります。4つの損失の要因は、各RAID-5書き込みが4つの合計I/O操作を生成するという事実になります。これは単純にパリティベースのRAIDを使用するコストです。

![](../38/img/fig38_8.PNG)

RAID-5は基本的にRAID-4と同じですが、ほんのわずかな場合を除いて、市場ではRAID-4をほぼ完全に置き換えています。唯一の場所は、大きな書き込み以外のことを決して実行しないことを知っているシステムにあるため、小さな書き込みの問題を完全に回避することです[HLM94]。そのような場合は、RAID-4が使用されることがあります。これは、構築がやや簡単であるためです。

## 38.8 RAID Comparison: A Summary
ここで、図38.8の単純化されたRAIDレベルの比較をまとめます。分析を簡略化するために、いくつかの詳細を省略したことに注意してください。たとえば、ミラーリングされたシステムでは、シーク時間が2シーク(各ディスクに1つ)の最大値であるため、単一のディスクに書き込む場合よりも平均シーク時間が少し長くなります。したがって、2つのディスクへのランダム書き込み性能は、一般に、1つのディスクのランダム書き込み性能よりも少し低くなります。また、RAID-4/5のパリティディスクを更新する場合、古いパリティの最初の読み取りは完全なシークと回転を引き起こす可能性がありますが、パリティの2回目の書き込みは回転だけになります。

ただし、図38.8の比較では本質的な違いを把握でき、RAIDレベルのトレードオフを理解するのに役立ちます。 レイテンシ解析では、Tを使用して、単一のディスクへの要求にかかる時間を表します。

結論として、パフォーマンスを厳密に求め、信頼性に気にしない場合は、明らかにストライピングが最適です。ただし、ランダムI/Oパフォーマンスと信頼性が必要な場合は、ミラーリングが最適です。あなたが支払うコストは容量です。容量と信頼性が主な目標である場合は、RAID5が最適です。あなたが支払うコストは小さな書き込みパフォーマンスです。最後に、シーケンシャルI/Oを常に行い、容量を最大化したい場合は、RAID-5が最適です。

## 38.9 Other Interesting RAID Issues
RAIDについて考えるときに話し合うことができる(そしておそらくそうすべき)多くの他の興味深い考えがあります。私たちが最終的に書くかもしれないものは次のとおりです。

たとえば、元のタクソノミーのレベル2と3、複数のディスク障害に耐えるレベル6など、他の多くのRAID設計があります[C + 04]。ディスクに障害が発生したときにRAIDが行うこともあります。場合によっては、故障したディスクをカバーするためのhot spare(ホットスペア)があります。また、障害発生時のパフォーマンス、障害が発生したディスクの再構築中のパフォーマンスはどうなりますか？さらに、潜在的なセクタのエラーやブロックの破損[B + 08]を考慮する、より現実的なフォルトモデルや、そのようなフォルトを処理する多くのテクニックがあります(詳細については、データの完全性の章を参照してください)。最後に、RAIDをソフトウェアレイヤーとして構築することもできます。ソフトウェアRAIDシステムは安価ですが、一貫性のあるアップデート問題[DAA05]を含む他の問題があります。

## 38.10 Summary
我々はRAIDについて議論しました。RAIDは、多数の独立したディスクを、大規模で大容量で信頼性の高い単一のエンティティに変換します。重要なのは、それは透過的に行われるため、上記のハードウェアとソフトウェアは、その変換を忘れがちです。

選択可能なRAIDレベルは数多くあり、使用するRAIDレベルはエンドユーザーにとって重要なものに大きく依存します。たとえば、ミラーリングされたRAIDはシンプルで信頼性が高く、一般的にパフォーマンスは高くなりますが、容量にコストがかかります。対照的に、RAID-5は信頼性が高く、容量面では優れていますが、仕事量に書き込みが少ない場合はパフォーマンスが非常に悪くなります。RAIDを選択し、特定の仕事量に対してそのパラメータ(チャンクサイズ、ディスク数など)を適切に設定することは困難であり、科学というよりか、もはや芸術です。

## 参考文献
[B+08] “An Analysis of Data Corruption in the Storage Stack”  
Lakshmi N. Bairavasundaram, Garth R. Goodson, Bianca Schroeder, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau  
FAST ’08, San Jose, CA, February 2008  
Our own work analyzing how often disks actually corrupt your data. Not often, but sometimes! And thus something a reliable storage system must consider.

[BJ88] “Disk Shadowing”  
D. Bitton and J. Gray  
VLDB 1988  
One of the first papers to discuss mirroring, herein called “shadowing”.  

[CL95] “Striping in a RAID level 5 disk array”  
Peter M. Chen, Edward K. Lee  
SIGMETRICS 1995  
A nice analysis of some of the important parameters in a RAID-5 disk array.  

[C+04] “Row-Diagonal Parity for Double Disk Failure Correction”  
P. Corbett, B. English, A. Goel, T. Grcanac, S. Kleiman, J. Leong, S. Sankar  
FAST ’04, February 2004  
Though not the first paper on a RAID system with two disks for parity, it is a recent and highlyunderstandable version of said idea. Read it to learn more.

[DAA05] “Journal-guided Resynchronization for Software RAID”  
Timothy E. Denehy, A. Arpaci-Dusseau, R. Arpaci-Dusseau  
FAST 2005  
Our own work on the consistent-update problem. Here we solve it for Software RAID by integrating the journaling machinery of the file system above with the software RAID beneath it.

[HLM94] “File System Design for an NFS File Server Appliance”  
Dave Hitz, James Lau, Michael Malcolm  
USENIX Winter 1994, San Francisco, California, 1994  
The sparse paper introducing a landmark product in storage, the write-anywhere file layout or WAFL file system that underlies the NetApp file server.

[K86] “Synchronized Disk Interleaving”  
M.Y. Kim.  
IEEE Transactions on Computers, Volume C-35: 11, November 1986  
Some of the earliest work on RAID is found here.  

[K88] “Small Disk Arrays - The Emerging Approach to High Performance”  
F. Kurzweil.  
Presentation at Spring COMPCON ’88, March 1, 1988, San Francisco, California Another early RAID reference.

[P+88] “Redundant Arrays of Inexpensive Disks”  
D. Patterson, G. Gibson, R. Katz.  
SIGMOD 1988  
This is considered the RAID paper, written by famous authors Patterson, Gibson, and Katz. The paper has since won many test-of-time awards and ushered in the RAID era, including the name RAID itself!

\newpage

# 39 Interlude: Files and Directories
ここまでは、CPUの仮想化であるプロセスと、メモリの仮想化であるアドレス空間の2つの主要なオペレーティングシステムの抽象化の発展を見てきました。この2つの抽象化によって、プログラムは、あたかも自分の世界であるかのように動くことができます。あたかもそれ自身のプロセッサ(または複数のプロセッサ)、自らのメモリを持っているかのように動くことができます。このような錯覚は、システムのプログラミングをはるかに容易にし、今日ではデスクトップやサーバだけでなく、携帯電話などのプログラマブルなプラットフォーム上でますます普及しています。

このセクションでは、仮想化パズルに重要な要素である永続ストレージを追加します。古典的なハードディスクドライブまたはより現代的なソリッドステートストレージデバイスなどの永続ストレージデバイスは、情報を永続的に(または少なくとも長期間)保存します。停電が発生したときに内容が失われるメモリとは異なり、永続記憶装置はそのようなデータをそのまま維持します。したがって、OSはそのようなデバイスに特別な注意を払わなければなりません。これは、ユーザーが実際に気にかけているデータを保存する場所です。

>> CRUX: HOW TO MANAGE A PERSISTENT DEVICE  
>> OSは永続的なデバイスをどのように管理すべきですか？APIとは何ですか？実装の重要な側面は何ですか？

したがって、今後のいくつかの章では、パフォーマンスと信頼性を向上させる方法に焦点を当て、永続データを管理するための重要なテクニックについて検討します。ただし、APIの概要、つまりUNIXファイルシステムと対話するときに表示されると予想されるインターフェイスについて説明します。

## 39.1 Files and Directories
ストレージの仮想化では、2つの主要な抽象概念が時間とともに発展しています。最初はファイルです。ファイルは単なる線形バイト配列です。各バイトは読み書きできます。各ファイルには、ある種の低レベルの名前が付いています。通常、いくつかの種類があります。ユーザーはこの名前を認識していないことがよくあります(次のとおりです)。歴史的な理由から、ファイルの低レベルの名前はよくそのinode番号と呼ばれます。私たちは将来の章でinodeについてもっと学びます。今のところ、各ファイルに関連付けられているiノード番号があると仮定します。

ほとんどのシステムでは、OSはファイルの構造(例えば、画像であるか、テキストファイルであるか、Cソースコードであるか)をほとんど知りません。むしろ、ファイルシステムの責任は、そのようなデータをディスクに永続的に保存し、データを再度要求するときに、最初にそこに置いたものを確実に取得することです。そうすることはそれほど単純ではありません！

2番目の抽象化はディレクトリの抽象化です。ファイルのようなディレクトリも低レベルの名前(つまり、iノード番号)を持っていますが、その内容は非常に特殊です(ユーザが読める名前、低レベルの名前)のリストを含んでいます。たとえば、低レベルの名前「10」を持つファイルがあり、ユーザーが判読可能な「foo」という名前で参照されているとします。"foo"が存在するディレクトリは、ユーザが読める名前を低レベルの名前にマッピングするエントリ("foo"、"10")を持ちます。ディレクトリ内の各エントリは、ファイルまたは他のディレクトリを参照します。ディレクトリを他のディレクトリに配置することにより、ユーザは任意のディレクトリツリー(またはディレクトリ階層)を構築することができ、その下にすべてのファイルおよびディレクトリが格納されます。

![](../39/img/fig39_1.PNG)

ディレクトリ階層はルートディレクトリ(UNIXベースのシステムではルートディレクトリは単に/と呼ばれます)から始まり、目的のファイルまたはディレクトリが指定されるまで、ある種のセパレータを使用して後続のサブディレクトリの名前を付けます。たとえば、ユーザーがルートディレクトリ/にディレクトリfooを作成し、ディレクトリfooにbar.txtというファイルを作成した場合、絶対パス名でファイルを参照できます。この場合は/foo/bar.txtです。より複雑なディレクトリツリーについては、図39.1を参照してください。この例の有効なディレクトリは/、/foo、/bar、/bar/bar、/bar/fooです、有効なファイルは/foo/bar.txtおよび/bar/foo/bar.txtです。ディレクトリとファイルは、ファイルシステムツリー内の異なる場所にある限り、同じ名前を持つことができます(たとえば、図のbar.txtという2つのファイル/foo/bar.txtと/bar/foo/bar.txtがあります)

>> TIP: THINK CAREFULLY ABOUT NAMING  
>> 命名は、コンピュータシステムの重要な側面である[SK09]。UNIXシステムでは、あなたが考えることができる事実上すべてがファイルシステムによって命名されます。ファイル、デバイス、パイプ、さらにはプロセス[K84]だけでなく、普通の古いファイルシステムのように見えます。名前の統一性により、システムの概念モデルが簡単になり、シンプルでモジュラーなシステムになります。したがって、システムまたはインターフェースを作成するときは、使用している名前について注意深く考えてください。

また、この例のファイル名には、barとtxtの2つの部分があり、ピリオドで区切られています。最初の部分は任意の名前ですが、ファイル名の2番目の部分は通常、Cコード(例：.c)であるか、画像(例：.jpg)であるか、音楽ファイル(例：.mp3)であるかなど、ファイルの種類を示すために使用されます。しかし、通常これは規約に過ぎません。通常、main.cという名前のファイルに含まれているデータは確かにCのソースコードであるという強制はありません。

したがって、ファイルシステムが提供するすばらしいことの1つがわかります。関心のあるすべてのファイルの名前を付ける便利な方法です。名前は、すべてのリソースにアクセスするための最初のステップで名前を付けることができるため、システムでは重要です。UNIXシステムでは、このように、ファイルシステムは、ディスク、USBスティック、CD-ROM、その他多くのデバイス上のファイルにアクセスする統一された方法を提供します。事実、多くのその他のものは、単一のディレクトリツリーの配下にあります。

## 39.2 The File System Interface
次に、ファイルシステムのインタフェースについて詳しく説明します。まず、ファイルの作成、アクセス、削除の基本について説明します。これは簡単だと思うかもしれませんが、途中で`unlink()`と呼ばれるファイルを削除するために使用される不思議な呼び出しを発見します。うまくいけば、この章の最後では、この謎はあなたにとって、神秘的なものではなくなるでしょう！

## 39.3 Creating Files
まず、ファイルの作成という最も基本的な操作から始めます。これは、openシステムコールで実現できます。`open()`を呼び出してO_CREATフラグを渡すことにより、プログラムは新しいファイルを作成することができます。現在の作業ディレクトリに"foo"というファイルを作成するコードの例をいくつか示します。
```c
int fd = open("foo", O_CREAT|O_WRONLY|O_TRUNC, S_IRUSR|S_IWUSR);
```
ルーチン`open()`はいくつかの異なるフラグをとります。この例では、2番目のパラメータは存在しない場合はファイル(O_CREAT)を作成し、ファイルは書き込むことのみ(O_WRONLY)ができるようにします。ファイルがすでに存在する場合は、0バイトのサイズに切り捨てて、既存のコンテンツ(O_TRUNC)を削除します。3番目のパラメータは、パーミッションを指定します。この場合、所有者がファイルを読み書き可能にします。

>> ASIDE: THE `CREAT()` SYSTEM CALL
ファイルを作成する古い方法は、次のように`creat()`を呼び出すことです。  
```c
int fd = creat("foo"); // option: パーミッションを設定するための第2フラグを追加する
```
>> `creat()`は`open()`として次のフラグで考えることができます：O_CREAT | O_WRONLY | O_TRUNCです。`open()`はファイルを作成することができるので、`creat()`の使用法は少し賛成できません(実際には`open()`へのライブラリ呼び出しとして実装することができます)。しかし、それはUNIXの知識の特別な場所を保持しています。具体的には、ケン・トンプソンがUNIXを再設計していた場合、彼が何をどうやってやるのかと質問されたとき、彼は「私はeをスペルとして作るだろう」と答えました。

`open()`の重要な点の1つは、ファイルディスクリプタです。ファイルディスクリプタは、単なる整数で、プロセスごとのプライベートなもので、ファイルにアクセスするためにUNIXシステムで使用されます。したがって、ファイルが開かれると、そのファイル記述子を使用して、そのファイルを読み書きする権限を持っているとみなします。

このようにして、ファイルディスクリプタは機能[L84]、つまり、特定の操作を実行する権限を与える不透明なハンドルです。ファイルディスクリプタを考えるもう一つの方法は、ファイル型オブジェクトへのポインタです。そのようなオブジェクトがあれば、`read()`や`write()`のようにファイルにアクセスするための他の"メソッド"を呼び出すことができます。以下に、ファイル記述子がどのように使用されるかを見ていきます。

## 39.4 Reading and Writing Files
一度私たちがいくつかのファイルを持っていれば、私たちはそれらを読み書きしたいかもしれません。既存のファイルを読むことから始めましょう。コマンドラインで入力していた場合は、プログラムcatを使用して、ファイルの内容を画面にダンプすることができます。
```
prompt> echo hello > foo
prompt> cat foo
hello
prompt>
```
このコードスニペットでは、プログラムechoの出力をファイルfooにリダイレクトします。ファイルfooには単語"hello"が含まれています。次に、catを使用してファイルの内容を表示します。しかし、catプログラムはどのようにしてファイルfooにアクセスしますか？

これを見つけるために、私たちは非常に便利なツールを使用して、プログラムによるシステムコールをトレースします。Linuxでは、このツールはstraceと呼ばれます。他のシステムにも同様のツールがあります(Macの場合はdtruss、古いUNIXの場合はtrussを参照)。straceは、プログラムの実行中にシステムコールが発生したときにそれをトレースし、画面にトレースをダンプして表示します。

>> TIP: USE STRACE (AND SIMILAR TOOLS)  
>> straceツールは、どんなプログラムであるかを確認する素晴らしい方法を提供します。これを実行することで、プログラムが作るシステムコールをトレースしたり、引数や戻りコードを調べたり、一般的に何が起きているかを知ることができます。  
また、このツールにはかなり役に立ついくつかの議論があります。例えば、-fはフォークされた子もトレースします。-tは各呼び出し時の時刻を報告します。-eはtrace = open、close、read、writeのシステムコールの呼び出しをトレースし、他のすべてを無視します。その他にもより多くの強力なフラグがあります。マニュアルページを読んで、このすばらしいツールを活用する方法を見つけてください。

straceを使用してcatが何をしているかを把握する例を示します(読みやすくするためにいくつかの呼び出しを削除しました)。  
![](../39/img/fig39_1_1.PNG)  
catが最初に行うことは、読み込み用のファイルを開くことです。私たちはこれについて注意すべき事柄をいくつか挙げておきます。最初に、ファイルはO_RDONLYフラグで示されているように、読み込み専用(書き込みではない)に開かれています。第2に、64ビットオフセットを使用します(O_LARGEFILE)。`open()`の呼び出しが成功し、値3を持つファイルディスクリプタを返します。

最初に`open()`を呼び出すと、期待どおり0またはおそらく1ではなく3が返されます。各実行中のプロセスには、すでに3つのファイルが開いています。標準入力(プロセスが入力を受け取るために読み取ることができる)、標準出力(プロセスを情報を画面にダンプするために書き込むことができます)、標準エラー(そのプロセスはエラーメッセージを書き込むことができます)があります。これらは、それぞれファイル記述子0,1,2で表されます。したがって、最初に別のファイルを開くと(上記のcatのように)、ファイル記述子3になります。

openが成功すると、catは`read()`システムコールを使用してファイルからいくつかのバイトを繰り返し読み込みます。`read()`の最初の引数はファイルディスクリプタで、ファイルシステムにどのファイルを読み込ませるかを指示します。プロセスはもちろん、一度に複数のファイルを開くことができ、したがって、ディスクリプタは、オペレーティングシステムが特定の読み取りがどのファイルを参照するかを知ることを可能にします。2番目の引数は、`read()`の結果が格納されるバッファを指します。上記のシステムコールトレースでは、straceはこの箇所の読み込み結果を表示します("hello")。3番目の引数はバッファのサイズで、この場合は4 KBです。`read()`の呼び出しも正常に戻ります。ここでは、読み込んだバイト数(今回の場合は6バイト、単語"hello"の文字は5、行末マーカーは1で合わせて6)を返します。

この時点で、straceのもう1つの興味深い結果が得られます。これは、`write()`システムコールをファイルディスクリプタ1で1回呼び出すことです。前述のように、この記述子は標準出力として知られています。プログラムのcatが意味するように画面に"hello"という単語が表示されます。しかし、それは直接`write()`を呼び出すでしょうか？おそらくはそうでしょう(高度に最適化されている場合)しかし、もしそうでなければ、どのようにcatが出力するのかというと、ライブラリルーチン`printf()`を呼び出します。`printf()`は、渡されたすべての書式設定の詳細を調べ、最終的に標準出力で書込みを呼び出して結果を画面に出力します。

次に、catプログラムはファイルからさらに多くを読み込もうとしますが、ファイルに残っているバイトがないので、`read()`は0を返し、プログラムはファイル全体を読み取ったことを知ります。したがって、プログラムは`close()`を呼び出して、ファイル"foo"が完了したことを示し、対応するファイルディスクリプタを渡します。ファイルはこのようにして閉じられ、読み込みが完了します。ファイルの書き込みは、同様の手順で行います。まず、書き込みのためにファイルが開かれた後、大きなファイルに対して`write()`システムコールが呼び出され、その後`close()`が呼び出されます。straceを使用して、あなた自身が書いたプログラムのようなファイルへの書き込みをトレースするか、ddユーティリティでトレースします(例：dd if = foo of = bar)。

## 39.5 Reading And Writing, But Not Sequentially
ここまでは、ファイルの読み書き方法について説明しましたが、アクセスはすべて順番に行われています。つまり、最初から最後までファイルを読み込んだり、最初から最後までファイルを書き出したりしています。

ただし、ファイル内の特定のオフセットを読み書きできることが便利な場合もあります。たとえば、テキスト文書上に索引を作成し、それを使用して特定の単語を検索すると、文書内のランダムな一部のオフセットから読み込まれることがあります。これを行うには、`lseek()`システムコールを使用します。ここに関数プロトタイプがあります：  
```c
off_t lseek(int fildes, off_t offset, int whence);
```
最初の引数は使い慣れたものです(ファイルディスクリプタ)。2番目の引数はオフセットで、ファイルオフセットをファイル内の特定の場所に配置します。歴史的な理由からwhenceと呼ばれる第3引数は、シークの実行方法を正確に決定します。マニュアルページから：  
![](../39/img/fig39_1_2.PNG)  
この説明からわかるように、プロセスが開く各ファイルについて、OSは「現在の」オフセットを追跡します。このオフセットは、次の読み取りまたは書き込みがファイル内の読み取りまたは書き込みの開始位置を決定します。したがって、オープンファイルの抽象化の一部は、現在のオフセットを持つことであり、現在のオフセットは2つの方法のいずれかで更新されます。第1は、Nバイトの読み出しまたは書き込みが行われるときであり、Nが現在のオフセットに加算されます。したがって、各読み取りまたは書き込みは、暗黙的にオフセットを更新します。2番目はlseekで明示的に指定されており、上で指定したオフセットを変更します。

>> ASIDE: CALLING `LSEEK()` DOES NOT PERFORM A DISK SEEK  
>> 名前のわからないシステムコール`lseek()`は、多くの学生がディスクを理解しようとしているのを混乱させ、その上にあるファイルシステムの仕組みを混乱させます。2つを混同しないでください！`lseek()`コールは、OSメモリ内の変数を変更するだけで、特定のプロセスについて、次回の読み込みまたは書き込みが開始されるオフセットを追跡します。ディスクシークは、ディスクに発行された読取りまたは書込みが最後の読取りまたは書込みと同じトラックにないときに発生し、したがってヘッドの移動が必要になります。これをさらに混乱させることは、`lseek()`を呼び出してファイルのランダムな部分を読み書きすること、そしてそれらのランダムな部分を読み書きすることが実際にはより多くのディスクシークをもたらすことです。したがって、`lseek()`を呼び出すと、今後の読み取りまたは書き込みで確実にシーク(オフセットの変更)はしますが、どんなディスクI/Oも`lseek()`で発生することはありません。

この呼び出し`lseek()`は、ディスクアームを動かすディスクのシーク操作とは何の関係もないことに注意してください。`lseek()`の呼び出しは、単にカーネル内の変数の値を変更します。I/Oが実行されるとき、ディスクヘッドがどこにあるかに応じて、ディスクは要求を実行するための実際のシークを実行してもよいし、しなくてもよいです。

## 39.6 Writing Immediately with `fsync()`
ほとんどの場合、プログラムが`write()`を呼び出すと、それはファイルシステムに伝えているだけです。このデータを将来のある時点で永続ストレージに書き込んでください。パフォーマンス上の理由から、ファイルシステムはこのような書き込みをしばらく(例えば5秒または30秒)メモリにバッファします。後の時点で、書き込みは実際に記憶装置に発行されます。呼び出し元のアプリケーションの観点からは、書き込みはすばやく完了するようであり(たとえば、`write()`呼び出し後でディスクへの書き込みの前にマシンがクラッシュするなど)、データが失われることがあります。

しかし、一部のアプリケーションでは、この最終的な保証以上のものが必要です。たとえば、データベース管理システム(DBMS)では、正しい回復プロトコルを開発するには、時々ディスクへの書き込みを強制する能力が必要です。

これらのタイプのアプリケーションをサポートするために、ほとんどのファイルシステムはいくつかの追加の制御APIを提供します。UNIXの世界では、アプリケーションに提供されるインタフェースはfsync(int fd)として知られています。特定のファイルディスクリプタのプロセスが`fsync()`を呼び出すと、ファイルシステムは、指定されたファイルディスクリプタによって参照されるファイルに対して、ディスクにすべてのダーティな(つまりまだ書き込まれていない)データを強制します。これらの書き込みがすべて完了すると、`fsync()`ルーチンが戻ります。

`fsync()`の使い方の簡単な例を次に示します。コードはファイルfooを開き、そのファイルに単一のチャンクを書き込んだ後、`fsync()`を呼び出して書き込みがディスクに強制的に強制されるようにします。`fsync()`が返されると、アプリケーションはデータが永続化されていることを知り(もし、`fsync()`が正しく実装されていれば)、安全に動かすことができます。
```c
int fd = open("foo", O_CREAT|O_WRONLY|O_TRUNC, S_IRUSR|S_IWUSR);
assert(fd > -1);
int rc = write(fd, buffer, size);
assert(rc == size);
rc = fsync(fd);
assert(rc == 0);
```
興味深いことに、このシーケンスでは、あなたが期待できるすべてを保証するものではありません。場合によっては、ファイルfooを含むディレクトリを`fsync()`する必要があります。この手順を追加すると、ファイル自体がディスク上にあるだけでなく、新しく作成された場合でも、そのファイルが永続的にディレクトリの一部になります。驚くことではないが、この種の詳細は見落とされがちで、多くのアプリケーションレベルのバグが発生します[P + 13、P + 14]。

## 39.7 Renaming Files
いったんファイルがあると、ファイルに別の名前を付けることができると便利なことがあります。コマンドラインで入力するときは、これはmvコマンドで行います。この例では、ファイルfooの名前がbarに変更されています。  
```
prompt> mv foo bar
```
straceを使用すると、mvはシステムコールrename(char * old、char * new)を使用します。これは、ファイルの元の名前(old)と新しい名前(new)の2つの引数を正確にとります。`rename()`呼び出しが提供する興味深い保証の1つは、システムクラッシュに関してアトミックな呼び出しとして(通常)実装されていることです。名前の変更中にシステムがクラッシュした場合、ファイルは古い名前または新しい名前のいずれかに名前が付けられるといった、奇妙な中間状態は発生しません。

したがって、`rename()`は、ファイル状態へのアトミックな更新を必要とする特定の種類のアプリケーションをサポートするために重要です。ここで少し具体的に話します。ファイルエディタ(emacsなど)を使用していて、ファイルの途中に行を挿入したとします。この例のファイル名はfoo.txtです。新しいファイルが元の内容と行が挿入されていることを保証するために、エディタがファイルを更新する方法は次のとおりです(単純化のためにエラーチェックを無視します)。  
![](../39/img/fig39_1_3.PNG)  
この例では、エディタは単純です。新しい名前のファイルを一時的な名前(foo.txt.tmp)で書き出し、`fsync()`でディスクに強制的に書き出し、アプリケーションが新しいファイルを特定したら、メタデータと内容がディスク上にある場合は、一時ファイルの名前を元のファイルの名前に変更します。この最後のステップでは、ファイルの古いバージョンを同時に削除しながら、新しいファイルをアトミックにスワップし、アトミックなファイルの更新を実現します。

## 39.8 Getting Information About Files
ファイルアクセス以外にも、ファイルシステムは、格納している各ファイルについてかなりの量の情報を保持することが期待されます。一般に、ファイルメタデータに関するこのようなデータを呼び出します。特定のファイルのメタデータを確認するには、`stat()`または`fstat()`システムコールを使用します。これらの呼び出しは、ファイルにパス名(またはファイルディスクリプタ)をとり、ここに示すようなstat構造体を埋めます：  
![](../39/img/fig39_1_4.PNG)  
ファイルのサイズ(バイト数)、低レベルの名前(つまり、inode番号)、所有権情報、ファイルへのアクセスまたは変更に関する情報など、各ファイルについて多くの情報が保持されていることがわかります。この情報を表示するには、コマンドラインツールstatを使用します。  
![](../39/img/fig39_1_5.PNG)  
実際には、各ファイルシステムは通常、この種の情報をinodeという構造体に保持しています。私たちがファイルシステムの実装について話すとき、inodeについてもっと学びます。今のところ、inodeはファイルシステムによって保持されている永続的なデータ構造であり、その中に上記のような情報があると考えてください。

## 39.9 Removing Files
この時点で、ファイルを作成、アクセスして、それらを順次アクセスするかどうかを知っています。しかし、どのようにファイルを削除しますか？UNIXを使ったことがあるなら、あなたはおそらく知っていると思うでしょう：プログラムrmを実行するだけです。しかし、rmがファイルを削除するために使用するシステムコールは何ですか？straceでもう一度調べてみましょう。ここでは、厄介なファイル"foo"を削除します。  
![](../39/img/fig39_1_6.PNG)  
トレースされた出力から無関係なクラフトを削除し、不思議な名前のシステムコール`unlink()`を1回だけ呼び出します。ご覧のように、`unlink()`は削除されるファイルの名前だけを受け取り、成功するとゼロを返します。しかし、これは大きなパズルにつながります。なぜこのシステムコールが「リンク解除」と呼ばれていますか？単に「削除する」または「削除する」だけではありません。このパズルへの答えを理解するためには、単にファイルだけでなくディレクトリも理解する必要があります。

## 39.10 Making Directories
ファイル以外のディレクトリ関連のシステムコールを使用すると、ディレクトリの作成、読み取り、および削除を行うことができます。ディレクトリに直接書き込むことはできません。ディレクトリの形式はファイルシステムのメタデータと見なされるため、たとえばファイル、ディレクトリ、またはその他のオブジェクト型を作成するなどして間接的にディレクトリを更新することはできます。このようにして、ファイルシステムはディレクトリの内容が常に期待どおりであることを確認します。

ディレクトリを作成するには、単一のシステムコール`mkdir()`を使用できます。mkdirプログラムを実行してfooという単純なディレクトリを作成するとどうなるかを見てみましょう。  
![](../39/img/fig39_1_7.PNG)  

>> TIP: BE WARY OF POWERFUL COMMANDS  
>> プログラムrmは、強力なコマンドの素晴らしい例を私たちに提供し、時にはあまりにも多くのパワーが悪いことになることがあります。 たとえば、たくさんのファイルを一度に削除するには、次のように入力します。  
```
prompt> rm *
```
>> ここで、*は現在のディレクトリ内のすべてのファイルと一致します。しかし、時にはディレクトリも、実際にはすべての内容を削除したいこともあります。これを行うには、rmに再帰的に各ディレクトリへの降下を指示し、その内容も削除します。  
```
prompt> rm -rf *
```
>> この小さな文字列で問題が発生するのは、ファイルシステムのルートディレクトリから偶発的にコマンドを発行し、そこからすべてのファイルとディレクトリを削除する場合です。したがって、強力なコマンドの両刃の剣を覚えておいてください。少数のキーストロークで多くの作業を行うことができますが、すばやく簡単に大きな被害を受けることができます。

このようなディレクトリが作成されると、最低限の内容しか持たないものの、"空"とみなされます。具体的には、空のディレクトリには、自身を参照するエントリとその親を参照するエントリの2つのエントリがあります。前者は「.」(ドット)ディレクトリと呼ばれ、後者は「..」(ドットドット)と呼ばれます。これらのディレクトリは、プログラムlsにフラグ(-a)を渡すことで確認できます。  
![](../39/img/fig39_1_8.PNG)  

## 39.11 Reading Directories
ディレクトリを作成したので、それを読むこともできます。確かに、それはまさにlsプログラムのことです。lsのような独自の小さなツールを書いて、それがどのように行われるかを見てみましょう。

あたかもファイルであるかのようにディレクトリを開くのではなく、代わりに新しい呼び出しを使用します。以下は、ディレクトリの内容を出力するプログラム例です。このプログラムは、`opendir()`、`readdir()`、および`closedir()`の3つの呼び出しを使用してジョブを完了させます。どのように簡単にインターフェースを作っているのかというと、単純なループを使用して一度に1つのディレクトリエントリを読み込み、ディレクトリ内の各ファイルの名前とiノード番号を出力します。  
![](../39/img/fig39_1_9.PNG)  
以下の宣言は、struct direntデータ構造内の各ディレクトリエントリ内で利用可能な情報を示しています。  
![](../39/img/fig39_1_10.PNG)  
ディレクトリは情報が軽いので(基本的に、名前をinode番号にマッピングするだけで、他の詳細もいくつかあります)、プログラムは各ファイルの`stat()`を呼び出して、それぞれの長さやその他の詳細情報を取得したいでしょう。確かに、これはあなたが-lフラグを渡したときのこととまったく同じです。そのフラグの有無にかかわらず、自身でstraceを試してみてください。

## 39.12 Deleting Directories
最後に、`rmdir()`(同じ名前のプログラムrmdirが使用する)を呼び出して、ディレクトリを削除することができます。ただし、ファイルの削除とは異なり、ディレクトリを削除する方が危険です。単一のコマンドで大量のデータを削除する可能性があるためです。したがって、`rmdir()`は、削除される前にディレクトリが空である(すなわち、"."と".."エントリのみを有する)という要件を持っています。空でないディレクトリを削除しようとすると、`rmdir()`の呼び出しは失敗します。

## 39.13 Hard Links
ここでは、ファイルシステムのツリーにエントリを作る新しい方法を理解することによって、`link()`と呼ばれるシステムコールを通して、ファイルを削除することがなぜ`unlink()`によって行われるのかという謎に戻ってきました。`link()`システムコールは古いパス名と新しいパス名の2つの引数をとります。新しいファイル名を古いファイル名に「リンク」すると、同じファイルを参照する別の方法が基本的に作成されます。この例のように、コマンドラインプログラムlnを使用してこれを行います。  
```
prompt> echo hello > file
prompt> cat file
hello
prompt> ln file file2
prompt> cat file2
hello
```
ここでは、"hello"という単語を含むファイルを作成し、そのファイル(file)を呼び出しました。その後、lnプログラムを使用してそのファイルへのハードリンクを作成します。その後、fileを開くか、file2を開くかのどちらかでファイルを調べることができます。リンクが動作する方法は、リンクを作成するディレクトリに別の名前を作成し、元のファイルの同じiノード番号(つまり、低レベルの名前)を参照するだけです。ファイルはどのようにもコピーされません。むしろ、同じファイルを参照する2つの人の名前(fileとfile2)だけを持つようになりました。ディレクトリ自体にも、各ファイルのinode番号を表示することでこれを見ることができます：
```
prompt> ls -i file file2
67158084 file
67158084 file2
prompt>
```
lsに-iフラグを渡すことで、各ファイルのiノード番号(ファイル名と同様に)を出力します。そして、あなたはリンクが実際に何をしているかを知ることができます：同じ正確なinode番号(この例では67158084)への新しい参照を作成するだけです。

これで、`unlink()`が`unlink()`と呼ばれている理由が分かり始めました。ファイルを作成すると、実際には2つのことが行われます。最初に、サイズ、ブロックがディスク上にある場所など、ファイルに関するすべての関連情報を実質的に追跡する構造体(iノード)を作成しています。次に、人が読める名前をそのファイルにリンクし、そのリンクをディレクトリに入れます。

ファイルへのハードリンクを作成した後、元のファイル名(ファイル)と新しく作成されたファイル名(ファイル2)に違いはありません。実際には、これらのファイルは、ファイルに関する基となるメタデータへのリンクにすぎず、inode番号67158084にあります。

したがって、ファイルシステムからファイルを削除するには、`unlink()`を呼び出します。上記の例では、fileという名前のファイルを削除しても問題なくファイルにアクセスできます。
```
prompt> rm file
removed ‘file’
prompt> cat file2
hello
```
これは、ファイルシステムがファイルのリンクを解除すると、iノード番号内の参照カウントをチェックするためです。この参照カウント(リンクカウントとも呼ばれる)によって、ファイルシステムは、この特定のiノードにリンクされているファイル名の数を追跡できます。`unlink()`が呼び出されると、人間が読める名前(削除されるファイル)と指定されたinode番号との間の"リンク"が削除され、参照カウントが減少します。参照カウントが0になった場合にのみ、ファイルシステムはiノードおよび関連するデータブロックも解放し、ファイルを本当に「削除」します。

もちろん`stat()`を使ってファイルの参照カウントを見ることができます。ファイルへのハードリンクを作成して削除するときの状態を見てみましょう。この例では、同じファイルへのリンクを3つ作成してから削除します。リンク数を見てください！  
![](../39/img/fig39_1_11.PNG)  

## 39.14 Symbolic Links
本当に便利なもう1つのタイプのリンクがあります。これは、シンボリックリンクまたは時にはソフトリンクと呼ばれています。ハードリンクはいくらか限定されています。つまり、ディレクトリツリーにサイクルを作成する恐れがあるため、ディレクトリには作成できません。inode番号はファイルシステム全体ではなく、特定のファイルシステム内で一意であるため、他のディスクパーティション内のファイルにハードリンクすることはできません。したがって、シンボリックリンクと呼ばれる新しいタイプのリンクが作成されました。

このようなリンクを作成するには、同じプログラムlnを使用できますが、-sフラグを使用します。 次に例を示します。  
```
prompt> echo hello > file
prompt> ln -s file file2
prompt> cat file2
hello
```
ご覧のように、ソフトリンクの作成はほとんど同じように見え、元のファイルはファイル名ファイルとシンボリックリンク名file2でアクセスできるようになりました。

しかし、この表面の類似性を超えて、シンボリックリンクは実際にはハードリンクとはかなり異なっています。最初の違いは、シンボリックリンクは実際は別の種類のファイルそのものです。私たちはすでに正規のファイルとディレクトリについて話しました。シンボリックリンクは、ファイルシステムが知っている第3のタイプです。シンボリックリンクの統計はすべてを示します：
```
prompt> stat file
... regular file ...
prompt> stat file2
... symbolic link ...
```
lsを実行すると、この事実も明らかになります。lsの出力の長い形式の最初の文字を注意深く見ると、一番左の列の最初の文字は - 通常のファイル、ディレクトリの場合はd、ソフトリンクの場合はlです。シンボリックリンクのサイズ(この場合は4バイト)と、リンクが指しているもの(fileという名前のファイル)を見ることもできます。  
![](../39/img/fig39_1_12.PNG)  
file2が4バイトである理由は、シンボリックリンクが形成される方法がリンク先ファイルのパス名をリンクファイルのデータとして保持するためです。fileというファイルにリンクしているので、リンクファイルfile2は小さい(4バイト)です。より長いパス名にリンクすると、リンクファイルが大きくなります：  
![](../39/img/fig39_1_13.PNG)  
最後に、シンボリックリンクが作成される方法のために、彼らはぶら下がった参照(dangling reference)として知られているものの可能性を残します：  
![](../39/img/fig39_1_14.PNG)  
この例のように、ハードリンクとはまったく違うので、元のファイルfileを削除するとリンクはもはや存在しないパス名を指し示します。

## 39.15 Making and Mounting a File System
これで、ファイル、ディレクトリ、特定の種類の特殊な種類のリンクにアクセスするための基本的なインターフェイスを見てきました。しかし、多くの基となるファイルシステムから完全なディレクトリツリーを組み立てる方法について、議論すべきトピックがもう1つあります。この作業は、最初にファイルシステムを作成し、マウントして内容にアクセスできるようにすることで実現します。

ファイルシステムを作るために、ほとんどのファイルシステムは、通常このタスクを実行するmkfs(make fsと発音)と呼ばれるツールを提供します。アイデアは以下の通りです：ツールに入力としてデバイス(例えば、/dev/sda1などのディスクパーティション)とファイルシステムタイプ(ext3など)を与え、空のファイルシステムを書き始めるだけです。ルートディレクトリとして、そのディスクパーティションにコピーします。mkfsによると、ファイルシステムがあります！

しかし、いったんこのようなファイルシステムが作成されると、それは一様なファイルシステムツリー内でアクセス可能にする必要があります。この作業は、マウントプログラム(実際の作業を行うための、基となるシステムコール`mount()`を呼び出す)によって実現されます。マウントとは、既存のディレクトリをターゲットマウントポイントとして使用し、その時点で新しいファイルシステムをディレクトリツリーに貼り付けることです。

ここの例は役に立つかもしれません。デバイスパーティション/dev/sda1に格納されているext3ファイルシステムがアンマウントされているとします。ルートディレクトリにはaとbという2つのサブディレクトリがあり、それぞれにfooという単一のファイルが格納されています。このファイルシステムをマウントポイント/home/usersにマウントしたいとしましょう。次のように入力します。  
```
prompt> mount -t ext3 /dev/sda1 /home/users
```
成功した場合、マウントによってこの新しいファイルシステムが利用可能になります。ただし、新しいファイルシステムへのアクセス方法に注意してください。ルートディレクトリの内容を見るには、lsを次のように使用します。  
```
prompt> ls /home/users/
a b
```
ご覧のとおり、パス名/home/users/は、新しくマウントされたディレクトリのルートを参照します。同様に、パス名が/home/users/aと/home/users/bのディレクトリaとbにアクセスできます。最後に、fooという名前のファイルに/home/users/a/fooと/home/users/b/fooからアクセスできます。そして、マウントの美しさとして、複数のファイルシステムを持つ代わりに、マウントはすべてのファイルシステムを1つのツリーに統合し、命名を統一して便利にします。

あなたのシステムに何がマウントされているのか、どのポイントにマウントされているのかを確認するには、mountプログラムを実行するだけです。次のようなものが表示されます：  
![](../39/img/fig39_1_15.PNG)  
この狂った組み合わせは、ext3(標準のディスクベースのファイルシステム)、procファイルシステム(現在のプロセスに関する情報にアクセスするためのファイルシステム)、tmpfs(一時ファイル用のファイルシステム)、AFS(分散ファイルシステム)はすべて、この1つのマシンのファイルシステムツリーにまとめられています。

## 39.16 Summary
UNIXシステム(実際にはどのシステムでも)のファイルシステムインタフェースは、一見して非常に初歩的ですが、それを習得したいかどうかを理解することはたくさんあります。もちろん、単純にそれを使用するよりも、何も良いことはありません(やることはたくさんあります)。だからしてください！もちろん、もっと読んでください。いつものように、スティーブンス[SR05]が始まる場所です。

私たちは基本的なインターフェースを見学して、うまくいかに動作するかを少しは理解していました。さらに興味深いのは、APIのニーズを満たすファイルシステムを実装する方法です。これについては、次に詳しく解説します。

# 参考文献
[K84] “Processes as Files”  
Tom J. Killian  
USENIX, June 1984  
The paper that introduced the /proc file system, where each process can be treated as a file within a pseudo file system. A clever idea that you can still see in modern UNIX systems.

[L84] “Capability-Based Computer Systems”  
Henry M. Levy  
Digital Press, 1984  
Available: http://homes.cs.washington.edu/˜levy/capabook  
An excellent overview of early capability-based systems.

[P+13] “Towards Efficient, Portable Application-Level Consistency”  
Thanumalayan S. Pillai, Vijay Chidambaram, Joo-Young Hwang, Andrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau
HotDep ’13, November 2013  
Our own work that shows how readily applications can make mistakes in committing data to disk; in particular, assumptions about the file system creep into applications and thus make the applications work correctly only if they are running on a specific file system.

[P+14] “All File Systems Are Not Created Equal:  
On the Complexity of Crafting Crash-Consistent Applications” Thanumalayan S. Pillai, Vijay Chidambaram, Ramnatthan Alagappan, Samer Al-Kiswany, Andrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau  
OSDI ’14, Broomfield, Colorado  
The full conference paper on this topic – with many more details and interesting tidbits than the first workshop paper above.  

[SK09] “Principles of Computer System Design”  
Jerome H. Saltzer and M. Frans Kaashoek  
Morgan-Kaufmann, 2009  
This tour de force of systems is a must-read for anybody interested in the field. It’s how they teach systems at MIT. Read it once, and then read it a few more times to let it all soak in.

[SR05] “Advanced Programming in the UNIX Environment”  
W. Richard Stevens and Stephen A. Rago  
Addison-Wesley, 2005  
We have probably referenced this book a few hundred thousand times. It is that useful to you, if you care to become an awesome systems programmer.

\newpage

# 40 File System Implementation
この章では、vsfs(Very Simple File System)と呼ばれる単純なファイルシステムの実装を紹介します。このファイルシステムは、一般的なUNIXファイルシステムの簡略化されたバージョンであるため、今日の多くのファイルシステムで見られる基本的なオンディスク構造、アクセス方法、およびさまざまなポリシーを紹介しています。

ファイルシステムは純粋なソフトウェアです。CPUやメモリの仮想化の開発とは異なり、ファイルシステムの一部の機能を向上させるハードウェア機能を追加することはありません(ただし、ファイルシステムがうまく機能するようにデバイスの特性に注意する必要があります)。ファイルシステムの構築には大きな柔軟性があるため、AFS(Andrew File System)[H + 88]からZFS(SunのZettabyte File System)[B07]に至るまで、さまざまなファイルシステムが構築されています。これらのファイルシステムはすべて、さまざまなデータ構造を持ち、同輩よりも良くて悪いものもあります。したがって、ファイルシステムについて学ぶ方法は、事例研究です。まず、この章では、簡単なファイルシステム(vsfs)と、実際のファイルシステムについての一連の調査を行い、どう違うのかを演習のなかで理解していきましょう。

>> THE CRUX: HOW TO IMPLEMENT A SIMPLE FILE SYSTEM  
>> 単純なファイルシステムを構築するにはどうすればよいですか？ディスクにはどのような構造が必要ですか？彼らは何を追跡する必要がありますか？彼らはどのようにアクセスされますか？

## 40.1 The Way To Think
ファイルシステムについて考えるには、通常、2つの異なる側面について考えることをお勧めします。これらの側面の両方を理解すれば、ファイルシステムが基本的にどのように動作するかを理解していると思います。

1つめはファイルシステムのデータ構造です。言い換えれば、ファイルシステムがデータとメタデータを整理するためにどのような種類のオンディスク構造が利用されていますか？SGIのXFSのようなより洗練されたファイルシステムは、より複雑なツリーベースの構造(S + 96)を使用するのに対し、最初のファイルシステム(以下のvsfsを含む)はブロック配列やその他のオブジェクトのような単純な構造を採用しています。

>> ASIDE: MENTAL MODELS OF FILE SYSTEMS  
前に説明したように、メンタルモデルは、システムについて学ぶときに実際に開発しようとしているものです。ファイルシステムでは、質問の中に答えがあることがあります。例えば、ファイルシステムのデータとメタデータを格納するディスク上の構造はどうなりますか？プロセスがファイルを開くとどうなりますか？どのオンディスク構造が読み書き中にアクセスされるのですか？などです。メンタルモデルの作成と改善によって、ファイルシステムのソースコードの詳細を理解しようとする代わりに、何が起きているのかを抽象的に理解することができます。(もちろん、ソースコードを理解することも大事です)

ファイルシステムの第2の側面は、そのアクセス方法です。`open()`、`read()`、`write()`などのプロセスによって呼び出された呼び出しをその構造にどのようにマップしますか？特定のシステムコールの実行中に読み取られる構造はどれですか？どれに書きこまれますか？これらのステップはどれくらい効率的に実行されますか？

ファイルシステムのデータ構造とアクセス方法を理解している場合、システムの考え方の重要な部分である、それが本当にどのように機能するかについての良いメンタルモデルを開発しました。私たちの最初の実装を掘り下げながら、メンタルモデルの開発に取り掛かりましょう。

## 40.2 Overall Organization
現在、ディスク上のvsfsファイルシステムのデータ構造の全体的な構成を開発しています。まず、ディスクをブロックに分割する必要があります。単純なファイルシステムはブロックサイズを1つしか使用しません、そして今からそれを行います。一般的に使用される4 KBのサイズを選択しましょう。

そのため、ファイルシステムを構築するディスクパーティションについては、サイズが4KBの一連のブロックが簡単です。ブロックは、サイズNの4KBブロックのパーティション内で、0からN-1までアドレス指定されます。実際には64ブロックの非常に小さなディスクがあるとします。  
![](../40/img/fig40_1_1.PNG)  
ファイルシステムを構築するためにこれらのブロックに格納する必要があるものについて考えてみましょう。もちろん、最初に気になるのはユーザーデータです。実際には、どのファイルシステムのほとんどの領域もユーザーデータです(そして、そうでなければなりません)。ユーザーデータ用に使用するディスクの領域をデータ領域と呼びましょう。また、簡単にするために、ディスク上の固定された部分、たとえばディスク上の64個のブロックのうちの後ろの56個を予約します。  
![](../40/img/fig40_1_2.PNG)  
最後の章で学んだように、ファイルシステムは各ファイルに関する情報を追跡する必要があります。この情報は、メタデータの重要な部分であり、ファイル、ファイルのサイズ、所有者とアクセス権、アクセスと変更時刻、およびその他の同様の種類の情報を含むデータブロック(データ領域内)のようなものを追跡します。この情報を格納するために、ファイルシステムは通常、inodeと呼ばれる構造を持っています(以下のinodeについてさらに詳しく説明します)。

iノードに対応するためには、ディスク上にもスペースを確保する必要があります。ディスクのこの部分をiノード表と呼びましょう。これは単にディスク上のiノードの配列を保持しています。このように、ディスク上のイメージは、図のようになります。ここでは、私たちの64個のブロックのうちの5個をiノードに使用していると仮定しています(Iで示されています)。  
![](../40/img/fig40_1_3.PNG)  
inodeは通常128または256バイトのように大きくないことに注意してください。inode当たり256バイトと仮定すると、4KBのブロックは16個のinodeを保持でき、上記のファイルシステムは80個の合計inodeを含んでいます。この私たちの小さな64ブロックのパーティションに構築されたシンプルなファイルシステムでは、この数字はファイルシステムに保存できるファイルの最大数を表します。ただし、より大きなディスク上に構築された同じファイルシステムでは、より大きなinodeテーブルを割り当てるだけで、より多くのファイルに対応できるということに注意してください。

これまでの私たちのファイルシステムは、データブロック(D)とinode(I)を持っていますが、まだいくつか欠けています。あなたが推測したように、依然として必要な1つの主要なコンポーネントは、inodeやデータブロックが空いているか割り当てられているかを追跡する方法です。したがって、このような割り当て構造は、どのファイルシステムにおいても必須の要素です。

もちろん、多くの割り当て追跡方法が可能です。たとえば、最初の空きブロックを指す空きリストを使用して、次の空きブロックを指し示すことができます。代わりに、データ領域(データビットマップ)とiノードテーブル(iノードビットマップ)用のビットマップという単純で一般的な構造を選択します。ビットマップは、単純な構造です。各ビットは、対応するオブジェクト/ブロックが空き(0)か使用中(1)かを示すために使用されます。そして、私たちの新しいディスク上のレイアウトには、inodeビットマップ(i)とデータビットマップ(d)があります。  
![](../40/img/fig40_1_4.PNG)  
これらのビットマップには4 KBのブロック全体を使用するのはちょっと残念ですが、 そのようなビットマップは32Kオブジェクトが割り当てられているかどうかを追跡できますが、80個のinodeと56個のデータブロックしか持っていません。 しかし、単純化のためにこれらのビットマップのそれぞれに4 KBのブロック全体を使用しています。

慎重な読者(まだ起きている読者)は、非常に単純なファイルシステムのオンディスク構造の設計に1ブロック残っていることに気づいているかもしれません。私たちはスーパーブロックのためにこれを予約しています、下図のSで示されています。スーパーブロックには、この特定のファイルシステムに関する情報が含まれます。たとえば、ファイルシステム内にいくつのiノードとデータブロックがあるか(この場合はそれぞれ80と56)、iノードテーブルが開始する場所(ブロック3)などがあります。おそらく、ファイルシステムの種類(この場合はvsfs)を特定するための何らかのマジックナンバーも含まれています。  
![](../40/img/fig40_1_5.PNG)  
したがって、ファイルシステムをマウントするとき、オペレーティングシステムはまずスーパブロックを読み取り、さまざまなパラメータを初期化し、ボリュームをファイルシステムツリーにアタッチします。ボリューム内のファイルにアクセスすると、システムは必要なオンディスク構造を探す場所を正確に認識します。

## 40.3 File Organization: The Inode
ファイルシステムの最も重要なオンディスク構造の1つは、iノードです。事実上すべてのファイルシステムは、これと同様の構造を持っています。名前inodeは、索引ノードの略語であり、UNIX[RT74]やこれまでのシステムで使用されていた歴史的な名前で、簡単なシステムかもしれません。これらのノードはもともと配列に配列されていたために使用され、配列は特定のinodeにアクセスする際に索引付けされます。

>> ASIDE: DATA STRUCTURE — THE INODE  
>> inodeは、長さ、パーミッション、および構成ブロックの位置など、特定のファイルのメタデータを保持する構造を記述するために、多くのファイルシステムで使用される汎用名です。その名前は、少なくともUNIXまで(そしておそらく、以前のシステムではないにしてもMulticsに戻ります)戻ります。inode番号がディスク上のinodeの配列にインデックスを付けてその番号のiノードを見つけるために使用されるので、インデックスノードの略です。ここからわかるように、inodeの設計はファイルシステム設計の重要な部分の1つです。現代のシステムのほとんどは、追跡しているすべてのファイルに対してこのような構造を持っていますが、呼び方は違います(dnode、fnodeなどの別のものを呼び出すことがあります。)

各iノードは暗黙のうちにファイルの低レベルの名前と呼ばれる番号(inumber)で暗黙的に参照されます。vsfs(およびその他の単純なファイルシステム)では、inumber(i番号)が与えられているので、対応するinodeがディスク上のどこにあるかを直接計算することができます。例えば、上記のようなvsfsのinodeテーブルを取ると、20 KBのサイズ(5つの4 KBブロック)が得られ、80個のiノードから構成されます(各iノードが256バイトであると仮定します)。さらにinode領域が12KBから始まると仮定する(すなわち、スーパーブロックが0KBから始まり、inodeビットマップがアドレス4KBにあり、データビットマップが8KBであり、したがってinodeテーブルが直後に来ると仮定する)。したがって、vsfsでは、ファイルシステムパーティションの先頭に、次のようなレイアウトがあります。  
![](../40/img/fig40_1_6.PNG)  
inode番号32を読み込むには、ファイルシステムは最初にinode領域(32・sizeof(inode)または8192)へのオフセットを計算し、それをディスク上のinodeテーブルの開始アドレス(inodeStartAddr = 12KB)に追加し、したがって、望んだinodeブロックの正しいバイトアドレス20KBに達します。ディスクはbyte addressable(バイト・アドレス可能)ではなく、多数のaddressable(アドレス可能)なセクタ、通常は512バイトで構成されていることを思い出してください。したがって、inode32を含むinodeのブロックをフェッチするために、ファイルシステムは、望んだinodeブロックをフェッチするために、セクタ(20×1024) / 512すなわち40への読み出しを発行します。より一般的には、iノードブロックのセクタアドレスiaddrは、以下のように計算することができます。  
```c
blk = (inumber * sizeof(inode_t)) / blockSize;
sector = ((blk * blockSize) + inodeStartAddr) / sectorSize;
```
それぞれのinodeの中には、ファイルの種類(通常ファイル、ディレクトリなど)、ファイルのサイズ、割り当てられたブロック数、保護情報(ファイルの所有者、アクセス権)、ファイルが作成、変更、または最後にアクセスされた時を含むいくつかの時間情報、およびそのデータブロックがディスク上のどこに存在するかに関する情報(例えば、何らかのポインタ)といったファイルに関するすべての必要な情報を含んでいます。このようなファイルに関するすべての情報をメタデータといいます。実際、純粋なユーザーデータではないファイルシステム内の情報は、よくそのように呼ばれます。ext2 [P09]のiノードの例を図40.1に示します。

![](../40/img/fig40_1.PNG)  

inodeの設計における最も重要な決定の1つは、データブロックがどこにあるかをどのように参照するかです。1つの簡単なアプローチは、iノード内に1つ以上の直接ポインタ(ディスクアドレス)を持つことです。各ポインタは、ファイルに属する1つのディスクブロックを参照します。そのようなアプローチは限定されています。たとえば、実際に大きなファイル(ブロックのサイズに直接ポインタの数を掛けたものよりも大きい)を作成する場合は、不運です。

### The Multi-Level Index
より大きいファイルをサポートするために、ファイルシステム設計者はinode内に異なる構造を導入しなければなりませんでした。1つの一般的な考え方は、間接ポインタと呼ばれる特別なポインタを持つことです。ユーザーデータを含むブロックを指すのではなく、より多くのポインターを含むブロックを指し、各ポインターはユーザーデータを指します。したがって、iノードは、いくつかの固定数の直接ポインタ(例えば、12個)を修正して、および単一の間接ポインタを持つようにします。ファイルが十分に大きくなると、(ディスクのデータブロック領域から)間接ブロックが割り当てられ、間接ポインタのためのinodeのスロットは、それを指すように設定されます。 4KBのブロックと4バイトのディスクアドレスを想定すると、さらに1024個のポインタが追加されます。ファイルは(12 + 1024)・4Kまたは4144KBになります。

>> TIP: CONSIDER EXTENT-BASED APPROACHES  
別のアプローチは、ポインタの代わりにエクステントを使うことです。エクステントは単にディスクポインタと長さ(ブロック単位)です。したがって、ファイルの各ブロックにポインタを必要とする代わりに、ファイルのディスク上の場所を指定するポインタと長さが必要です。ファイルを割り当てるときにディスク上の空き領域が連続しているのを見つけにくい場合があるため、一つ(ブロック単位)のエクステントが制限されています。したがって、エクステントベースのファイルシステムでは、二つ以上(ブロック単位)のエクステントが許可されることが多く、ファイルの割り当て中にファイルシステムに自由度が増します。  
2つのアプローチを比較すると、ポインタベースのアプローチが最も柔軟ですが、ファイルごとに大量のメタデータを使用します(特に大きなファイルの場合)。エクステントベースのアプローチは柔軟性は低くなりますが、コンパクトです。特に、ディスク上に十分な空き領域があり、ファイルを連続して配置できる(実際にはどのようなファイル割り当て方針の目標でもあります)場合にうまく機能します。

驚くべきことではありませんが、このようなアプローチでは、さらに大きなファイルをサポートしたいことがあります。これを行うには、別のポインタinodeを追加してください：二重間接ポインタです。このポインタは、間接ブロックへのポインタを含むブロックを指し、各ブロックはデータブロックへのポインタを含みます。したがって、間接的な二重ブロックは、1024×1024または100万個の4KBブロックを追加してファイルを拡張する可能性、つまり4GBを超えるファイルの追加をサポートすることができるでしょう。あなたはさらに大きいファイルを追加することを望むかもしれません。そして、これがどこに向かうのかを知っています：トリプル間接ポインタです。

全体として、このimbalanced tree(不均衡なツリー)は、ファイルブロックを指すためのマルチレベルインデックスアプローチと呼ばれています。例えば、12個の直接ポインタ、1個の間接ブロック、2個の間接ブロックの三つが合わさった例を調べてみましょう。4KBのブロックサイズと4バイトのポインタを仮定すると、この構造は4GBをちょうど上回るファイル(すなわち、(12 + 1024 + 1024 ^ 2)×4KB)を収容することができます。トリプル間接ブロックを追加することで、どれだけのファイルを扱うことができるのか分かりますか？(ヒント：かなり大きい)

多くのファイルシステムでは、Linux ext2 [P09]やext3、NetAppのWAFLなどの一般的に使用されているファイルシステムや元のUNIXファイルシステムなど、マルチレベルのインデックスを使用しています。SGI XFSやLinux ext4などの他のファイルシステムでは、単純なポインタの代わりにエクステントを使用します。エクステントベースのスキームがどのように機能するかの詳細については、前のセクションを参照してください(仮想メモリの議論のセグメントに似ています)。

あなたは疑問に思うかもしれません：なぜこのようなimbalanced tree(不均衡なツリー)を使用しますか？どうして別のアプローチはありませんか？ さて、多くの研究者が、ファイルシステムとその使用方法、そして何十年にもわたって一定の「真実」を見つける研究してきました。そのような発見の1つは、ほとんどのファイルが小さいことです。この不均衡なデザインはそのような現実を反映しています。ほとんどのファイルが実際に小さい場合、この場合に最適化することは理にかなっています。したがって、少数の直接ポインタ(12が典型的な数)では、inodeは直接的に48KBのデータを指し示すことができ、大きなファイルに対しては1つ(またはそれ以上)の間接ブロックが必要です。最近の研究Agrawal et.al [A + 07]については、図40.2にその結果をまとめています。

>> ASIDE: LINKED-BASED APPROACHES  
inodeを設計するもう一つの簡単な方法は、linked listを使うことです。したがって、inode内では、複数のポインタを持つ代わりに、ファイルの最初のブロックを指すポインタが必要です。大きなファイルを処理するには、そのデータブロックの末尾に別のポインタを追加するなどして、大きなファイルをサポートすることができます。  
あなたが推測したように、リンクされたファイルの割り当ては、一部の仕事量ではうまく機能しません。たとえば、ファイルの最後のブロックを読み込むことや、ランダムアクセスを行うことなどについて考えてみてください。したがって、リンクされた割り当てをより良くするために、一部のシステムでは、データブロック自体に次のポインタを格納するのではなく、リンク情報のメモリ内テーブルを保持します。このテーブルは、データブロックDのアドレスによって索引付けされます。エントリの内容は、単にDの次のポインタ、すなわちDに続くファイル内の次のブロックのアドレスです。NULLもそこに入ります。このNULLは、ファイルの終わり、または特定のブロックがフリーであることを示します。そのような次のポインタのテーブルを持つことにより、リンクされた割り当てスキームは、最初に(メモリ内の)テーブルをスキャンして目的のブロックを見つけて、それを直接(ディスク上に)アクセスすることによって、ランダムなファイルアクセスを効果的に行うことができます。  
そのようなテーブルはよい方法に思えるでしょうか？我々が説明したことは、ファイルアロケーションテーブル、すなわちFATファイルシステムと呼ばれるものの基本的な構造です。これは、NTFS [C94]より前のこの古典的な古いWindowsファイルシステムは、単純なリンクベースの割り当て方式に基づいています。標準的なUNIXファイルシステムとの相違点もあります。例えば、iノード自体はなく、ファイルに関するメタデータを格納し、前記ファイルの最初のブロックを直接参照するディレクトリエントリであり、ハードリンクの作成を不可能にします。これらの詳細については、Brouwer [B02]を参照してください。

もちろん、inode設計の空間では、他にも多くの可能性があります。結局のところ、inodeは単なるデータ構造であり、関連する情報を格納し、それを効果的に参照できるデータ構造であれば十分です。ファイルシステムソフトウェアは容易く変更されるので、仕事量やテクノロジが変化した場合、さまざまな設計を検討してください。

![](../40/img/fig40_2.PNG)  

## 40.4 Directory Organization
vsfs(多くのファイルシステムのように)では、ディレクトリは単純な構成です。ディレクトリは基本的に(エントリ名、iノード番号)のペアのリストを含んでいます。特定のディレクトリ内のファイルまたはディレクトリごとに、そのディレクトリのデータブロックに文字列と数字があります。各文字列には、長さもあります(可変サイズの名前を仮定)。

たとえば、ディレクトリdir(iノード番号5)に3つのファイル(foo、bar、およびfoobar_is_a_pretty_longname)があり、それぞれinode番号12,13,24であるとします。dirのディスク上のデータは次のようになります。  
![](../40/img/fig40_2_1.PNG)  
この例では、各エントリには、inode番号、レコード長(名前の合計バイト数、残り余白の合計バイト数)、文字列の長さ(実際の名前の長さ)、および最後にエントリの名前があります。各ディレクトリに2つの余分なエントリがあることに注意してください。"."(ドット)と".."(ドットドット)です。ドットディレクトリは現在のディレクトリ(この例ではdir)ですが、ドットドットは親ディレクトリ(この場合はルート)です。

ファイルを削除すると(例えば`unlink()`を呼び出す)、ディレクトリの途中に空きスペースが残る可能性があります。そのため、(ゼロなどの予約済みのiノード番号などで)同様にマークする方法が必要です。このような削除は、レコードの長さが使用される理由の1つです。より大きなエントリや余分なスペースがあった場合、新しいエントリは古いエントリを再利用し、使用する可能性があります。

正確にディレクトリがどこに格納されているのか疑問に思うかもしれません。よくファイルシステムはディレクトリを特別なタイプのファイルとして扱います。したがって、ディレクトリにはinodeテーブルのどこかにinodeがあります(inodeのtypeフィールドは"regular file"ではなく"directory"とマークされています)。ディレクトリには、inodeが指すデータブロック(およびおそらく間接ブロック)があります。これらのデータブロックは、シンプルなファイルシステムのデータブロック領域に存在します。したがって、ディスク上の構造は変わりません。

また、ディレクトリエントリのこの単純な線形リストは、そのような情報を格納する唯一の方法ではないことに再度注意する必要があります。前述のように、任意のデータ構造が可能です。たとえば、XFS [S + 96]はディレクトリをBツリー形式で保存し、全体をスキャンする必要がある単純なリストを持つシステムよりも高速にファイルを作成する操作します(ただし、ファイル名を作成する前にファイル名が使用されていないことを保証する必要があります)。

## 40.5 Free Space Management
ファイルシステムは、inodeとデータブロックが空いているかどうかを追跡する必要があります。もし見つからなかったとき、新しいファイルやディレクトリに割り当てるような領域を見つける必要があります。したがって、空き領域の管理はすべてのファイルシステムにとって重要です。vsfsには、このタスクのための2つの単純なビットマップがあります。

>> ASIDE: FREE SPACE MANAGEMENT  
>> 空き領域を管理する多くの方法があります。ビットマップは単なる1つの方法です。一部の初期のファイルシステムでは、スーパーブロック内の単一のポインタが最初の空きブロックを保持していました。その空きブロックの中で次の空きブロックのポインタを持ち、システムの空きブロックを介してリストが形成されていました。ブロックが必要になったときに、ヘッドブロックが使用され、それに応じてリストが更新されていました。  
最新のファイルシステムは、より洗練されたデータ構造を使用します。たとえば、SGIのXFS [S + 96]は、ディスクのどのチャンクが空いているかをコンパクトに表現するために、何らかの形のBツリーを使用します。どのようなデータ構造でも、異なるタイムスペースのトレードオフが可能です。

たとえば、ファイルを作成する場合、そのファイルにiノードを割り当てる必要があります。したがって、ファイルシステムは、空いているinodeのビットマップを検索し、それをファイルに割り当てます。ファイルシステムはinodeを使用中で(1で)マークし、最終的にディスク上のビットマップを正しい情報で更新する必要があります。同様の一連のアクティビティが、データブロックが割り当てられるときに行われます。

新しいファイルにデータブロックを割り当てるときには、他のいくつかの考慮事項が有効になります。たとえば、ext2やext3などのLinuxファイルシステムの中には、新しいファイルが作成され、データブロックが必要なときに解放される一連のブロック(たとえば8)があります。そのような一連の空きブロックを見つけて新しく作成したファイルに割り当てることで、ファイルシステムはファイルの一部がディスク上で連続していることを保証し、パフォーマンスを向上させます。したがって、このような事前割り当てポリシーは、データブロックのためのスペースを割り当てる際に一般的に使用されるヒューリスティックです。

## 40.6 Access Paths: Reading and Writing
ファイルとディレクトリがディスクにどのように格納されているかを知ったので、ファイルの読み書きの動作中に操作の流れに沿うことができます。したがって、このアクセスパスで何が起こるかを理解することは、ファイルシステムの仕組みを理解する上での第2の鍵です。注意を払いましょう！

以下の例では、ファイルシステムがマウントされており、スーパーブロックがすでにメモリに入っていると仮定します。他のすべてのもの(つまり、inode、ディレクトリ)はまだディスク上にあります。

### Reading A File From Disk
この簡単な例では、まずファイル(例：/foo/bar)を開いて読み込み、それを閉じたいとします。この単純な例では、ファイルのサイズがちょうど4KB(つまり1ブロック)であると仮定します。

open("/foo/bar", O_RDONLY)呼び出しを発行するとき、ファイルシステムは最初にbarファイルのinodeを見つけ、ファイルに関する基本情報(アクセス許可情報、ファイルサイズなど)を取得する必要があります。そのためには、ファイルシステムはinodeを見つけることができなければなりませんが、今のところ完全なパス名しか存在しません。ファイルシステムはパス名を探し(トラバーサル)、目的のiノードを特定する必要があります。

![](../40/img/fig40_3.PNG)  

すべてのトラバーサルは、/と呼ばれるルートディレクトリのファイルシステムのルートから始まります。したがって、FSがディスクから読み込む最初のことは、ルートディレクトリのiノードです。しかし、このinodeはどこですか？iノードを見つけるには、そのi number(i番号)を知っていなければなりません。通常、ファイルまたはディレクトリのi number(i番号)は親ディレクトリにあります。ルートには親がありません(定義によって)。したがって、ルートのinode番号は「よく知られている」必要があります。FSは、ファイルシステムがマウントされているときに、その内容を知る必要があります。ほとんどのUNIXファイルシステムでは、ルートのiノード番号は2です。したがって、プロセスを開始するために、FSはiノード番号2(最初のiノードブロック)を含むブロックを読み取ります。

inodeが読み込まれると、FSは内部を調べて、ルートディレクトリの内容を含むデータブロックへのポインタを見つけ出すことができます。したがって、FSはこれらのディスク上のポインタを使用してディレクトリを読み込みます。この場合、fooのエントリを探します。1つまたは複数のディレクトリデータブロックを読み込むことによって、fooのエントリが見つかります。一度検出されると、FSは、次に必要となるfooのiノード番号(44と言う)も検出します。

次のステップは、目的のinodeが見つかるまでパス名を再帰的に走査することです。この例では、FSはfooのiノードを含むブロックとそのディレクトリデータを読み取り、最終的にbarのiノード番号を検出します。`open()`の最後のステップは、barのinodeをメモリに読み込むことです。FSは最終的なパーミッションチェックを行い、このプロセスのファイルディスクリプタをプロセス単位のオープンファイルテーブルに割り当て、それをユーザに返します。

オープンすると、プログラムは`read()`システムコールを発行してファイルから読み込むことができます。最初の読み込み(`lseek()`が呼び出されていなければオフセット0)は、ファイルの最初のブロックを読み込み、inodeを参照してそのようなブロックの位置を探します。新しい最後のアクセス時間でiノードを更新することもできます。この読み込みは、このファイル記述子のメモリ内オープンファイルテーブルをさらに更新し、次の読み込みが第2のファイルブロックなどを読み込むようにファイルオフセットを更新します。

>> ASIDE: READS DON’T ACCESS ALLOCATION STRUCTURES  
>> 私たちは、多くの学生がビットマップなどの割り当て構造によって混乱するのを見てきました。特に、ファイルを読み込んで、新しいブロックを割り当てないとき、ビットマップに引き続き参照されていると多くの人は考えていました。しかし、それは本当ではありません。ビットマップなどの割り当て構造は、割り当てが必要な場合にのみアクセスされます。inode、ディレクトリ、および間接ブロックには、読み取り要求を完了するために必要なすべての情報が含まれています。つまり、inodeが既にそれ(割り当てられているブロック)を指しているときに、ブロックが割り当てられていることを確認する必要はありません。

ある時点でファイルが閉じられます。ここでやるべき仕事ははるかに少ない。明らかに、ファイルディスクリプタの割り当てを解除する必要がありますが、今のところそれはFSが本当に必要とするものです。ディスクI/Oは行われません。

このプロセス全体を図40.3に示します(時間が長くなる)。この図では、openによってファイルのinodeを最終的に見つけるために多数の読み込みが行われます。その後、各ブロックを読み込むには、まずファイルシステムがinodeを参照してからブロックを読み込み、次にinodeの最終アクセス時のフィールドを書き込みで更新する必要があります。時間をかけて、何が起こっているのか理解しましょう。

また、オープンによって生成されるI/Oの量は、パス名の長さに比例することにも注意してください。パス内の追加ディレクトリごとに、そのinodeとそのデータを読み込む必要があります。これを悪化させると大きなディレクトリが存在することになります。ここでは、ディレクトリの内容を取得するために1つのブロックだけを読み取る必要がありますが、大きなディレクトリでは、目的のエントリを見つけるために多くのデータブロックを読み取る必要があります。つまり、ファイルを読むときにかなり悪くなることがあります。あなたが見つけたいと思うように、ファイルを書き出す(そして特に新しいものを作る)ことはさらに悪いことです。

### Writing to Disk
ファイルへの書き込みも同様のプロセスです。まず、ファイルを開く必要があります(上記のように)。次に、アプリケーションは新しいコンテンツでファイルを更新するために`write()`呼び出しを発行できます。最後に、ファイルが閉じられます。

読み込みとは異なり、ファイルへの書き込みはブロックを割り当てることもできます(たとえば、ブロックが上書きされている場合を除きます)。新しいファイルを書き出すとき、各書き込みはディスクにデータを書き込むだけでなく、どのブロックをファイルに割り当てるかを最初に決定し、それに応じてディスクの他の構造(例えば、データビットマップおよびiノード)を更新する必要がある。したがって、ファイルへの各書き込みは、論理的に5つのI/Oを生成します.1つはデータビットマップを読み込み(新しく割り当てられたブロックを使用するように更新する)、1つはビットマップを書き込み(新しい状態をディスクに反映させる)、2つ以上のブロック読み込んだ後にinode(新しいブロックの場所で更新されます)に書き込み、最後に実際のブロック自体を書き込む1つのブロックです。

書き込みトラフィックの量は、ファイル作成などの単純で一般的な操作を考慮するとさらに悪化します。ファイルを作成するには、ファイルシステムはiノードを割り当てるだけでなく、新しいファイルを含むディレクトリ内に領域を割り当てる必要があります。inodeビットマップへの読み込み(空きinodeを見つける)、inodeビットマップへの書き込み(割り当て済みとしてマークする)、新しいinode自体への書き込み(それの初期化)、ディレクトリのデータに1つ(ファイルの高いレベルの名前をそのiノード番号にリンクするため)、ディレクトリのinodeを読み書きして更新をします。新しいエントリを収容するためにディレクトリを拡張する必要がある場合、追加のI/O(すなわち、データビットマップおよび新しいディレクトリブロック)も必要になります。これら全てはただファイルを作成するだけのためです。

ファイル/foo/barが作成され、3つのブロックが書き込まれる特定の例を見てみましょう。図40.4は、`open()`(ファイルを作成する)と3回の4KB書き込みがそれぞれの間に何が起こるかを示しています。

![](../40/img/fig40_4.PNG)

この図では、ディスクへの読み込みと書き込みがグループ化され、その中でシステムコールが発生したときに発生します。発生する可能性のある大雑把な順序は、図の上から下に向かっています。ファイルを作成するにはどれくらいの作業が必要ですか？この場合は10 I/Oです、パス名を探して最終的にファイルを作成します。また、各割り当て書き込みには5つのI/Oが必要であることがわかります。つまり、iノードを読み込んで更新するペアと、データビットマップを読み込んで更新するペアと、最後にデータ自体の書き込みです。どのようにしてファイルシステムはこれを妥当な効率で達成できますか？

>> THE CRUX: HOW TO REDUCE FILE SYSTEM I/O COSTS  
>> ファイルのオープン、読み取り、書き込みなどの操作が最も単純な場合でも、ディスク上に散在する膨大なI/O操作が発生します。非常に多くのI/Oを実行するための高いコストを削減するために、ファイルシステムは何をすることができますか？

## 40.7 Caching and Buffering
上記の例に示すように、ファイルの読み取りと書き込みは高価になり、(遅い)ディスクに対して多くのI/Oが発生します。明らかに大きなパフォーマンス上の問題を改善するために、ほとんどのファイルシステムは、システムメモリ(DRAM)を積極的に使用して重要なブロックをキャッシュします。

キャッシュを使用しない上記の例を想像してみましょう。キャッシュを使用しないと、すべてのファイルを開くには、ディレクトリ階層内の各レベル(少なくとも1つはそのディレクトリのinodeを読み込み、データを読み込むには少なくとも2回)が必要です。長いパス名(たとえば、1/2/3/ ... /100/file.txt)では、ファイルシステムは文字通りファイルを開くために何百もの読み込みを実行します。

初期のファイルシステムでは、一般的なブロックを保持するために固定サイズのキャッシュが導入されました。仮想メモリの議論のように、LRUやさまざまなバリアントなどの戦略によって、どのブロックをキャッシュに保持するかが決定されます。この固定サイズのキャッシュは通常、起動時にメモリの約10％になるように割り当てられます。

しかし、このメモリの静的なpartitioningは無駄です。ファイルシステムが特定の時点で10％のメモリを必要としない場合はどうなりますか？上述した固定サイズのアプローチでは、ファイルキャッシュ内の未使用ページを他の用途のために再利用することができず、したがって無駄になる。これとは対照的に、現代のシステムでは、動的partitioning手法が採用されています。具体的には、最新のオペレーティングシステムの多くは、仮想メモリページとファイルシステムページを統合ページキャッシュ[S00]に統合しています。このようにして、与えられた時間に多くのメモリを必要とするものに応じて、仮想メモリとファイルシステムの間でより柔軟にメモリを割り当てることができます。

今度は、キャッシュを使ったファイルオープンの例を想像してみましょう。最初のオープンでは、ディレクトリのinodeとデータを読み込むために大量のI/Oトラフィックが生成されることがありますが、同じファイル(または同じディレクトリ内のファイル)の後続のファイルが大部分キャッシュに書き込まれるため、I/Oは必要ありません。

キャッシュへの書き込みに対する影響についても考慮してみましょう。読み取りI/Oは十分に大きなキャッシュで回避することができますが、書き込みトラフィックは永続的になるためにディスクに移動する必要があります。したがって、キャッシュは、読み取りのために行うフィルタでは、書き込みトラフィックのフィルタとして機能しません。つまり、別のものが必要です。それは書き込みバッファリング(ときどき呼び出されるように)です。これには、多くのパフォーマンス上の利点があります。まず、書き込みを遅らせることによって、ファイルシステムはいくつかの更新のより小さなI/Oセットをバッチすることができます。たとえば、もし、1つのファイルが作成され、別のファイルが作成されるとすぐに更新されたときに、iノードのビットマップが更新された場合、ファイルシステムは最初の更新後に書き込みを遅らせることによってI/Oを保存します。第2に、多数の書き込みをメモリにバッファリングすることによって、システムは後続のI/Oをスケジューリングし、パフォーマンスを向上させることができます。

最後に、いくつかの書き込みはそれらを遅らせることで完全に避けられます。たとえば、アプリケーションがファイルを作成してから削除すると、ファイルの作成をディスクに反映させるために書き込みを遅らせると、完全にそれら(書き込み)を回避します。この場合、怠惰(ディスクへの書き込みブロックすること)は徳です。

>> TIP: UNDERSTAND STATIC VS. DYNAMIC PARTITIONING  
>> 異なるクライアント/ユーザー間でリソースを分割する場合は、静的パーティショニングまたは動的パーティショニングを使用できます。静的アプローチでは、単にリソースを固定された割合に1回だけ分割します。たとえば、メモリの利用者が2人いる場合、あるユーザーに固定量のメモリを割り当て、もう一人のユーザーには残りを割り当てます。ダイナミックなアプローチは柔軟性があり、時間の経過とともにリソースの量が異なる。たとえば、1人のユーザーが一定期間ディスク帯域幅のパーセンテージを高くすることがありますが、その後、システムが切り替わり、別のユーザーに使用可能なディスク帯域幅のより大きな部分を与えることを決定するかもしれません。  
それぞれのアプローチには利点があります。静的パーティショニングにより、各ユーザーはリソースの一部を受け取ることができ、通常は予測可能なパフォーマンスが向上し、実装がより簡単になります。動的パーティショニングは、(リソースを必要とするユーザーにアイドル状態のリソースを消費させることで)より優れた使用率を達成できますが、実装がより複雑になる可能性があり、アイドル状態のリソースが他のユーザーによって消費された後、必要な時に再び使うのに時間がかかります。よくあることですが、最良の方法はありません。むしろ、当面の問題について考える必要があり、どちらのアプローチが最も適切かを判断する必要があります。

上記の理由から、現代のファイルシステムの大部分は、5秒から30秒の間のどこかでメモリに書き込みをバッファリングします。これは、もう一つのトレードオフを表します。アップデートがディスクに伝播される前にシステムがクラッシュした場合、そのアップデートは失われます。ただし、書き込みを長時間メモリに保存することで、バッチ処理、スケジューリング、さらには書き込みの回避によってもパフォーマンスを向上させることができます。

一部のアプリケーション(データベースなど)では、このトレードオフが楽しめません。したがって、書き込みバッファリングによる予期せぬデータ損失を避けるため、`fsync()`を呼び出すか、キャッシュを回避するダイレクトI/Oインタフェースを使用するか、またはrawディスクインタフェースを使用してファイルシステム全体を回避します。ほとんどのアプリケーションはファイルシステムのトレードオフがありますが、デフォルトで満足させることができない場合は、システムが望むように動作するのに十分なコントロールがあります。

>> TIP: UNDERSTAND THE DURABILITY/PERFORMANCE TRADE-OFF  
>> ストレージシステムは、多くの場合、ユーザーに耐久性とパフォーマンスのトレードオフをもたらします。ユーザが直ちに耐久性があるように書かれたデータを望む場合、システムは新しく書き込まれたデータをディスクにコミットする必要があり、したがって書き込みは遅いです(しかし安全である)。しかし、ユーザーが小さなデータの損失を許容することができれば、システムはしばらくの間、書き込みをメモリにバッファリングし、後でディスクに書き込むことができます(バックグラウンドで)。そうすることで、書き込みが素早く完了したように見えるため、パフォーマンスが向上します。ただし、クラッシュが発生した場合、まだディスクにコミットされていない書き込みは失われ、その結果、トレードオフが発生します。このトレードオフを適切に行う方法を理解するには、ストレージシステムを使用するアプリケーションが必要とするものを理解することが最善です。たとえば、Webブラウザでダウンロードした最後の数枚のイメージを失うことは許容されるかもしれませんが、銀行口座に資金を追加しているデータベーストランザクションの一部が失うことは許容できないかもしれません。あなたが金持ちでない限りそうでしょう。

## 40.8 Summary
私たちはファイルシステムを構築するのに必要な基本的な機械を見てきました。各ファイル(メタデータ)についての情報が必要です。通常、inodeという構造体に格納されます。ディレクトリは、名前→inode番号のマッピングを格納する特定の種類のファイルです。そして、他の構造も必要です。たとえば、ファイルシステムでは、ビットマップなどの構造を使用して、どのinodeまたはデータブロックが空いているか、割り当てられているかを追跡することがよくあります。

ファイルシステム設計の素晴らしい点は、その自由性です。次の章で調べるファイルシステムは、この自由性を利用してファイルシステムの一部の側面を最適化します。私たちが未知のまま残した多くのポリシーの決定も明らかにあります。たとえば、新しいファイルが作成されると、そのファイルをディスク上のどこに配置する必要がありますか？この方針と今後の方針については、今後の章の対象となります。

## 参考文献
[A+07] Nitin Agrawal, William J. Bolosky, John R. Douceur, Jacob R. Lorch  
A Five-Year Study of File-System Metadata  
FAST ’07, pages 31–45, February 2007, San Jose, CA  
An excellent recent analysis of how file systems are actually used. Use the bibliography within to follow the trail of file-system analysis papers back to the early 1980s.

[B07] “ZFS: The Last Word in File Systems”  
Jeff Bonwick and Bill Moore  
Available: http://www.ostep.org/Citations/zfs last.pdf  
One of the most recent important file systems, full of features and awesomeness. We should have a chapter on it, and perhaps soon will.

[B02] “The FAT File System”  
Andries Brouwer  
September, 2002  
Available: http://www.win.tue.nl/˜aeb/linux/fs/fat/fat.html  
A nice clean description of FAT. The file system kind, not the bacon kind. Though you have to admit, bacon fat probably tastes better.

[C94] “Inside the Windows NT File System”  
Helen Custer  
Microsoft Press, 1994  
A short book about NTFS; there are probably ones with more technical details elsewhere.

[H+88] “Scale and Performance in a Distributed File System”  
John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham, Michael J. West.  
ACM Transactions on Computing Systems (ACM TOCS), page 51-81, Volume 6, Number 1, February 1988 A classic distributed file system; we’ll be learning more about it later, don’t worry.

[P09] “The Second Extended File System: Internal Layout”  
Dave Poirier, 2009  
Available: http://www.nongnu.org/ext2-doc/ext2.html  
Some details on ext2, a very simple Linux file system based on FFS, the Berkeley Fast File System. We’ll be reading about it in the next chapter.

[RT74] “The UNIX Time-Sharing System”  
M. Ritchie and K. Thompson  
CACM, Volume 17:7, pages 365-375, 1974  
The original paper about UNIX. Read it to see the underpinnings of much of modern operating systems.

[S00] “UBC: An Efficient Unified I/O and Memory Caching Subsystem for NetBSD”  
Chuck Silvers  
FREENIX, 2000  
A nice paper about NetBSD’s integration of file-system buffer caching and the virtual-memory page cache. Many other systems do the same type of thing.

[S+96] “Scalability in the XFS File System”  
Adan Sweeney, Doug Doucette, Wei Hu, Curtis Anderson,  
Mike Nishimoto, Geoff Peck  
USENIX ’96, January 1996, San Diego, CA  
The first attempt to make scalability of operations, including things like having millions of files in a directory, a central focus. A great example of pushing an idea to the extreme. The key idea behind this file system: everything is a tree. We should have a chapter on this file system too.

\newpage

# 41 Locality and The Fast File System
UNIXオペレーティングシステムが初めて導入されたとき、UNIXウィザードのKen Thompson氏が最初のファイルシステムを作成しました。"古いUNIXファイルシステム"と呼ぶことにしましょう。それは本当に簡単でした。基本的に、そのデータ構造はディスク上で次のようになりました。  
![](../41/img/fig41_1_1.PNG)  
スーパーブロック(S)には、ボリュームの大きさ、inodeの数、ブロックの空きリストの先頭へのポインタなど、ファイルシステム全体に関する情報が含まれていました。ディスクのiノード領域には、ファイルシステムのすべてのiノードが含まれていました。最後に、ディスクの大部分がデータブロックに取り込まれました。

古いファイルシステムについての良いことは、単純であり、ファイルシステムが提供しようとしていた基本的な抽象化、すなわちファイルとディレクトリ階層をサポートしていることでした。この使いやすいシステムは、過去の不器用なレコードベースのストレージシステムからの真の前進でした。ディレクトリ階層は、以前のシステムで提供されていたよりシンプルな1レベル階層以上の真の進歩でした。

## 41.1 The Problem: Poor Performance
問題：パフォーマンスはひどいものです。Berkeley [MJLF84]のKirk McKusickとその同僚たちが測定したように、パフォーマンスは悪くなり、ファイルシステムが全体のディスク帯域幅のわずか2％というところまで悪化しました。

主な問題は、古いUNIXファイルシステムがランダムアクセスメモリのようにディスクを処理していたことです。データを保持する媒体がディスクであることに関係なく、データは場所全体に広がっていたため、現実的で高価な位置決めコストが発生していました。例えば、あるファイルのデータブロックは、しばしばinodeから非常に遠く離れているため、最初にinodeを読み込み、次にファイルのデータブロックを読み込むときには高価なシークを引き起こします(かなり一般的な操作です)。

さらに悪いことに、空き領域が慎重に管理されていないため、ファイルシステムはかなり断片化してしまいます。 フリーリストは、ディスク全体に散らばっているブロックを指し、ファイルが割り当てられると、次の空きブロックを取るだけです。 その結果、論理的に連続したファイルには、ディスク全体を行き来してアクセスすることができ、パフォーマンスが大幅に低下しました。

たとえば、次のデータブロック領域を想定します。この領域には、サイズが2ブロックのそれぞれに4つのファイル(A、B、C、D)が含まれています。  
![](../41/img/fig41_1_2.PNG)  
BとDが削除された場合、結果のレイアウトは次のようになります。  
![](../41/img/fig41_1_3.PNG)  
見て分かるように、空き領域は4つの連続チャンクではなく、1つのブロックの2つのチャンクごとにそれぞれ分割されています。たとえば、サイズが4ブロックのファイルEを割り当てたいとします。  
![](../41/img/fig41_1_4.PNG)  
何が起こるかを見ることができます.Eはディスク全体に広がってしまい、その結果、Eにアクセスすると、ディスクからピーク(シーケンシャル)なパフォーマンスが得られません。むしろ、まずE1とE2を読んでから、シークしてからE3とE4を読み込みます。この断片化の問題は、古いUNIXファイルシステムでは常に発生し、パフォーマンスが低下します。副題：この問題は、ディスクデフラグツールが役立つものです。オンデイスクデータを再編成してファイルを連続して配置し、連続した1つまたは複数の領域に空き領域を作ったり、データを移動したり、iノードなどを書き換えたりして変更を反映させます。

もう1つの問題：元のブロックサイズが小さすぎます(512バイト)。従って、ディスクからデータを転送することは、本質的に非効率的でした。小さなブロックは内部断片化(ブロック内の無駄)を最小限に抑えたため良好でしたが、各ブロックに到達するためにはオーバーヘッドが必要な場合があったため、転送には問題がありました。したがって、問題：

>> THE CRUX: HOW TO ORGANIZE ON-DISK DATA TO IMPROVE PERFORMANCE  
パフォーマンスを向上させるためにファイルシステムのデータ構造をどのように整理できますか？ これらのデータ構造に加えて、どのようなタイプの割り当てポリシーが必要ですか？ ファイルシステムを「ディスク対応」にするにはどうすればよいですか？

## 41.2 FFS: Disk Awareness Is The Solution
Berkeleyのあるグループは、より高速でより高速なファイルシステムを構築することを決めました。そのファイルシステムは、巧みにFast File System(FFS)と呼ばれていました。この考え方は、ファイルシステムの構造と割り当てポリシーを「ディスクを意識した」ものにすることでパフォーマンスを向上させることでした。FFSはファイルシステム研究の新しい時代を迎えました。(`open()`、`read()`、`write()`、`close()`、および、その他の同じAPIであるファイルシステムコールを含む)同じファイルシステムに維持しながら、内部実装を変更することによって、新しいファイルシステムの構築を今日まで続いています。事実上、最新のファイルシステムはすべて、パフォーマンス、信頼性、その他の理由から内部を変更しながら、既存のインターフェイスに準拠している(したがって、アプリケーションとの互換性を維持します)。

## 41.3 Organizing Structure: The Cylinder Group
最初のステップは、ディスク上の構造を変更することでした。FFSは、ディスクを複数のシリンダグループに分割します。1つのシリンダは、ドライブの中央から同じ距離にあるハードドライブの異なる面にある一連のトラックです。いわゆる幾何学的形状とは明らかに類似しているため、円筒と呼ばれています。FFSは、N個の連続したシリンダをグループに集約するので、ディスク全体はシリンダグループの集合として見ることができます。ここには、6つのプラッターを持つドライブの4つあるうちの最も外側のトラックから3つのシリンダーで構成されるシリンダー・グループを示す簡単な例があります。(今回の場合N=3であるため、黒色のトラックはシリンダーに含まれません)  
![](../41/img/fig41_1_5.PNG)  
最新のドライブは、特定のシリンダが使用されているかどうかを本当に理解するためにファイルシステムに十分な情報をエクスポートしないことに注意してください。前述の[AD14a]のように、ディスクはブロックの論理アドレス空間をエクスポートし、ジオメトリの詳細をクライアントから隠します。したがって、現代のファイルシステム(Linux ext2、ext3、ext4など)ではなく、ドライブをブロックグループに編成します。各ブロックグループは、ディスクのアドレススペースのちょうど連続した部分です。下の図は、8つのブロックがすべて異なるブロックグループに編成されている例を示しています(実際のグループはもっと多くのブロックで構成されています)。  
![](../41/img/fig41_1_6.PNG)  
シリンダーグループまたはブロックグループと呼ばれても、FFSがパフォーマンスを向上させるために使用する中心的なメカニズムです。重大なことに、同じグループ内に2つのファイルを配置することで、FFSは、次々にアクセスすることによって、ディスク全体で長くシークを発生させることはありません。

これらのグループを使用してファイルとディレクトリを格納するには、ファイルとディレクトリをグループに配置し、そこに必要なすべての情報を追跡する必要があります。そうするために、FFSには、inode、データブロック、およびそれらのそれぞれが割り当てられているか解放されているかを追跡するいくつかの構造体のスペースなど、ファイルシステムが各グループ内に持つと考えられるすべての構造が含まれています。ここでは、FFSが単一のシリンダグループ内に保持するものを示しています。  
![](../41/img/fig41_1_7.PNG)  
ここで、この単一のシリンダグループのコンポーネントを詳細に調べてみましょう。FFSは、信頼性の理由からスーパーブロック(S)のコピーを各グループに保持します。スーパーブロックは、ファイルシステムをマウントするために必要です。複数のコピーを保持することで、1つのコピーが破損した場合でも、動作中のレプリカを使用してファイルシステムをマウントしてアクセスできます。

各グループ内で、FFSはグループのinodeとデータブロックが割り当てられているかどうかを追跡する必要があります。グループごとのinodeビットマップ(ib)とデータビットマップ(db)は、各グループのinodeとデータブロックに対してこの役割を果たします。ビットマップは、ファイルシステムの空き領域を管理する優れた方法です。古いファイルシステムの空きリストの断片化の問題を避けるために、大きな空き領域を見つけてファイルに割り当てるのが簡単だからです。

最後に、iノードとデータブロック領域は、以前の非常に単純なファイルシステム(VSFS)の領域とまったく同じです。通常、各シリンダグループのほとんどはデータブロックで構成されています。

>> ASIDE: FFS FILE CREATION
>> たとえば、ファイルの作成時に更新するデータ構造を考えてみましょう。この例では、ユーザーが新しいファイル/foo/bar.txtを作成し、ファイルが1ブロック(4KB)であると仮定します。このファイルは新しいもので、新しいinodeが必要です。したがって、inodeビットマップと新しく割り当てられたiノードの両方がディスクに書き込まれます。ファイルにはデータも含まれているため、割り当ても必要です。データビットマップとデータブロックはこうして(最終的に)ディスクに書き込まれます。したがって、現在のシリンダグループへの少なくとも4回の書き込みが行われます(これらの書き込みは、発生前に一時的にメモリにバッファリングされる可能性があります)。ただこれが全てではありません！特に、新しいファイルを作成するときには、そのファイルをファイルシステム階層に配置する必要があります。つまり、ディレクトリを更新する必要があります。具体的には、親ディレクトリfooを更新してbar.txtのエントリを追加する必要があります。この更新は、fooの既存のデータブロックに収まるか、または関連するデータビットマップとともに新しいブロックを割り当てる必要があります。ディレクトリの新しい長さを反映し、時間フィールドを更新するために(last modified timeなどの)fooのinodeも更新する必要があります。これらが、新しいファイルを作成するだけの全作業です！

## 41.4 Policies: How To Allocate Files and Directories
このグループ構造を導入したFFSは、パフォーマンスを向上させるために、ファイルとディレクトリおよび関連メタデータをディスクに配置する方法を決定する必要があります。基本的なマントラ(真言)はシンプルです。関連するものを一緒にしておくことです(そしてその結果として、関連性のないものを遠くに保ちます)。

したがって、マントラに従うためには、FFSは何が「関連する」ものであるかを決定し、それを同じブロックグループ内に置かなければいけません。反対に、関係のないアイテムは異なるブロックグループに配置する必要があります。この目的を達成するために、FFSはいくつかの簡単な配置ヒューリスティックを利用します。

最初はディレクトリの配置です。FFSは単純なアプローチを採用しています。割り当てられたディレクトリの数が少ない(グループ間でディレクトリのバランスを調整する)シリンダ数と、空きinode数が多いシリンダグループ(その後にたくさんのファイルを割り当てることができます)を見つけて、ディレクトリデータとiノードをグループに置きます。もちろん、他のヒューリスティックをここで使用することができます(例えば、空きデータブロックの数を考慮に入れて)。ファイルの場合、FFSは2つのことを行います。まず(一般的なケースでは)、ファイルのデータブロックをinodeと同じグループに割り当てて、(古いファイルシステムのように)inodeとデータの間の長いシークを防ぎます。

次に、同じディレクトリーにあるすべてのファイルを、それらが入っているディレクトリーのシリンダー・グループに入れます。つまり、ユーザーが、/a/b、/a/c、/a/d、/b/fの4つのファイルを作成すると、FFSは、最初の3つを互いに近く(同じグループ)に配置し、4つ目を遠くに(他のグループに)配置しようとします。このような割り当ての例を見てみましょう。この例では、各グループに10個のinodeと10個のデータブロック(非現実的に小さな番号)があり、3つのディレクトリ(ルートディレクトリ/、/a、/b)と4つのファイル(/a/c、/a/d、/a/e、/b/f)は、FFSポリシーごとに配置されます。通常のファイルはそれぞれ2ブロックのサイズであり、ディレクトリにはただ1ブロックのデータしかないものとします。この図では、各ファイルまたはディレクトリ( /はルートディレクトリ、/aはa、/b/fはfなど)に明瞭な記号を使用しています。  
![](../41/img/fig41_1_8.PNG)  
FFSポリシーは2つのポジティブなことを行います。FFSポリシーは、各ファイルのデータブロックが各ファイルのiノードの近くにあり、同じディレクトリ内のファイルが互いに近くにあることです。(すなわち、/a/c、/a/d、/a/eはグループ1にすべてあり、ディレクトリ/bとそのファイル/b/fはグループ2で互いに近くにあります)これとは対照的に、グループ全体でiノードを広げ、グループのiノードテーブルがすぐにいっぱいにならないようにしようとするiノード割り当てポリシーを見てみましょう。したがって、最終的な割り当ては次のようになります。  
![](../41/img/fig41_1_9.PNG)  
図からわかるように、このポリシーは実際にファイル(およびディレクトリ)データをそれぞれのiノードの近くに保ちますが、ディレクトリ内のファイルは任意にディスクの周りに広がっているため、名前ベースの場所は保持されません。ファイル/a/c、/a/d、/a/eへのアクセスは、FFSアプローチのように1つではなく3つのグループに分かれています。

FFSポリシーヒューリスティックは、ファイルシステムのトラフィックや、特に微妙なものについての広範な調査に基づいているわけではありません。むしろ、彼らは昔ながらの良い常識に基づいています(CSは結局何を意味するのでしょうか？)。ディレクトリ内のファイルは、多くの場合、一緒にアクセスされます。ファイルの束をコンパイルし、それらを単一の実行可能ファイルにリンクすることを想像してください。このような名前空間ベースのローカリティが存在するため、FFSはパフォーマンスを向上させ、関連ファイル間のシークが素敵で短いのです。

## 41.5 Measuring File Locality
これらのヒューリスティックが意味を成すかどうかをよりよく理解するために、ファイルシステムのアクセスの痕跡を分析し、実際に名前空間の局所性があるかどうかを見てみましょう。なんらかの理由で、このトピックについての良い研究は文献にないようです。

具体的には、SEERトレース[K94]を使用して、ディレクトリツリー内の「遠く離れた」ファイルアクセスが互いにどのように関連していたかを分析します。たとえば、ファイルfを開いた後、トレース内の次のファイルを開くと、ディレクトリツリー内のこれら2つのファイル間の距離はゼロになります(同じファイルであるため)。ディレクトリdir(すなわちdir/f)内のファイルfが開かれ、続いて同じディレクトリ(すなわちdir/g)内のファイルgが開いている場合、2つのファイルアクセス間の距離は1です。同じディレクトリですが、同じファイルではありません。私たちの距離メトリックは、言い換えれば、2つのファイルの共通の祖先を見つけるために移動するディレクトリツリーの距離を測定します。それらがツリーに近づくほど、メトリックは低くなります。

![](../41/img/fig41_1.PNG)

図41.1は、すべてのトレースの全体にわたってSEERクラスタ内のすべてのワークステーションでSEERトレースで観測された地域を示しています。このグラフは、x軸は差メトリックをプロットし、y軸はその差異のファイルオープン率の累積パーセントを示しています。具体的には、SEERトレース(グラフの「Trace」と表示)では、以前に開かれたファイルに対するファイルアクセスが約7％であり、ファイルアクセスの40％近くが同じファイルまたは同じディレクトリ内の1つに(つまり、0または1の差)を返します。したがって、FFSの局所性の仮定は(少なくともこれらの痕跡に対して)意味があります。

興味深いことに、25％程度のファイルへのアクセスは、2の距離を持つファイルに対するものでした。このタイプのローカリティは、ユーザーが複数のレベルの方法で関連ディレクトリのセットを構成し、それらの間で一貫してジャンプするときに発生します。たとえば、ユーザーがsrcディレクトリを持ち、オブジェクトファイル(.oファイル)をobjディレクトリに構築し、これらのディレクトリが両方ともメインのprojディレクトリのサブディレクトリである場合、共通のアクセスパターンはproj/src/fooになります.cに続いてproj/obj/foo.oが続きます。projは共通の祖先であるため、これら2つのアクセス間の距離は2です。FFSは、そのポリシー内でこのタイプのローカリティを取得しないため、そのようなアクセスの間にさらに悪影響が生じる可能性があります。

比較のため、グラフには「ランダム」トレースのローカリティも表示されます。ランダムトレースは、既存のSEERトレース内からランダムな順序でファイルを選択し、これらのランダムに順序付けられたアクセス間の距離メトリックを計算することによって生成されました。ご覧のように、予想どおり、ランダムトレースには名前空間の局所性が少なくなります。しかし、最終的にはすべてのファイルが共通の祖先(例えばroot)を共有し、一部局所性があるため、ランダムは(あくまで)比較する対象としては有用である。

## 41.6 The Large-File Exception
FFSでは、ファイルの配置に関する一般的なポリシーの1つの重要な例外があり、大きなファイルの場合に発生します。別の規則がなければ、大きなファイルは最初に配置されたブロックグループ(もしくは別の場所)を完全に埋めるでしょう。このようにブロックグループを充填することは、後続の「関連する」ファイルが、このブロックグループ内に配置されることを防ぐため、ファイルアクセスの局所性を損なう可能性があり、望ましくないです。

したがって、大きなファイルの場合、FFSは次の処理を行います。FFSは、いくつかのブロックが第1のブロックグループに割り当てられた後(例えば、12ブロックまたはiノード内で利用可能な直接ポインタの数)、ファイルの次の「大きな」チャンク(例えば、第1のブロック 間接ブロック)を別のブロックグループ(利用率が低いために選択されている可能性があります)に配置します。そのとき、ファイルの次のチャンクは、さらに別のブロックグループに配置されます。

この方針をよりよく理解するためにいくつかの図を見てみましょう。大きなファイルの例外がなければ、1つの大きなファイルはすべてのブロックをディスクの1つの部分に配置します。10個のinodeと1グループあたり40個のデータブロックで構成されたFFS内の30個のブロックを含むファイル(/a)の小さな例を調べます。ここでは、大きなファイルの例外を除いたFFSを示します。  
![](../41/img/fig41_1_10.PNG)  
この図でわかるように、/aはグループ0のデータブロックのほとんどを占めていますが、他のグループは空のままです。現在ルートディレクトリ(/)に他のファイルが作成されている場合、そのグループのデータのための余裕はあまりありません。

大容量ファイルの例外(ここでは各チャンクに5つのブロックが設定されています)では、FFSはファイルをグループ全体に広げ、その結果1つのグループ内の利用率が高すぎません。  
![](../41/img/fig41_1_11.PNG)  
巧みな読者(それはあなたです)は、気づいているかもしれません。シーケンシャルなファイルアクセスの比較的一般的なケース(ユーザーやアプリケーションが順番に0〜29のチャンクを読み込んだ場合など)では、ディスク全体にファイルのブロックを広げるとパフォーマンスが低下することに気付くでしょう。しかし、チャンクサイズを慎重に選択することで、この問題に対処できます。

具体的には、チャンクサイズが十分に大きい場合、ファイルシステムはディスクからデータを転送する時間のほとんどを費やし、ブロックのチャンク間を探すシークで(比較的)少し時間を費やします。支払ったオーバーヘッドごとにさらに多くの作業を行うことによってオーバーヘッドを削減するこのプロセスは、amortization(償却)と呼ばれ、コンピュータシステムでは一般的な手法です。

例を挙げておきましょう。ディスクの平均位置決め時間(すなわち、シークおよび回転)が10ミリ秒であると仮定します。更に、ディスクが40MB/sでデータを転送すると仮定します。半分のチャンクのシーク時間で半分のデータの転送時間を費やす(ピークディスク性能の50％を達成する)ことが目標だった場合は、10msの位置決めごとに10msのデータ転送に費やす必要があります。ですから、問題は次のようになります。転送で10ミリ秒を費やすにはチャンクがどれくらい大きくなければなりませんか？それは簡単です。ちょうど我々の古い友人、数学、特にディスクの章で言及された寸法分析を使用してください[AD14a]：  
![](../41/img/fig41_1_12.PNG)  
基本的には、この方程式の意味は次のとおりです。データを40 MB/秒で転送する場合は、シークするたびに409.6KBだけ転送する必要があります。このとき、半分のシーク時間で半分のデータ転送を達成する必要があります。同様に、ピーク帯域幅の90％(約3.69MB)、またはピーク帯域幅(40.6MB！)の99％を達成するために必要なチャンクのサイズを計算できます。ご覧のように、ピークに近づきたいほど、これらのチャンクは大きくなります(これらの値のプロットについては、図41.2を参照してください)。

しかし、大規模なファイルをグループ全体に広げるために、FFSはこのタイプの計算を使用しませんでした。代わりに、iノード自体の構造に基づいて簡単なアプローチをとっていました。最初の12個の直接ブロックは、inodeと同じグループに配置されました。後続の各間接ブロックと、それが指し示すすべてのブロックが異なるグループに配置されました。ブロックサイズが4KBで、ディスクのアドレスが32ビットの場合、この方法では、ファイル(4MB)の1024ブロックが別々のグループに配置され、直接ポインタによって指し示されるファイルの最初の48KBが例外となります。

![](../41/img/fig41_2.PNG)

ディスクメーカーの傾向としては、ディスクメーカーが表面にビット数を増やしたら、転送速度がかなり速くなりますが、シークに関連するドライブの機械的な側面(ディスクアームの速度と回転速度)はゆっくりと向上します[P98]。時間が経つにつれて、機械的なコストは比較的高価になり、そのコストを償却するためには、シークの間にもっと多くのデータを転送する必要があります。

## 41.7 A Few Other Things About FFS
FFSはいくつかの革新も導入しました。特に、デザイナーは小さなファイルを扱うことを非常に心配していました。それが判明したので、多くのファイルは2KB程度の大きさであったが、4KBのブロックを使用することでデータを転送するのは効率は良かったのですが、ディスクスペースの効率は悪くなってしまいました。したがって、この内部的な断片化は、典型的なファイルシステムではディスクのおよそ半分が無駄になる可能性があります。

FFSの設計者がヒットした解決策は簡単で、問題を解決しました。彼らは、ファイルシステムがファイルに割り当てることができる512バイトの小さなブロックであるサブブロックを導入することに決めました。したがって、小さなファイル(サイズが1KB)を作成した場合、2つのサブブロックを占有し、4KBブロック全体を無駄にすることはありません。ファイルが大きくなり、4KBのデータを取得するまで、ファイルシステムは、512バイトのブロックを割り当て続けます。4KBのデータを取得した場合、FFSは4KBブロックを見つけて、そのサブブロックを4KBブロックにコピーし、将来の使用のためにサブブロックを解放します。

このプロセスは非効率的であり、ファイルシステムに多くの余分な作業(特に、コピーを実行するための余分なI/O)を必要とすることがあります。そして、あなたはもう一度やって来るだろう！したがって、FFSは、一般的に、libcライブラリを変更することでこの最悪な動作を回避しました。ライブラリは書き込みをバッファリングしてからファイルシステムの4KBのチャンクに発行するので、ほとんどの場合、サブブロックの特殊化は完全に回避されます。

FFSが導入したもう一つのすばらしい点は、パフォーマンスのために最適化されたディスクレイアウトでした。その時代(SCSIや他の最新のデバイスインタフェースの前の)、ディスクは全く洗練されておらず、ホストCPUは操作をより実践的に制御する必要がありました。図41.3の左側のように、ファイルがディスクの連続するセクタに置かれたときにFFSに問題が発生しました。

![](../41/img/fig41_3.PNG)

特に、この問題は順序読み出し時に発生しました。FFSは最初にブロック0への読み出しを発行します。読み出しが完了し、FFSがブロック1への読み出しを発行した時点では、それは遅すぎました。すなわち、ブロック1の読み出しの前にヘッドを通り過ぎるため、一度回転を待たなければいけません。

図41.3の右側に示すように、FFSはこの問題を別のレイアウトで解決しました。他のすべてのブロック(この例では)をスキップすることによって、FFSはディスクヘッドを通過する前に次のブロックを要求するのに十分な時間を確保します。実際、FFSは余分な回転を避けるためにレイアウト(設計)を行い、特定のディスクに対して、スキップする必要があるブロックの数を十分に把握するほどスマートでした。このテクニックは、FFSがディスクの特定のパフォーマンスパラメータを把握し、それらを使って厳密なずらしたレイアウトスキーム(設計計画)を決定するので、parameterization(パラメータ化)と呼ばれていました。

あなたは考えているかもしれません。この計画は結局大したことではありません。実際には、このタイプのレイアウトでピーク帯域幅の50％しか得られません。なぜなら、各ブロックを一度読み取るためには、各トラックを2回周回しなければならないからです。幸運なことに、現代のディスクはずっとスマートです。内部的にトラック全体を読み込んで内部のディスクキャッシュ(これはまさにトラックバッファと呼ばれます)にバッファリングします。その後、トラックへの次の読み込みで、ディスクはキャッシュから目的のデータを返します。したがって、ファイルシステムはこれらの信じられないほど低いレベルの詳細を心配する必要はありません。抽象化と高水準のインタフェースは、適切に設計されたときに良いことになります。

他のいくつかのユーザビリティ改善も加えられました。FFSは、長いファイル名を可能にする最初のファイルシステムの1つであったため、従来の固定サイズのアプローチ(例えば、8文字)ではなくファイルシステムでより表現力のある名前が可能になりました。さらに、シンボリックリンクと呼ばれる新しい概念が導入されました。前の章[AD14b]で説明したように、ハードリンクは、(ファイルシステム階層にループを導入する恐れがあるために)ディレクトリを指すことができず、同じボリューム内のファイルのみを指し示すことができます(inode番号はまだ意味があるはずです)。シンボリックリンクを使用すると、システム上の他のファイルやディレクトリの「エイリアス」を作成することができ、柔軟性が向上します。FFSは、ファイルの名前を変更するための原子的な`rename()`操作も導入しました。基本技術を超えた使いやすさの向上により、FFSはより強力なユーザーベースになる可能性が高くなりました。

>> TIP: MAKE THE SYSTEM USABLE  
>> おそらく、FFSの最も基本的な教訓は、ディスク対応レイアウトの概念的な概念を導入しただけでなく、単にシステムをより使いやすくした多くの機能を追加したということです。長いファイル名、シンボリックリンク、およびリネーム操作はすべてアトミックに機能し、システムのユーティリティを改善しました。この研究論文について書くのは難しい(「The Symbolic Link: Hard Link’s Long Lost Cousin」についての14ページを読んでみよう)が、そのような小さな機能はFFSをより有用にし、採用の機会を増やす可能性が高いです。システムを使いやすくすることは、深い技術革新よりも重要であることが多いです。

## 41.8 Summary
ファイル管理の問題は、オペレーティングシステム内で最も興味深い問題の1つであることが明らかになり、最も重要なハードディスクというデバイスに対処する方法が示されていたことから、FFSの導入はファイルシステムの歴史における大きな潮流でした。それ以来、何百もの新しいファイルシステムが開発されてきましたが、今日でも多くのファイルシステムがFFSからの手がかりを取ります(例えば、Linux ext2とext3は明らかにFFSの子孫です)。明らかなのはすべての現代のシステムはFFSの主な教訓から成り立っていることです。それは、「ディスクをディスクのように扱う」ということです。

## 参考文献
[AD14a] “Operating Systems: Three Easy Pieces”  
Chapter: Hard Disk Drives  
Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau  
There is no way you should be reading about FFS without having first understood hard drives in some detail. If you try to do so, please instead go directly to jail; do not pass go, and, critically, do not collect 200 much-needed simoleons.

[AD14b] “Operating Systems: Three Easy Pieces”  
Chapter: File System Implementation  
Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau  
As above, it makes little sense to read this chapter unless you have read (and understood) the chapter on file system implementation. Otherwise, we’ll be throwing around terms like “inode” and “indirect block” and you’ll be like “huh?” and that is no fun for either of us.

[K94] “The Design of the SEER Predictive Caching System”  
G. H. Kuenning  
MOBICOMM ’94, Santa Cruz, California, December 1994  
According to Kuenning, this is the best overview of the SEER project, which led to (among other things) the collection of these traces.

[MJLF84] “A Fast File System for UNIX”  
Marshall K. McKusick, William N. Joy, Sam J. Leffler, Robert S. Fabry  
ACM Transactions on Computing Systems, 2:3, pages 181-197.  
August, 1984. McKusick was recently honored with the IEEE Reynold B. Johnson award for his contributions to file systems, much of which was based on his work building FFS. In his acceptance speech, he discussed the original FFS software: only 1200 lines of code! Modern versions are a little more complex, e.g., the BSD FFS descendant now is in the 50-thousand lines-of-code range.

[P98] “Hardware Technology Trends and Database Opportunities”  
David A. Patterson  
Keynote Lecture at the ACM SIGMOD Conference (SIGMOD ’98)  
June, 1998  
A great and simple overview of disk technology trends and how they change over time.

\newpage

# 42 Crash Consistency: FSCK and Journaling
これまで見てきたように、ファイルシステムは、ファイルシステムから期待される基本的な抽象化をサポートするために必要なファイル、ディレクトリ、その他すべてのメタデータなど、一連のデータ構造を管理します。ほとんどのデータ構造(例えば、実行中のプログラムのメモリ内にあるもの)とは異なり、ファイルシステムのデータ構造は永続的でなければならず、長時間に渡って生き残り、電力損失があってもデータを保持しなければいけません(ハードディスクやデバイスフラッシュベースのSSDのように)

ファイルシステムが直面する大きな課題の1つは、停電やシステムクラッシュがあっても永続的なデータ構造を更新する方法です。具体的には、ディスク上の構造を更新する途中で、誰かが電源コード上を移動してマシンの電源が失われたらどうなりますか？または、オペレーティングシステムにバグが発生し、クラッシュしたらどうなりますか？停電やクラッシュのため、永続的なデータ構造を更新するのは非常に難しく、クラッシュ一貫性の問題として知られているファイルシステムの実装において新しい興味深い問題につながります。

この問題は非常に理解しやすいです。特定の操作を完了するために、2つのオンディスク構造AとBを更新する必要があるとします。ディスクは一度に1つの要求のみを処理するため、これらの要求の1つが最初にディスクに到達します(AまたはB)。1回の書き込みが完了してからシステムがクラッシュしたり電源が切れた場合、ディスク上の構造は矛盾した状態になります。したがって、すべてのファイルシステムを解決する必要があるという問題があります。

>> THE CRUX: HOW TO UPDATE THE DISK DESPITE CRASHES  
>> システムは、2つの書き込みの間でクラッシュまたは電力を失う可能性があり、したがって、ディスク上の状態が部分的にしか更新されない可能性があります。クラッシュ後、システムは起動し、(ファイルにアクセスするために)ファイルシステムを再度マウントします。任意の時点でクラッシュが発生する可能性があることを考えれば、ファイルシステムがディスク上のイメージを合理的な状態に保つにはどうすればよいでしょうか？

この章では、この問題をより詳しく説明し、ファイルシステムがそれを克服するために使用したいくつかの方法を見ていきます。fsckやファイルシステムチェッカーと呼ばれる古いファイルシステムのアプローチを調べてみましょう。次に、書き込みごとに若干のオーバーヘッドを追加するが、クラッシュや電力損失からより迅速に回復するテクニックである、ジャーナリング(write ahead loggingとも呼ばれる)という別のアプローチに注目します。私たちは、Linux ext3 [T98、PAA05](比較的近代的なジャーナリングファイルシステム)が実装しているいくつかの異なるジャーナリングの仕組みを含めて、ジャーナリングの基本的な機構について議論する予定です。

## 42.1 A Detailed Example
ジャーナリングの調査を開始するために、例を見てみましょう。ディスク上の構造を何らかの方法で更新する仕事量を使用する必要があります。ここでは、仕事量が単純であると仮定します。単一のデータブロックを既存のファイルに追加することです。追加は、ファイルを開き、ファイルのオフセットをファイルの最後に移動するために`lseek()`を呼び出し、ファイルを閉じる前にファイルに4KBの単一の書き込みを発行することによって行われます。

これまでに見たようなファイルシステムと同様に、ディスク上に標準的な単純なファイルシステム構造を使用しているとします。この小さな例には、inodeビットマップ(inodeあたり8ビット、inodeあたり一つ)データビットマップ(8ビット、データブロックあたり1つ)、iノード(合計8つ、0から7までの番号、4つのブロックにまたがる)、データブロック(合計8つ、0から7までの番号付け)を含んでいます。このファイルシステムの図は次のとおりです。  
![](../42/img/fig42_1_1.PNG)  
図に示されている構造を見ると、inodeビットマップにマークされ、単一に割り当てられた1つのinode(inode番号2)とデータビットマップにマークされ、単一に割り当てられたデータブロック(データブロック4)があります。inodeはこのiノードの最初のバージョンであるため、I[v1]と表示されます。それはすぐに更新されます(上記の仕事量のため)。この単純化されたinodeの内部を見てみましょう。さっそくI[v1]の内部を見ていきます：  
![](../42/img/fig42_1_2.PNG)  
この単純化されたiノードでは、ファイルのサイズは1(1つのブロックが割り当てられています)、最初のダイレクトポインタがブロック4(ファイルの最初のデータブロックであるDa)を指し、3つのダイレクトポインターはすべてnullに設定されます(使用されていないことを示す)。もちろん、実際のinodeにはさらに多くのフィールドがあります。詳細については、前の章を参照してください。

ファイルに追加するときに新しいデータブロックを追加するので、ディスク上の3つの構造を更新する必要があります：inode(新しいブロックを指し示す必要があり、追加のためにサイズが大きくなる)新しいデータブロックであるDb、および新しいデータブロックが割り当てられたことを示すためにデータビットマップの新しいバージョン(それをB[v2]と呼ぶ)を含みます。

したがって、システムのメモリには、ディスクに書き込む必要がある3つのブロックがあります。更新されたiノード(iノードバージョン2、またはI[v2]の短縮形)は、次のようになります。  
![](../42/img/fig42_1_3.PNG)  
更新されたデータビットマップ(B[v2])は00001100のようになります。最後に、データブロック(Db)があります。それはユーザーがファイルに入れたものです。おそらく音楽を盗んだのでしょうか？私たちが望むのは、ファイルシステムの最終的なオンディスクイメージが次のようになることです。  
![](../42/img/fig42_1_4.PNG)  
この移行を達成するためには、ファイルシステムは、inode(I[v2])、ビットマップ(B[v2])、およびデータブロック(Db)の3つのディスクへの個別書き込みを実行する必要があります。これらの書き込みは通常、ユーザが`write()`システムコールを発行したときにすぐには起こらないことに注意してください。むしろ、dirty inode、ビットマップ、および新しいデータは、メインメモリ(ページキャッシュまたはバッファキャッシュ内)に最初に少しの時間でおかれます。ファイルシステムが最終的にそれらをディスクに書き込むことを決定すると(例えば5秒または30秒後に)、ファイルシステムは必要な書き込み要求をディスクに発行します。残念ながら、クラッシュが発生すると、ディスクへのこれらの更新が妨げられます。特に、これらの書き込みのうちの1つまたは2つが実行された後にクラッシュが発生し、3つすべてが完了していない場合、ファイルシステムは面白い状態になる可能性があります。
### Crash Scenarios
問題をよりよく理解するために、いくつかのクラッシュシナリオの例を見てみましょう。1回の書き込みだけが成功したとします。このように3つの可能な結果があります。

- データブロック(Db)だけがディスクに書き込まれます。  
この場合、データはディスク上にありますが、それを指すinodeはなく、ブロックが割り当てられていることを示すビットマップもありません。したがって、書き込みが起こらなかったかのようになります。このケースは、ファイルシステムの一貫性の観点から、まったく問題ではありません。

- 更新されたinode(I[v2])だけがディスクに書き込まれます。  
この場合、inodeはDbが書き込まれる直前のディスクアドレス(5)を指していますが、Dbは書き込まれていません。したがって、そのポインタを信頼すれば、ディスクからガベージデータ(ディスクアドレス5の古い内容)を読み込みます。
さらに、ファイルシステムの矛盾と呼ばれる新しい問題があります。ディスク上のビットマップは、データブロック5が割り当てられていないが、inodeはそれが持っていると言っています。ビットマップとiノードとの間の不一致は、ファイルシステムのデータ構造の不一致です。ファイルシステムを使用するには、何らかの形でこの問題を解決する必要があります(詳細は後述)。

- 更新されたビットマップ(B[v2])のみがディスクに書き込まれます。  
この場合、ビットマップはブロック5が割り当てられていることを示しますが、それを指すiノードはありません。したがって、ファイルシステムには一貫性がありません。ブロック5がファイルシステムによって決して使用されないので、この書込みは未解決のまま残された場合、space leak(スペースリーク)が生じます。  

3つのブロックをディスクに書き込む試みには、さらに3つのクラッシュシナリオがあります。これらの場合、2回の書き込みは成功し、最後の書き込みは失敗します。  

- inode(I[v2])とビットマップ(B[v2])はディスクに書き込まれますが、データ(Db)は書き込まれません。  
この場合、ファイルシステムのメタデータは完全に一貫しています。つまり、inodeにはブロック5へのポインタがあり、ビットマップには5が使用されていることが示されているため、ファイルシステムのメタデータの観点から見ても問題ありません。しかし1つの問題があります：5は再びそれにゴミ(古いデータ)を入れています。

- inode(I[v2])とデータブロック(Db)は書き込まれますが、ビットマップ(B[v2])は書き込まれません。  
この場合、inodeはディスク上の正しいデータを指していますが、inodeと古いバージョンのビットマップ(B1)の間に矛盾があります。したがって、ファイルシステムを使用する前に問題を解決する必要があります。

- ビットマップ(B[v2])とデータブロック(Db)は書き込まれますが、iノード(I[v2])は書き込まれません。  
この場合、iノードとデータビットマップの間に再び矛盾があります。しかし、ブロックが書き込まれ、ビットマップがその使用法を示していても、inodeがファイルを指していないので、どのファイルに属しているかわかりません。

### The Crash Consistency Problem
うまくいけば、これらのクラッシュシナリオから、クラッシュのためにディスク上のファイルシステムイメージに発生する可能性がある多くの問題を見ること、ファイルシステムのデータ構造に不整合、スペースリークが発生する可能性、ガベージデータをユーザーに返すこと等々あります。私たちが理想的にやってみたいのは、ファイルシステムをある一貫性のある状態(例えば、ファイルが追加される前)から別のものに(例えば、inode、ビットマップ、新しいデータブロックがディスクに書き込まれた後)に移動することです。残念ながら、ディスクは一度に1つの書き込みしかコミットしないため、これらの更新の間にクラッシュまたは電力損失が発生する可能性があるため、簡単には実行できません。この一般的な問題をクラッシュ一貫性の問題と呼びます(一貫性のある更新の問題とも呼ぶことができます)。

## 42.2 Solution #1: The File System Checker
初期のファイルシステムは、簡単なアプローチをとっていました。基本的に、彼らは不一致を起こさせ、後で(リブート時に)修正することに決めました。このアプローチの古典的な例は、これを行うツールfsckにあります。fsckは、そのような矛盾を見つけて修復するためのUNIXツールです。ディスクパーティションをチェックして修復するための同様のツールが、異なるシステムに存在します。このような方法ではすべての問題を解決できないことに注意してください。例えば、inodeがガベージデータを指しているファイルシステムが一貫しているように見える上のケースがあります。唯一の実際の目標は、ファイルシステムのメタデータが内部的に一貫していることを確認することです。

ツールfsckは、McKusickとKowalskiの論文[MK96]に要約されているように、いくつかの段階で動作します。これは、ファイルシステムがマウントされ、使用可能になる前に実行されます(fsckは、実行中に他のファイルシステムアクティビティが実行されていないことを前提としています)。一度終了すると、オンディスクファイルシステムは一貫していて、ユーザーがアクセスできるようにする必要があります。ここでは、fsckの動作の基本的な概要を示します。

- Superblock(スーパーブロック)  
fsckは、スーパーブロックが妥当であるかどうか最初にチェックします。ほとんどの場合、ファイルシステムのサイズが割り当てられたブロック数よりも大きいかどうかを確認するなどの健全性チェックが行われます。通常、これらのsanity(健全性)チェックの目的は、疑わしい(破損した)スーパーブロックを見つけることです。この場合、システム(または管理者)は、スーパーブロックの代替コピーを使用することを決定することができる。

- Free blocks(フリーブロック)  
次に、fsckはinode、間接ブロック、二重間接ブロックなどをスキャンして、ファイルシステム内でどのブロックが現在割り当てられているかを理解します。この知識を使用して、割り当てビットマップの正しいバージョンを生成します。したがって、ビットマップとiノードの間に矛盾がある場合、iノード内の情報を信頼することによって解決されます。同じタイプのチェックがすべてのinodeに対して実行され、使用中のように見えるすべてのinodeがinodeビットマップでそのようにマークされることを確認します。

- Inode state(ノードの状態)  
各iノードは、破損またはその他の問題がないかどうかチェックされます。たとえば、fsckは、割り当てられた各iノードが有効なタイプのフィールド(通常のファイル、ディレクトリ、シンボリックリンクなど)を持っているかどうかを確認します。簡単に修正できないinodeフィールドに問題がある場合、inodeは疑わしいとみなされ、fsckによってクリアされます。このとき、inodeビットマップが対応して更新されます。

- Inode links  (iノードリンク)  
また、fsckは各割り当てられたiノードのリンク数も確認します。ご存知のように、リンク数は、この特定のファイルへの参照(つまり、リンク)を含む異なるディレクトリの数を示します。リンクカウントを確認するために、fsckはルートディレクトリから開始してディレクトリツリー全体をスキャンし、ファイルシステム内のすべてのファイルとディレクトリの独自のリンク数を作成します。新しく計算されたカウントとiノード内で見つかったカウントとの間に不一致がある場合、通常はinode内のカウントを固定することによって是正措置を講じる必要があります。割り当てられたinodeが検出されたがディレクトリを参照していない場合、lost + foundディレクトリに移動されます。

- Duplicates(重複)  
また、fsckは、重複ポインタ、すなわち、2つの異なるiノードが同じブロックを参照する場合もチェックします。1つのiノードが明らかに悪い場合は、消去される可能性があります。あるいは、指し示しブロックをコピーして、各iノードに必要に応じて独自のコピーを与えることもできます。

- Bad blocks (不良ブロック)  
すべてのポインタのリストをスキャンしながら、不良ブロックポインタのチェックも実行されます。ポインタは、有効な範囲外のものを明示的に指す場合(例えば、パーティションサイズより大きなブロックを参照するアドレスを有する場合など)、「不良」とみなされます。この場合、fsckはインテリジェントな処理を行うことはできません。iノードまたは間接ブロックからポインタを削除(クリア)するだけです。

- Directory checks  (ディレクトリチェック)  
fsckはユーザーファイルの内容を理解しません。ただし、ディレクトリには、ファイルシステム自体によって作成された特別な形式の情報が格納されています。したがって、fsckは、各ディレクトリの内容について追加の整合性チェックを実行し、"."と".."が最初のエントリであり、ディレクトリエントリで参照されている各iノードが割り当てられていることを確認し、ディレクトリが階層全体で複数回リンクされないようにします。

ご覧のように、動作中のfsckを構築するには、ファイルシステムの複雑な知識が必要です。そのようなコードがすべての場合に正しく動作することを確認することは困難です[G + 08]。しかし、fsck(と同様のアプローチ)は、より大きな、おそらくより根本的な問題を抱えています。それは、非常に遅いということです。ディスクボリュームが非常に大きい場合は、ディスク全体をスキャンして割り当てられたすべてのブロックを検索し、ディレクトリツリー全体を読み取るには数分または数時間かかることがあります。ディスクが大容量になり、RAIDが普及したときのfsckのパフォーマンスは、(最近の進歩[M + 13]にもかかわらず)法外になりました。

より高いレベルでは、fsckの基本的な前提はちょっと不合理なようです。ディスクに3つのブロックしか書き込まれていない上記の例を考えてみましょう。わずか3ブロックの更新中に発生した問題を解決するためにディスク全体をスキャンするのは非常に高価です。このような状況は、寝室の床に鍵を落とした後、地下室から出発してすべての部屋を通って鍵検索アルゴリズムを検索することに似ています。それは確実に動作しますが、全てを探索することは無駄です。このように、ディスク(およびRAID)が増えるにつれて、研究者や実務者は他のソリューションを探すようになりました。

## 42.3 Solution #2: Journaling (or Write-Ahead Logging)
おそらく一貫した更新の問題に対する最も一般的な解決策は、データベース管理システムの世界からアイデアを盗むことです。このアイデアは、write ahead logging(先行書き込みログ)と呼ばれ、この種の問題を正確に解決するために考案されたものです。ファイルシステムでは、歴史的な理由から、write ahead logggin journaling(先行書き込みログ記録ジャーナリング)を呼び出します。これを行うための最初のファイルシステムはCedar [H87]でしたが、Linux ext3とext4、reiserfs、IBMのJFS、SGIのXFS、Windows NTFSなど、現代の多くのファイルシステムがこの考え方を使用しています。

基本的な考え方は次のとおりです。ディスクを更新するときは、構造を上書きする前に、まずあなたがしようとしていることを説明する小さなメモ(ディスク上の他の場所、よく知られている場所)を書き留めます。つまり、このメモを書くことは"先行書き込み"の部分であり、"ログ"として整理する構造であるwrite ahead loggingに書きます。

メモをディスクに書き込むことで、構造の更新(上書き)中にクラッシュが発生した場合、元に戻って作成したメモを見て再試行することができます。したがって、ディスク全体をスキャンするのではなく、クラッシュ後に修正する対象(および修正方法)を正確に知ることができます。設計上、ジャーナリングは更新中に少しの作業を追加することで、リカバリ時に必要な作業量を大幅に削減します。

ここでは、普及しているジャーナリングファイルシステムであるLinux ext3がジャーナリングをファイルシステムに組み込む方法について説明します。ディスク上の構造のほとんどはLinux ext2と同じです。例えば、ディスクはブロックグループに分割され、各ブロックグループにはinodeとデータビットマップ、inodeとデータブロックがあります。新しい鍵構造はジャーナル自体であり、パーティション内または他のデバイス内のスペースをいくらか占有します。したがって、ext2ファイルシステム(ジャーナルなし)は次のようになります。  
![](../42/img/fig42_1_5.PNG)  
ジャーナルが同じファイルシステムイメージ内に置かれているとします(ただし、別のデバイスやファイルシステム内のファイルに配置されることもあります)。ジャーナルを持つext3ファイルシステムは次のようになります。  
![](../42/img/fig42_1_6.PNG)  
実際の違いはジャーナルの存在だけであり、もちろん使い方もです。

### Data Journaling
データジャーナリングの仕組みを理解するための簡単な例を見てみましょう。 データジャーナリングは、Linux ext3ファイルシステムのモードとして利用できます。ここから、この議論の多くがベースになっています。

inode(I[v2])、ビットマップ(B[v2])、およびデータブロック(Db)をディスクに書き戻したい場合には、もう一度標準アップデートを行ってください。最終的なディスクの場所に書き込む前に、最初にそれらをログ(a.k.a. journal)に書き込む予定です。このログのようになります：  
![](../42/img/fig42_1_7.PNG)  

ここに5つのブロックを書きました。トランザクション開始(TxB)は、ファイルシステムに対する保留中の更新に関する情報(ブロックI[v2]、B[v2]、およびDbの最終アドレスなど)を含むこの更新について教えてくれます、またその中に何らかの種類トランザクション識別子(TID)があります。中央の3つのブロックには、ブロック自体の正確な内容が含まれています。これはphysical logging(物理ロギング)と呼ばれ、更新の正確な物理的な内容をジャーナルに入れています(logical logging(論理ロギング)はジャーナルにアップデートのよりコンパクトな論理表現を置きます。例えば、"このアップデートはデータブロックDbにファイルXを追加したい"です。これはもう少し複雑ですが、ログの領域を節約し、パフォーマンスを向上させることができます)。最終ブロック(TxE)は、このトランザクションの最後のマーカーであり、TIDも含みます。

このトランザクションが安全にディスクに保存されると、ファイルシステム内の古い構造を上書きする準備が整います。このプロセスをチェックポイントと呼びます。したがって、ファイルシステムをチェックポイントする(すなわち、ジャーナル内の保留中の更新を最新のものにする)ために、I[v2]、B[v2]およびDbの書き込みを上記のディスク位置に発行します。これらの書き込みが正常に完了すると、ファイルシステムのチェックポイントが成功し、基本的に完了しています。したがって、私たちの最初の一連の操作は以下になります：  

1. Journal write(ジャーナルライト)  
トランザクション開始ブロック、保留中のすべてのデータおよびメタデータ更新、およびトランザクション終了ブロックを含むトランザクションをログに書き込みます。これらの書き込みが完了するのを待ちます。  
2. Checkpoint(チェックポイント)  
保留中のメタデータとデータの更新をファイルシステムの最終的な場所に書き込みます。

この例では、最初にTxB、I[v2]、B[v2]、Db、TxEをジャーナルに書きます。これらの書き込みが完了すると、I[v2]、B[v2]、Dbをディスク上の最終位置にチェックポイントすることで更新を完了します。

ジャーナルへの書き込み中にクラッシュが発生した場合、状況は少し難解になります。ここでは、トランザクションのブロックセット(たとえば、TxB、I[v2]、B[v2]、Db、TxE)をディスクに書き込もうとしています。これを行う簡単な方法の1つは、それぞれに一度一回ずつ発行し、それぞれが完了するのを待ってから、次を発行することです。しかし、これは遅いです。理想的には、5回の書き込みをすべて一度に発行したいと思います。5回の書き込みを1回の書き込みに変えて高速化することができます。しかし、以下の理由から、これは安全ではありません：このような大きな書き込みがある場合、ディスクは内部的にスケジューリングを実行し、大きな書き込みの小さな部分を任意の順序で完了することがあります。したがって、ディスクは、(1)TxB、I[v2]、B[v2]、TxEを書き込み、後で(2)Dbだけ内部的に書き込むかもしれません。残念ながら、ディスクが(1)と(2)の間で電源を失った場合、これがディスク上で終了します。

>> ASIDE: FORCING WRITES TO DISK  
>> 2つのディスク書き込みの間で順序付けを行うためには、現代のファイルシステムにはいくつかの注意が必要です。昔は、AとBの2つの書き込み間の順序を強制するのは簡単でした。ディスクにAの書き込みを発行し、書き込みが完了したらディスクがOSに割り込むのを待ってから、Bの書き込みを発行します。

ディスク内のライトキャッシュの使用が増えたため、状況はやや複雑になりました。書き込みバッファリングを有効にすると(即時報告とも呼ばれることもあります)、書き込みはディスクのメモリキャッシュに置かれ、まだディスクに到達していないときに書き込みを完了したことをディスクはOSに通知します。その後、OSが次の書き込みを発行する場合、以前の書き込み後にディスクに到達することを保証しません。したがって、書き込み間の順序付けは保持されません。1つの解決策は、書き込みバッファリングを無効にすることです。しかし、より現代的なシステムは、特別な注意を払い、明示的な書き込みバリアを発行します。バリアが完了すると、バリアの後に発行された書き込みの前である、バリアがディスクに到着する前に発行されたすべての書き込みが保証されます。

このすべての機械は、ディスクの正しい動作に大きな信頼を必要とします。残念なことに、最近の調査では、「高性能」ディスクを提供しようとしているディスクメーカーの中には、書き込みバリア要求を明示的に無視しているため、ディスクが一見高速に動作するものの、誤動作の危険があります[C + 13、R + 11 ]。Kahanは、「速いことは間違っていたとしても、遅い時間にほとんどいつも勝つ。」と言いました。  
![](../42/img/fig42_1_8.PNG)  
なぜこれが問題なのですか？さて、トランザクションは有効なトランザクションのように見えます(シーケンス番号が一致する開始点と終了点があります)。さらに、ファイルシステムはその第4のブロックを見ることができず、それが間違っていることを知ることができません。結局のところ、それは任意のユーザーデータです。したがって、システムがリブートしてリカバリを実行すると、このトランザクションが再生され、ゴミ・ブロックである'??'の内容がDbがある場所に無意識にコピーされます。これは、ファイル内の任意のユーザーデータにとって悪いことです。スーパーブロックのようなファイルシステムの重要な部分にファイルシステムがマウントできなくなる可能性がある場合は、それはずっと悪いことです。

この問題を回避するために、ファイルシステムは2つのステップでトランザクション書き込みを発行します。まず、TxEブロックを除くすべてのブロックをジャーナルに書き込み、一度にこれらの書き込みを発行します。これらの書き込みが完了すると、ジャーナルは次のようになります(私たちの追加作業を再度仮定します)。  
![](../42/img/fig42_1_9.PNG)  
これらの書き込みが完了すると、ファイルシステムはTxEブロックの書き込みを発行し、ジャーナルをこのsafe state(安全状態)という最終状態にします。  
![](../42/img/fig42_1_10.PNG)  
このプロセスの重要な側面は、ディスクによって提供される原子性の保証です。ディスクでは、512バイトの書き込みが発生するか、それとも発生しないのかが保証されています(半分書きされることはありません)。したがって、TxEの書き込みがアトミックであることを確認するには、それを単一の512バイトブロックにする必要があります。したがって、ファイルシステムを更新する現在のプロトコルは、3つのフェーズのそれぞれが次のようにラベル付けされています。  
1. Journal write(ジャーナルライト)  
トランザクションの内容(TxB、メタデータ、データを含む)をログに書き込みます。これらの書き込みが完了するのを待ちます。  
2. Journal commit(ジャーナルコミット)  
トランザクションコミットブロック(TxEを含む)をログに書き込みます。書き込みが完了するまで待ちます。トランザクションはコミットされていると言います。  
3. Checkpoint(チェックポイント)  
アップデートの内容(メタデータとデータ)を最終的なディスク上の場所に書き込みます。  

### Recovery
ファイルシステムがクラッシュから回復するためにジャーナルの内容をどのように使用できるかを理解しましょう。この一連の更新中にいつでもクラッシュが発生する可能性があります。トランザクションがログに安全に書き込まれる前にクラッシュが発生した場合(つまり、上記の手順2が完了する前)、私たちの仕事は簡単です。保留中の更新は単にスキップされます。トランザクションがログにコミットした後で、チェックポイントが完了する前にクラッシュが発生した場合、ファイルシステムは次のように更新を回復できます。システムが起動すると、ファイルシステムのリカバリプロセスはログをスキャンし、ディスクにコミットしたトランザクションを探します。これらのトランザクションはこのように(順番に)再生され、ファイルシステムはトランザクション内のブロックを最終的なオンディスク位置に書き戻そうと再度試みる。この形式のロギングは、最も簡単な形式の1つで、redo loggingと呼ばれます。

ジャーナル内のコミットされたトランザクションをリカバリすることにより、ファイルシステムはディスク上の構造が一貫していることを保証し、ファイルシステムをマウントして新しい要求の準備を進めることができます。

チェックポイント処理中の任意の時点で、ブロックの最終位置の更新が完了した後でも、クラッシュが発生することは問題ありません。最悪の場合、これらの更新のうちのいくつかは、回復中に再び単に実行されます。リカバリはまれな操作である(予期しないシステムクラッシュの後でのみ行われる)ため、いくつかの重複した書き込みは気にしません。

### Batching Log Updates
基本的なプロトコルが余計なディスクトラフィックを増やす可能性があることに気づいたかもしれません。たとえば、file1とfile2という2つのファイルを同じディレクトリに作成するとします。1つのファイルを作成するには、inodeのビットマップ(新しいiノードを割り当てるため)、新しく作成したファイルのiノード、新しいディレクトリを含む親ディレクトリのデータブロックディレクトリエントリ、および親ディレクトリのiノード(現在は新しい変更時刻があります)が表示されます。ジャーナリングでは、この2つのファイル作成のそれぞれについて、この情報を論理的にジャーナルにコミットします。ファイルが同じディレクトリにあり、同じiノードブロック内のinodeを持っていたとしても、これは慎重でなければ、これらの同じブロックを何度も何度も書くことになります。

この問題を解決するために、ファイルシステムの中には、一度に1つずつディスクを更新するものはありません(Linuxのext3など)。むしろ、すべての更新をグローバルトランザクションにバッファリングすることができます。上記の例では、2つのファイルが作成されると、ファイルシステムは、メモリ内のinodeビットマップ、ファイルのinode、ディレクトリデータ、ディレクトリのinodeをdirtyとマークし、現在のトランザクションを構成するブロックのリストにそれらを追加します。最後に、これらのブロックをディスクに書き込む(例えば、5秒のタイムアウトの後)と、この単一のグローバルトランザクションは、上記のすべての更新の詳細を含むコミットがされます。したがって、更新をバッファリングすることによって、ファイルシステムは多くの場合ディスクへの過剰な書き込みトラフィックを回避することができます。

### Making The Log Finite
したがって、我々は、ファイルシステム上のディスク構造を更新するための基本的なプロトコルに到達しました。ファイルシステムのバッファは、しばらくの間メモリ内の更新します。最終的にディスクに書き込むとき、ファイルシステムは最初にトランザクションの詳細をジャーナル(write ahead log)に慎重に書き出します。トランザクションが完了すると、ファイルシステムはそれらのブロックをディスク上の最終的な場所にチェックポイントします。ただし、ログは有限のサイズです。この図に示すようにトランザクションを追加し続けると、すぐに処理が完了します。何が起こったと思いますか？  
![](../42/img/fig42_1_11.PNG)  
ログがいっぱいになると、2つの問題が発生します。最初の方が簡単ですが、それほど重要ではありません。ログが大きければ大きいほど、回復プロセスは回復するためにログ内のすべてのトランザクションを順番に再生する必要があるため、回復に時間がかかります。第2の問題の方が問題です。ログがいっぱい(またはほぼ満杯)になるとディスクにコミットすることができなくなり、ファイルシステムが「役に立たない」状態になります。

これらの問題に対処するために、ジャーナリングファイルシステムでは、ログを循環データ構造として扱い、何度も繰り返し使用します。このため、ジャーナルはcircular log(循環ログ)と呼ばれることがあります。これを行うには、ファイルシステムがチェックポイントの後で何らかのアクションを実行する必要があります。具体的には、トランザクションがチェックポイントされると、ファイルシステムはジャーナル内で占有していたスペースを解放し、ログスペースを再利用できるようにする必要があります。この目的を達成する方法はたくさんあります。たとえば、ジャーナルスーパーブロック内のログ内で最も古いもの、チェックポイントのない最新のトランザクションを単にマークすることができます。他のすべてのスペースはフリーです。ここに図解があります：  
![](../42/img/fig42_1_12.PNG)  
journal superblock(ジャーナルスーパーブロック)(メインファイルシステムのスーパーブロックと混同しないでください)では、ジャーナリングシステムは、チェックポイントされていないトランザクションを知るための十分な情報を記録します。したがって、循環モデルのログを再利用することでリカバリ時間を短縮することを可能にします。そして、私たちは基本的なプロトコルにもう一つのステップを追加します：  
1. Journal write(ジャーナルライト)  
トランザクションの内容(TxBと更新内容を含む)をログに書き込みます。 これらの書き込みが完了するのを待ちます。  
2. Journal commit(ジャーナルコミット)  
トランザクションコミットブロック(TxEを含む)をログに書き込みます。 書き込みが完了するまで待ちます。 トランザクションは現在コミットされています。  
3. checkpoint  
アップデートの内容をファイルシステム内の最終的な場所に書き込みます。  
4. free  
その後、ジャーナルのスーパーブロックを更新して、ジャーナル内のトランザクションをフリーでマークしてください。

したがって、私たちは最終的なdata journaling protocol(データジャーナリングプロトコル)を持っています。しかし、まだ問題が残っています。各データブロックをディスクに2回書き込んでいます。これは、システムクラッシュのような稀な問題に対して支払うにはコストがかかり過ぎています。データを2回書き込まずに一貫性を保つ方法を見つけられるでしょうか？

### Metadata Journaling
リカバリは速くなりました(ジャーナルをスキャンし、ディスク全体をスキャンするのではなく、いくつかのトランザクションを再生する)が、ファイルシステムの通常の動作はわれわれが望むよりも遅くなります。特に、ディスクへの書き込みごとに、最初にジャーナルに書き込むので、書き込みトラフィックが倍増します。この倍増は、シーケンシャル・ライト・仕事量中に特に苦労し、ドライブのピーク書き込み帯域幅の半分で処理を進めます。さらに、ジャーナルへの書き込みとメインファイルシステムへの書き込みとの間には、コストがかかるシークがあり、いくつかの仕事量に顕著なオーバーヘッドが加わります。

すべてのデータブロックをディスクに2回書き込むコストが高いため、パフォーマンスを向上させるためにいくつかの方法を試しました。例えば、上で説明したジャーナリングモードは、すべてのユーザーデータ(ファイルシステムのメタデータを追加)をジャーナリングするため、(Linux ext3のように)データジャーナリングと呼ばれることがあります。ジャーナリングのより単純な(そしてより一般的な)形式は、ordered journaling(順序付けされたジャーナリング)(または単にメタデータジャーナリング)と呼ばれることもありますが、ユーザーデータがジャーナルに書き込まれない点を除いてほぼ同じです。したがって、上記と同じ更新を実行すると、次の情報がジャーナルに書き込まれます。  
![](../42/img/fig42_1_13.PNG)  
以前にログに書き込まれたデータブロックDbは、余分な書き込みを避けてファイルシステムの適切な場所に書き込まれます。ディスクへのI/Oトラフィックのほとんどがデータであるため、データを2回書き込まないことは、ジャーナリングのI/O負荷を大幅に削減します。しかし、変更は面白い質問を提起します：いつデータブロックをディスクに書き込むべきですか？

問題をよりよく理解するためにファイルの例を追加してみましょう。更新はI[v2]、B[v2]、Dbの3つのブロックで構成されます。最初の2つはメタデータであり、ログに記録され、チェックポイントされます。後者はファイルシステムに一度だけ書き込まれます。Dbをいつディスクに書き込む必要がありますか？それは重要ですか？

判明したように、データ書き込みの順序付けは、メタデータだけのジャーナリングに関して重要です。たとえば、トランザクション(I[v2]とB[v2]を含む)が完了した後にDbをディスクに書き込むとどうなりますか？残念ながら、この方法には問題があります。ファイルシステムは一貫していますが、私はI[v2]がガベージデータを指してしまうことになります。具体的には、I[v2]とB[v2]が書かれていますが、Dbがそれをディスクに作られていない場合を考えてみてください。ファイルシステムは次にリカバリを試みます。なぜなら、Dbがログに記録されていないからです。そのため、ファイルシステムはI[v2]とB[v2]への書き込みを再生し、(ファイルシステムメタデータの観点から)一貫したファイルシステムを生成します。しかし、I[v2]は、ガーベージデータを指しています。すなわち、Dbの先頭にあったスロットにあったものが何であれです。

このような状況が起こらないようにするために、いくつかのファイルシステム(Linux ext3など)は、関連するメタデータがディスクに書き込まれる前に、(通常のファイルの)データブロックをディスクに書き込みます。具体的には、プロトコルは次のとおりです。  
1. Data write(データ書き込み)  
データを最終的な場所に書き込む。完了するのを待ちます(待機はオプションです;詳細は下記参照)。  
2. Journal metadata write(ジャーナルメタデータの書き込み)  
書き込み開始ブロックとメタデータをログに書き込みます。書き込みが完了するのを待ちます。  
3. Journal commit(ジャーナルコミット)  
トランザクションコミットブロック(TxEを含む)をログに書き込みます。書き込みが完了するまで待ちます。トランザクション(データを含む)は現在コミットされています。  
4. Checzpoint metadata(チェックポイントのメタデータ)  
メタデータ更新の内容をファイルシステム内の最終的な場所に書き込みます。  
5. Free  
その後、ジャーナルスーパーブロックで取引をフリーでマークします。

データの書き込みを最初に強制することで、ファイルシステムはポインタがゴミを指し示すことがないことを保証することができます。実際には、「オブジェクトが参照する前に参照されるオブジェクトを書く」というこのルールは、クラッシュ一貫性のコアであり、他のクラッシュ一貫性スキーム[GP94](詳細は以下を参照)によってさらに悪用されます。

ほとんどのシステムでは、完全なデータジャーナリングよりもメタデータジャーナリング(ext3の注文ジャーナルに似ています)が一般的です。たとえば、Windows NTFSとSGIのXFSは両方とも、ある形式のメタデータジャーナリングを使用します。Linux ext3では、dataモード、orderedモード、unorderedモードのいずれかを選択できます(unorderedモードでは、いつでもデータを書き込むことができます)。これらのモードはすべてメタデータの一貫性を保ちます。これらはデータのセマンティクスが異なります。

最後に、上記のプロトコルに示されているように、ジャーナルへの書き込みを発行する前に(ステップ2)、データ書き込みを完了させる(ステップ1)ことは正確である必要はないことに注意してください。具体的には、トランザクション開始ブロックとメタデータをジャーナルに書き込むことを発行することは問題ありません。唯一の実際の要件は、ジャーナルコミットブロックの発行前にステップ1と2が完了していることです(ステップ3)。

### Tricky Case: Block Reuse
興味深いコーナーケースがいくつかあり、ジャーナリングをもっと難しくし、議論する価値があります。それらの多くはブロックの再利用を中心に展開されています。ext3の主力の1人であるStephen Tweedie氏は次のように述べています。「システム全体のなんとなく悲惨な部分は何ですか？...ファイルの削除です。削除に関係することはすべて業が深いです。削除とは何か...ブロックが削除されてから再割り当てされると、何が起こるかという悪夢があります。」[T00]

Tweedieの具体的な例は次のとおりです。何らかの形式のメタデータジャーナリングを使用しているとします(ファイルのデータブロックはジャーナリングされません)。fooというディレクトリがあるとしましょう。ユーザーはfooにエントリを追加し(ファイルを作成するなど)、fooの内容(ディレクトリはメタデータとみなされるため)がログに書き込まれます。fooディレクトリのデータの場所がブロック1000であると仮定します。したがって、ログには次のような内容が含まれます。  
![](../42/img/fig42_1_14.PNG)  
この時点で、ユーザーはディレクトリ内だけでなくディレクトリ自体もすべて削除し、ブロック1000を解放して再利用します。最後に、ユーザーは新しいファイル(foobarなど)を作成し、fooに属していた同じブロック(1000)を再利用します。foobarのiノードは、データと同様にディスクにコミットされます。ただし、メタデータジャーナリングが使用されているため、foobarのiノードだけがジャーナルにコミットされます。ファイルfoobar内のブロック1000の新しく書き込まれたデータはジャーナリングされません。  
![](../42/img/fig42_1_15.PNG)  
今、クラッシュが発生し、これらの情報はすべてログに残っていると仮定します。再生中、回復プロセスは、ブロック1000におけるディレクトリデータの書き込みを含むログ内のすべてを単に再生します。この再生は古いディレクトリの内容で現在のファイルfoobarのユーザデータを上書きします！明らかに、これは正しいリカバリアクションではありません。そして、確実にfoobarファイルを読むときにユーザは驚いてしまいます。

この問題にはいくつかの解決策があります。たとえば、前記ブロックの削除がジャーナルからチェックポイントが外に出るまで、ブロックを再使用することはできないようにすればよいです。Linux ext3が代わりに使っているのは、新しいタイプのレコードをジャーナルに追加することです。revoke record(取り消しレコード)と呼ばれます。上記の場合、ディレクトリを削除すると、取り消しレコードがジャーナルに書き込まれます。ジャーナルを再生するとき、システムは最初にそのような取り消しレコードをスキャンします。そのような取り消されたデータは決して再生されないので、上記の問題は回避されます。

### Wrapping Up Journaling: A Timeline
ジャーナリングの議論を終了する前に、議論したプロトコルをそれぞれのタイムラインで要約します。図42.1にメタデータだけでなくデータをジャーナリングする場合のプロトコルを示します。図42.2に、メタデータのみをジャーナリングする場合のプロトコルを示します。

![](../42/img/fig42_1.PNG)

各図では、時間が下方向に増加し、図の各行は書き込みが発行されるか完了するかの論理時間を示しています。たとえば、データジャーナリングプロトコル(図42.1)では、トランザクション開始ブロック(TxB)の書き込みとトランザクションの内容を論理的に同時に発行することができ、したがって任意の順序で完了することができます。しかし、トランザクション終了ブロック(TxE)への書込みは、前の書き込みが完了するまで発行してはいけません。同様に、データブロックとメタデータブロックへのチェックポイント書き込みは、トランザクション終了ブロックがコミットされるまで開始できません。水平方向の破線は、書き込み注文要件を遵守しなければならない場所を示しています。

メタデータジャーナリングプロトコルについても同様のタイムラインが表示されます。データ書込みは、トランザクションへの書込みが開始され、ジャーナルの内容と同時に論理的に発行されることに注意してください。ただし、トランザクション終了が発行される前に、それが発行され完了する必要があります。

![](../42/img/fig42_2.PNG)

最後に、タイムラインの各書き込みについてマークされた完了時間は任意であることに注意してください。実際のシステムでは、I/Oサブシステムによって完了時刻が決定され、書き込みを並べ替えてパフォーマンスを向上させることができます。命令の唯一の保証は、プロトコルの正確さのために強制されなければならないものです(図の水平の破線で示されています)。

## 42.4 Solution #3: Other Approaches
これまで、ファイルシステムのメタデータを一貫して保つための2つのオプションであるfsckに基づく遅延アプローチ、ジャーナリングと呼ばれるより積極的なアプローチについて説明しました。しかしながら、これらは唯一の2つのアプローチではありません。ソフトアップデート[GP94]として知られているこのようなアプローチの1つが、GangerとPattによって導入されました。

このアプローチでは、ファイルシステムへのすべての書き込みを慎重に指示して、ディスク上の構造が一貫性のない状態にならないようにします。たとえば、参照されているデータブロックを、それを参照しているinodeより前にディスクに書き込むことで、inodeがゴミを指していないことを保証できます。同様なルールでファイルシステムのすべての構造を得ることができます。

しかし、ソフトアップデートを実装することは難しいことです。上記のジャーナリング層は、正確なファイルシステム構造の知識をほとんど持たずに実装することができますが、ソフトアップデートは各ファイルシステムデータ構造の複雑な知識を必要とし、システムにかなりの複雑さを加えます。

別のアプローチは、コピーオンライト(yes、COW)と呼ばれ、SunのZFS [B07]を含む多くの一般的なファイルシステムで使用されています。この手法は、ファイルやディレクトリを上書きすることはありません。むしろ、ディスク上の未使用の場所に新しい更新を置きます。多数の更新が完了した後、COWファイルシステムは、ファイルシステムのルート構造を反転して、新しく更新された構造へのポインタを含みます。そうすることで、ファイルシステムの整合性が保たれます。このテクニックについては、今後の章でlog structured file system(LFS)について議論するときに詳しく学習します。LFSはCOWの初期の例です。

もう一つのアプローチはウィスコンシンで開発したアプローチです。backpointer based consistency(またはBBC)と題されたこの技法では、書込みの間に順序付けは行われません。整合性を達成するために、システム内のすべてのブロックに追加のバックポインタが追加されます。たとえば、各データブロックには、それが属するiノードへの参照があります。ファイルにアクセスするとき、ファイルシステムは、フォワードポインタ(例えば、inodeまたは直接ブロック内のアドレス)がそれを参照するブロックを指し示しているかどうかをチェックすることによって、ファイルが一貫しているかどうかを判定することができます。その場合、すべてが安全にディスクに到達していなければならず、したがってファイルは一貫しています。そうでなければ、ファイルは不整合であり、エラーが返されます。ファイルシステムにバックポインタを追加することで、新しい形式の遅延クラッシュ整合性を達成することができます[C + 12]。

最後に、ジャーナルプロトコルがディスク書き込みが完了するのを待たなければならない回数を減らす手法も検討しました。楽観的な衝突の一貫性[C + 13]が付与されたこの新しい手法では、可能な限り多くのディスクへの書き込みが行われ、トランザクションチェックサム[P + 05]の一般化された形式や他のいくつかの手法を使用して、発生した不整合性を検出します。一部の仕事量では、これらの楽観的な手法により、パフォーマンスが向上します。しかし、本当にうまく機能するには、わずかに異なるディスクインタフェースが必要です[C + 13]。

## 42.5 Summary
クラッシュの一貫性の問題を導入し、この問題を攻撃するさまざまなアプローチについて説明しました。ファイルシステムチェッカーを構築する古いアプローチは機能しますが、現代のシステムでは回復が遅すぎる可能性があります。したがって、多くのファイルシステムでジャーナリングが使用されています。ジャーナリングは、O(ディスク容量の大きさ)からO(サイズのログ)までのリカバリ時間を短縮し、クラッシュや再起動後のリカバリを大幅に高速化します。このため、現代の多くのファイルシステムではジャーナリングが使用されています。ジャーナリングはさまざまな形で提供されることがあります。最も一般的に使用されるのはordered metadata journalingで、ジャーナルへのトラフィック量を削減しながら、ファイルシステムのメタデータとユーザーデータの両方に対して合理的な一貫性の保証を維持します。

## 参考文献
[B07] “ZFS: The Last Word in File Systems”  
Jeff Bonwick and Bill Moore  
Available: http://www.ostep.org/Citations/zfs last.pdf  
ZFS uses copy-on-write and journaling, actually, as in some cases, logging writes to disk will perform better.

[C+12] “Consistency Without Ordering”  
Vijay Chidambaram, Tushar Sharma, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau  
FAST ’12, San Jose, California  
A recent paper of ours about a new form of crash consistency based on back pointers. Read it for the exciting details!

[C+13] “Optimistic Crash Consistency”  
Vijay Chidambaram, Thanu S. Pillai, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau  
SOSP ’13, Nemacolin Woodlands Resort, PA, November 2013  
Our work on a more optimistic and higher performance journaling protocol. For workloads that call fsync() a lot, performance can be greatly improved.

[GP94] “Metadata Update Performance in File Systems”  
Gregory R. Ganger and Yale N. Patt  
OSDI ’94  
A clever paper about using careful ordering of writes as the main way to achieve consistency. Implemented later in BSD-based systems.

[G+08] “SQCK: A Declarative File System Checker”  
Haryadi S. Gunawi, Abhishek Rajimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau  
OSDI ’08, San Diego, California  
Our own paper on a new and better way to build a file system checker using SQL queries. We also show some problems with the existing checker, finding numerous bugs and odd behaviors, a direct result of the complexity of fsck.

[H87] “Reimplementing the Cedar File System Using Logging and Group Commit”  
Robert Hagmann  
SOSP ’87, Austin, Texas, November 1987  
The first work (that we know of) that applied write-ahead logging (a.k.a. journaling) to a file system.

[M+13] “ffsck: The Fast File System Checker”  
Ao Ma, Chris Dragga, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau  
FAST ’13, San Jose, California, February 2013  
A recent paper of ours detailing how to make fsck an order of magnitude faster. Some of the ideas have already been incorporated into the BSD file system checker [MK96] and are deployed today.

[MK96] “Fsck - The UNIX File System Check Program”  
Marshall Kirk McKusick and T. J. Kowalski  
Revised in 1996  
Describes the first comprehensive file-system checking tool, the eponymous fsck. Written by some of the same people who brought you FFS.

[MJLF84] “A Fast File System for UNIX”  
Marshall K. McKusick, William N. Joy, Sam J. Leffler, Robert S. Fabry  
ACM Transactions on Computing Systems.  
August 1984, Volume 2:3  
You already know enough about FFS, right? But yeah, it is OK to reference papers like this more than once in a book.

[P+05] “IRON File Systems”  
Vijayan Prabhakaran, Lakshmi N. Bairavasundaram, Nitin Agrawal, Haryadi S. Gunawi, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau  
SOSP ’05, Brighton, England, October 2005  
A paper mostly focused on studying how file systems react to disk failures. Towards the end, we introduce a transaction checksum to speed up logging, which was eventually adopted into Linux ext4.

[PAA05] “Analysis and Evolution of Journaling File Systems”  
Vijayan Prabhakaran, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau  
USENIX ’05, Anaheim, California, April 2005  
An early paper we wrote analyzing how journaling file systems work.

[R+11] “Coerced Cache Eviction and Discreet-Mode Journaling”  
Abhishek Rajimwale, Vijay Chidambaram, Deepak Ramamurthi, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau  
DSN ’11, Hong Kong, China, June 2011  
Our own paper on the problem of disks that buffer writes in a memory cache instead of forcing them to disk, even when explicitly told not to do that! Our solution to overcome this problem: if you want A to be written to disk before B, first write A, then send a lot of “dummy” writes to disk, hopefully causing A to be forced to disk to make room for them in the cache. A neat if impractical solution.

[T98] “Journaling the Linux ext2fs File System”  
Stephen C. Tweedie  
The Fourth Annual Linux Expo, May 1998  
Tweedie did much of the heavy lifting in adding journaling to the Linux ext2 file system; the result, not surprisingly, is called ext3. Some nice design decisions include the strong focus on backwards compatibility, e.g., you can just add a journaling file to an existing ext2 file system and then mount it as an ext3 file system.

[T00] “EXT3, Journaling Filesystem”  
Stephen Tweedie  
Talk at the Ottawa Linux Symposium, July 2000  
olstrans.sourceforge.net/release/OLS2000-ext3/OLS2000-ext3.html  
A transcript of a talk given by Tweedie on ext3.  

[T01] “The Linux ext2 File System”  
Theodore Ts’o, June, 2001.  
Available: http://e2fsprogs.sourceforge.net/ext2.html  
A simple Linux file system based on the ideas found in FFS. For a while it was quite heavily used; now it is really just in the kernel as an example of a simple file system.

\newpage

# 43 Log-structured File Systems
90年代初頭、John Ousterhout教授と大学院生Mendel Rosenblumが率いるバークレー校のグループが、ログ構造ファイルシステム[RO91]と呼ばれる新しいファイルシステムを開発しました。そうする動機は、以下の観察に基づいていました。

- システムメモリが増えている
メモリが大きくなるにつれ、より多くのデータをメモリにキャッシュすることができます。より多くのデータがキャッシュされるにつれて、ディスクトラフィックはますますキャッシュからの読み取りが処理されるため、書き込みから構成されます。したがって、ファイルシステムのパフォーマンスは主にその書き込みパフォーマンスによって決まります。

- ランダムI/Oパフォーマンスとシーケンシャル(順次)I/Oパフォーマンスの間に大きなギャップがあります
ハードドライブの転送帯域幅は、長年にわたって大きく増加しています[P98]。より多くのビットがドライブの表面にパックされると、アクセスするときの帯域幅が増加します。しかし、シークおよび回転遅延のコストは緩やかに低下しています。安価で小型のモーターを高速で回転させたり、ディスクアームをより迅速に動かすことは困難です。したがって、シーケンシャルにディスクを使用できる場合は、シークとローテーションを引き起こすアプローチよりも大きなパフォーマンス上の利点があります。

- 既存のファイルシステムは、多くの一般的な仕事量でパフォーマンスが低下します。
たとえば、FFS [MJLF84]は、FFS [MJLF84]は、一つのサイズブロックの新しいファイルを作成するために多数の書き込みを実行します。新しいinodeに1つ、inodeのビットマップの更新のために1つ、ファイルが存在するディレクトリデータブロックのために1つ、ディレクトリiノードを更新するために1つ、一部の新しいファイルの新しいデータブロックのために1つ、割り当てられたデータブロックをマークするためのデータビットマップに1つがあります。したがって、FFSはこれらすべてのブロックを同じブロックグループ内に配置しますが、FFSは多くの短いシークとそれに続く回転遅延を招きます。そのため、パフォーマンスはピークシーケンシャル帯域幅をはるかに下回ります。

- ファイルシステムはRAID対応ではありません
たとえば、RAID-4とRAID-5の両方で、1つのブロックへの論理書き込みによって4つの物理I/Oが発生する小さな書き込み問題が発生します。既存のファイルシステムは、この最悪の場合のRAID書き込み動作を回避しようとしません。

>> TIP: DETAILS MATTER  
>> すべての興味深いシステムは、いくつかの一般的なアイデアと多くの詳細から構成されています。時には、あなたがこれらのシステムについて学んでいる時、あなたは自分自身に"おっ！、良いアイデア発見！後は詳細考えるだけだ」と思うでしょう。また、これを使って、物事の実際の仕組みを半分だけ学べばいいと思うでしょう。しかし、このようなことをしないでください！多くの場合、詳細は重要です。LFSで見るように、一般的な考え方は理解しやすいですが、実際には動作するシステムを構築するためには、すべての難しいケースを考える必要があります。

理想的なファイルシステムは、書き込みパフォーマンスに重点を置き、ディスクの帯域幅を使用します。また、データを書き出すだけでなく、ディスク上のメタデータ構造を頻繁に更新する一般的な仕事量でも、パフォーマンスが向上します。最後に、RAIDとシングルディスクでうまくいくものです。

導入されたRosenblumとOusterhoutの新しいタイプのファイルシステムは、Log structured File Systemの略でLFSと呼ばれていました。ディスクに書き込むとき、LFSは最初にすべての更新(メタデータを含む)をメモリセグメントにバッファリングします。セグメントがいっぱいになると、ディスクの未使用部分に1回の長いシーケンシャル転送でディスクに書き込まれます。LFSは既存のデータを上書きすることはありませんが、むしろ常にフリーな場所にセグメントを書き込みます。セグメントが大きいため、ディスク(またはRAID)が効率的に使用され、ファイルシステムのパフォーマンスがその限界に近づきます。

>> THE CRUX: HOW TO MAKE ALL WRITES SEQUENTIAL WRITES?  
ファイルシステムはどのようにしてすべての書き込みを順次書き込みに変換できますか？読み取りの場合、このタスクは不可能です。読み取るブロックはディスク上のどこにあってもかまいません。しかし、書き込みの場合、ファイルシステムには常に選択肢があります。これはまさに私たちが開発しようとしている選択肢です。

## 43.1 Writing To Disk Sequentially
したがって、私たちは最初の課題を抱えています。ファイルシステム状態へのすべての更新をディスクへの一連の順次書き込みに変換するにはどうすればよいですか？これをよりよく理解するために、簡単な例を使用してみましょう。データブロックDをファイルに書き込んでいるとします。データブロックをディスクに書き込むと、次のオンディスクレイアウトが発生し、DはディスクアドレスA0に書き込まれます。  
![](../43/img/fig43_1_1.PNG)  
ただし、ユーザーがデータブロックを書き込むときは、ディスクに書き込まれるデータだけではありません。更新が必要な他のメタデータもあります。この場合、ファイルのiノード(I)をディスクに書き込んで、データブロックDを指します。ディスクに書き込むと、データブロックとiノードは次のようになります(注意してほしいのは、iノードはデータブロックと同じくらいに大きく見えます。これ一般的ではないケースです。大部分のシステムでは、データブロックは4 KBのサイズですが、inodeはだいたい128バイトくらいで小さいです)。  
![](../43/img/fig43_1_2.PNG)  

## 43.2 Writing Sequentially And Effectively
残念ながら、ディスクへの順次書き込みは、効率的な書き込みを保証するのに十分ではありません。たとえば、時間TにAをアドレス指定する単一のブロックを書き込無ことを考えてみてください。そのとき、ちょっと待ってからアドレスA + 1(次のブロックアドレスが順番に)に書き込まれますが、その時刻はT + δです。最初の書き込みと2番目の書き込みの間に、残念ながら、ディスクが回転しています。2回目の書き込みを発行すると、コミットされる前に回転のためにほとんどの時間待機します(具体的には、回転に時間T_rotationがかかる場合、ディスクはT_rotation - δを待ってからディスク表面に2回目の書き込みをコミットできます)。したがって、順番にディスクに書き込むだけでは、ピークパフォーマンスを達成するには不十分であることがうかがえます。むしろ、良好な書き込みパフォーマンスを達成するためには、ドライブに多数の連続書き込み(または1回の大きな書き込み)を発行する必要があります。

この目的を達成するために、LFSは書き込みバッファリングと呼ばれる古くからの技術を使用しています(実際、この考え方はコンピューティングの歴史のなかでも早い時期に発明された可能性が高いです。その利点についてはSolworthとOrji [SO90]を参照してください。その潜在的な危険については、Mogul [M94]を参照してください。)ディスクに書き込む前に、LFSはメモリ内の更新を追跡します。十分な数の更新を受信すると、ディスクに一度に書き込むので、ディスクの効率的な使用が保証されます。

LFSが一度に書き込む更新の大部分は、セグメントの名前で参照されます。この用語はコンピュータシステムでは過度に使用されていますが、ここではLFSが書き込みをグループ化するために使用する大規模なチャンクを意味します。したがって、ディスクに書き込むとき、LFSはメモリ内セグメントの更新をバッファし、そのセグメントを一度にディスクに書き込みます。セグメントが十分大きければ、これらの書き込みは効率的になります。ここでは、LFSが2セットの更新を小さなセグメントにバッファリングする例を示します。実際のセグメントは大きくなります(数MB)。最初の更新は、ファイルjへの4回のブロック書き込みです。2番目のブロックはファイルkに追加される1つのブロックです。LFSは、7ブロックのセグメント全体を一度にディスクにコミットします。これらのブロックの結果として得られるディスク上のレイアウトは次のとおりです。  
![](../43/img/fig43_1_3.PNG)  

## 43.3 How Much To Buffer?
ディスクに書き込む前にLFSが何回更新すべきでしょうかという質問を提起します。答えはもちろん、ディスクそのものです。具体的には転送速度と比較して位置決めのオーバーヘッドがどれほど高いかによって異なります。同様の分析については、FFSの章を参照してください。例えば、各書き込みの前に位置決め(すなわち、回転およびシークオーバーヘッド)がおよそT_position秒かかると仮定します。さらに、ディスク転送速度はR_peak MB/sであると仮定します。このようなディスクで実行するときにLFSが書き込む前にどれくらいバッファするべきですか？

これについて考える方法は、書き込むたびにオーバーヘッドである位置決めコストの固定費を支払うことです。したがって、その費用を償却するためには、どれくらい書く必要がありますか？書き込みが多いほど、(明らかに)良くなり、ピーク帯域幅を近づけることができます。具体的な答えを得るには、D MBを書き出すと仮定しましょう。このデータのチャンク(T_write)を書き出す時間は、位置決め時間T_positionプラス転送時間D(D / R_peak)、または  
![](../43/img/fig43_1_4.PNG)  
したがって、書き込まれたデータの量を書き込む合計時間で割った有効な書き込み速度(R_effective)は次のようになります。  
![](../43/img/fig43_1_5.PNG)  
私たちが興味を持っているのは、効率レート(R_effective)をピークレートに近づけることです。具体的には、効率レートをピークレートの何分のFにする必要があります(0 < F < 1、典型的なFは0.9、ピークレートの90％)数学的形式では、これは、R_effective = F × R_peakを望むことを意味します。この時点で、Dについて解くことができます  
![](../43/img/fig43_1_6.PNG)  
例として、位置決め時間が10ミリ秒、ピーク転送速度が100 MB/sのディスクを例に挙げてみましょう。ピークの90％(F = 0.9)の実効帯域幅が必要であると仮定します。この場合、D = 0.9 / 0.1 × 100MB/s × 0.01秒= 9MBとなります。いくつかの異なる値を試して、ピーク帯域幅に近づくためにバッファリングする必要があるかどうかを確認してください。ピークの95％に達するにはどれだけの量が必要ですか？99％ではどうでしょうか？

## 43.4 Problem: Finding Inodes
LFSでのinodeの発見方法を理解するには、典型的なUNIXファイルシステムでinodeを見つける方法を簡単に見てみましょう。FFSや古いUNIXファイルシステムのような典型的なファイルシステムでは、inodeを見つけるのは簡単です。なぜなら、それらは配列で編成され、固定された場所にディスク上に配置されるからです。たとえば、古いUNIXファイルシステムでは、すべてのiノードがディスクの固定部分に保持されます。したがって、特定のinodeを見つけるために、inode番号と開始アドレスを指定すると、そのinode番号にinodeのサイズを掛け、その値をディスク上の配列の開始アドレスに加えるだけで、正確なディスクアドレスを計算することができます。inode番号を指定すると、配列ベースの索引付けは迅速かつ簡単です。

FFSはinodeテーブルをチャンクに分割し、各シリンダグループ内にiノードのグループを配置するため、FFSでiノード番号が指定されたiノードを見つけることはわずかに複雑です。したがって、inodeの各チャンクの大きさとそれぞれの開始アドレスを知る必要があります。その後、計算は同様で簡単です。

LFSでは、人生はより困難です。どうしてでしょうか？私たちはすべてのディスクにiノードを散らしてしまいました！さらに悪いことに、上書きすることはありません。したがって、最新バージョンのinode(私たちが望むもの)は動いています。

## 43.5 Solution Through Indirection: The Inode Map
この問題を解決するために、LFSの設計者は、iノードマップ(imap)と呼ばれるデータ構造を通じて、iノード番号とiノード間の間接レベルを導入しました。imapはinode番号を入力として受け取り、最新のバージョンのinodeのディスクアドレスを生成する構造体です。したがって、エントリごとに4バイト(ディスクポインタ)の単純な配列として実装されることが多いと想像できます。inodeがディスクに書き込まれるたびに、imapは新しい場所で更新されます。

残念ながら、imapは永続的(すなわち、ディスクに書き込まれる)に保たれる必要があります。そのようにすることで、LFSはクラッシュ中のiノードの位置を追跡することができ、必要に応じて動作します。したがって質問があります、ディスク上のどこにimapを置くべきですか？

それはもちろん、ディスクの固定された部分に生きることができます。残念なことに、頻繁に更新されると、ファイル構造の更新に続いてimapへの書き込みが必要になるため、パフォーマンスが低下します(つまり、それぞれの更新とimapの固定場所とのディスクシークが多くなります)。

代わりに、LFSはinodeマップのチャンクを、他のすべての新しい情報を書いている場所のすぐ隣に置きます。したがって、以下のように、データブロックをファイルkに追加するとき、LFSは実際には次のように、新しいデータブロック、そのiノード、iノードマップの一部をすべて一緒にディスクに書き込みます。  
![](../43/img/fig43_1_7.PNG)  
この図では、imapと書かれているブロックに格納されているimap配列の部分は、inode kがディスクアドレスA1にあることをLFSに伝え、格納します。このiノードは、そのデータブロックDがアドレスA0にあることをLFSに指示します。

## 43.6 Completing The Solution: The Checkpoint Region
するどい読者であれば、ここで問題に気づいたかもしれません。どのようにしてinodeマップを見つけることができますか？それらはディスク全体に広がっていますか？最終的には魔法はありません。ファイルルックアップを開始するには、ファイルシステムにディスク上の固定された既知の場所が必要です。

LFSはこれのためにcheckpoint region(CR)と呼ばれる固定された場所をディスク上に持っています。checkpoint regionは、最新のinodeマップのポインタ(すなわち、そのアドレス)を含み、したがって、最初にCRを読み取ることによってinodeマップ部分を見つけることができます。checkpoint regionは定期的に更新されるだけなので(30秒ごとなど)、パフォーマンスに悪影響はありません。したがって、ディスク上のレイアウトの全体的な構造にはcheckpoint region(最新のinodeマップを示す)が含まれています。iノードマップ部分にはそれぞれiノードのアドレスが含まれています。inodeは典型的なUNIXファイルシステムと同じようにファイル(およびディレクトリ)を指しています。

次に、checkpoint regionの例を示します(ディスクの先頭、アドレス0にあります)。また、単一のimapチャンク、inode、データブロックがあります。実際のファイルシステムはもちろん大きなCR(実際には2つあります。後で学んでいきます)、多くのimapチャンク、さらにはもっと多くのinode、データブロックなどがあります。  
![](../43/img/fig43_1_8.PNG)  

## 43.7 Reading A File From Disk: A Recap
LFSがどのように動作するのかを理解するために、ディスクからファイルを読み込むために必要なことを見てみましょう。私達が始めるものをメモリに何も持っていないと仮定してください。最初に読み込まなければならないディスク上のデータ構造は、checkpoint regionです。checkpoint regionは、inodeマップ全体に対するポインタ(すなわち、ディスクアドレス)を含み、したがってLFSはinodeマップ全体を読み込み、メモリ内にキャッシュします。この時点以降、ファイルのiノード番号が与えられると、LFSは単にimapのinode numberからinode disk addressへのマッピングを検索し、最新のinodeのバージョンを読み込みます。この時点でファイルからブロックを読み込むには、直接ポインタ、間接ポインタ、二重間接ポインタを必要に応じてを使用して、LFSは典型的なUNIXファイルシステムとまったく同じように処理します。一般的なケースでは、LFSはディスクからファイルを読み込む際に、通常のファイルシステムと同じ数のI/Oを実行する必要があります。imap全体がキャッシュされるので、読み込み中にLFSが行う余計な作業は、imap内のinodeのアドレスを検索することです。

## 43.8 What About Directories?
ここまでは、inodeとデータブロックだけを考慮して、少し議論を単純化しました。しかし、ファイルシステム内のファイル(私たちの好きな偽のファイル名の1つである/home/remzi/fooなど)にアクセスするには、いくつかのディレクトリにもアクセスする必要があります。では、LFSはディレクトリデータをどのように格納していますか？

幸いにも、ディレクトリ構造は基本的に古典的なUNIXファイルシステムと同じです。ディレクトリは(名前、iノード番号)マッピングの集まりにすぎません。たとえば、ディスク上にファイルを作成する場合、LFSは新しいiノード、いくつかのデータ、ディレクトリデータ、このファイルを参照するiノードを書き込む必要があります。LFSはディスク上で順番に実行することを覚えておいてください(しばらくの間、更新をバッファリングした後)。したがって、ディレクトリにファイルfooを作成すると、ディスク上に次の新しい構造が生成されます。  
![](../43/img/fig43_1_9.PNG)  
iノードマップの部分には、ディレクトリファイルdirと新しく作成されたファイルfの両方の場所に関する情報が含まれています。したがって、ファイルfoo(iノード番号k)にアクセスする場合は、最初にinodeマップ(通常はメモリにキャッシュされている)を調べて、ディレクトリdir(A3)のiノードの場所を探します。次にディレクトリのinodeを読んで、ディレクトリのデータの場所(A2)を取得します。このデータブロックを読み取ると、(foo、k)の名前からiノード番号へのマッピングが得られます。その後、inode番号k(A1)の位置を見つけるためにinodeマップを再度参照し、最後にアドレスA0で目的のデータブロックを読み込みます。

再帰的更新問題[Z + 12]と呼ばれる、inodeマップが解決するもう1つの重大な問題がLFSにあります。この問題は、(LFSのような)決して更新されず、ディスク上の新しい場所に更新を移動するようなファイルシステムで発生します。

具体的には、iノードが更新されるたびに、ディスク上のその位置が変わります。私たちが注意していなかった場合、これはこのファイルを指すディレクトリへの更新を伴い、そのディレクトリの親ディレクトリの変更を強制していたでしょう。そしてそうなっていくと、ずっとファイルシステムツリーの上になるでしょう。

LFSは、この問題を賢明にinodeマップで回避します。inodeの場所が変更されても、変更はディレクトリ自体に反映されません。むしろ、imap構造体は更新されますが、ディレクトリは同じ名前から番号へのマッピングを保持します。したがって、間接参照によって、LFSは再帰的な更新の問題を回避します。

## 43.9 A New Problem: Garbage Collection
あなたはLFSに関する別の問題に気づいたかもしれません。ファイルの最新バージョン(そのinodeとデータを含む)をディスク上の新しい場所に繰り返し書き込みます。このプロセスは、効率的な書き込みを維持しながら、LFSが古いバージョンのファイル構造をディスク全体に分散させることを意味します。私たちは(むしろ無意識のうちに)これらの古いバージョンをゴミと呼んでいます。

たとえば、iノード番号kで参照される既存のファイルがある場合を考えてみましょう。このファイルは単一のデータブロックD0を指しています。このブロックを更新し、新しいinodeと新しいデータブロックの両方を生成します。その結果、LFSのディスク上のレイアウトは次のようになります(簡単にするためにimapやその他の構造体は省略しています。imapの新しいチャンクは新しいiノードを指すようにディスクに書き込む必要があります)。  
![](../43/img/fig43_1_10.PNG)  
この図では、iノードとデータブロックの両方に古いバージョン(左側のバージョン)と現在のバージョン(右側のバージョン)の2つのバージョンがあることがわかります。データブロックを(論理的に)更新する単純な行為によって、いくつかの新しい構造がLFSによって永続化されなければならず、したがって、前記ブロックの古いバージョンはディスク上に残されなければならない。もう1つの例として、元のファイルkにブロックを追加するとします。この場合、新しいバージョンのinodeが生成されますが、古いデータブロックはまだinodeによって指し示されます。したがって、それはまだ生きており、現在のファイルシステムの大部分です。  
![](../43/img/fig43_1_11.PNG)  
では、古いバージョンのinodeやデータブロックなどではどうしたらいいですか？古いバージョンのファイルを保持して、古いファイルのバージョンを復元できるようにすることもできます(たとえば、誤ってファイルを上書きまたは削除した場合など)。そのようなファイルシステムは、ファイルの異なるバージョンを追跡するため、バージョン管理ファイルシステムとして知られています。

しかし、代わりにLFSは最新のライブバージョンのみを保持します。したがって(バックグラウンドで)、LFSは定期的にファイルデータ、inode、およびその他の構造の古いバージョンを見つけて、それらをクリーニングする必要があります。このようにして、次の書き込みに使用するために、ディスク上のブロックを再びフリーにする必要があります。クリーニングのプロセスはガベージコレクションの一種であり、プログラムのために未使用のメモリを自動的に解放するプログラミング言語で発生するテクニックです。

以前は、LFSのディスクへの大きな書き込みを可能にするメカニズムと同じくらい重要なセグメントについても説明しました。それが判明したとき、彼らはまた、効果的なクリーンに不可欠です。LFSクリーナーが、クリーニング中に単一のデータブロック、iノードなどを単に通過して解放した場合に起こることを想像してください。その結果、ディスク上の割り当てられたスペースの間にいくつかのフリーの穴が混在したファイルシステムが作成されました。LFSは連続して高性能なディスクに書き込むための大きな連続領域を見つけることができないため、書き込みパフォーマンスはかなり低下します。

その代わりに、LFSクリーナーはセグメント単位で動作します。したがって、後続の書き込みのために大きなスペースのチャンクをクリアします。基本的なクリーニングプロセスは次のように動作します。定期的に、LFSクリーナーは古い(部分的に使用されている)セグメントを読み込み、これらのセグメント内にどのブロックが存在するかを判断し、その中にライブブロックだけを含む新しいセグメントセットを書き出し、古いものを解放し、書き込みます。具体的には、クリーナーは既存のM個のセグメントを読み込み、その内容をN個の新しいセグメント(N < M)に圧縮し、N個のセグメントを新しい場所にディスクに書き込みます。その後、古いM個のセグメントは解放され、後続の書き込みのためにファイルシステムによって使用されることができます。

しかし、今私たちは2つの問題を残しています。1つはメカニズムです：LFSはどのセグメント内のどのブロックが生きているのか、どのブロックが死んでいるのかを教えてくれますか？2番目はポリシーです：どのくらいの頻度でクリーナーを実行する必要がありますか、どのセグメントをクリーニングする必要がありますか？

## 43.10 Determining Block Liveness
最初にそのメカニズムに取り組んでいます。オンディスクセグメントS内のデータブロックDが与えられると、LFSはDが生存しているかどうかを判定できなければならない。そうするために、LFSは、各ブロックを記述する各セグメントに少し余分な情報を追加します。具体的には、LFSは、各データブロックDについて、そのiノード番号(それが所属するファイル)およびそのオフセット(ファイルのどのブロックであるか)を含みます。この情報は、segment summary blockと呼ばれるセグメントの先頭の構造体に記録されます。

この情報があれば、ブロックが生存しているか死んでいるかを判断するのは簡単です。ディスク上のアドレスAにあるブロックDについては、segment summary blockを調べ、そのinode番号NとオフセットTを見つけます。次に、imapを調べて、Nがどこにあるかを調べ、ディスクからNを読み込みます(おそらく、優れていればメモリ上に存在しています)。最後に、オフセットTを使用して、iノード(または間接ブロック)を調べ、iノードがこのファイルのT番目のブロックがディスク上にあると考える場所を確認します。それがディスクアドレスAを正確に指す場合、LFSはブロックDがライブであると結論付けることができます。他の場所を指す場合、LFSはDが使用されていない(すなわち、死んでいる)と判断し、このバージョンがもはや必要ではないことを知ることができます。このプロセスの擬似コードの要約を以下に示します。  
![](../43/img/fig43_1_12.PNG)  
次に、セグメントサマリブロック(SSとマークされている)が、アドレスA0のデータブロックが実際にオフセット0のファイルkの一部であることを記録するメカニズムを示す図です。kのimapをチェックすることによって、それが実際にその場所を指していることを確認してください。  
![](../43/img/fig43_1_13.PNG)  
LFSが生存率を決定するプロセスをより効率的にするために必要な、いくつかのショートカットがあります。たとえば、ファイルが切り捨てられたり削除されたりすると、LFSはそのバージョン番号を増やし、新しいバージョン番号をimapに記録します。また、オンディスクセグメントにバージョン番号を記録することで、LFSは、ディスク上のバージョン番号とimap内のバージョン番号を単純に比較することにより、上記のより長いチェックを短絡することができます。

## 43.11 A Policy Question: Which Blocks To Clean, And When?
上記のメカニズムに加えて、LFSには、クリーニングの時期とクリーニングの対象となるブロックの両方を決定する一連のポリシーが含まれている必要があります。クリーニングする時期を決めるのは簡単です。定期的、アイドル時間中、ディスクがいっぱいであるときです。

どのブロックを清掃すべきかを決定することはより困難であり、多くの研究論文の主題となっています。元のLFSの論文[RO91]では、著者はホットセグメントとコールドセグメントに分離しようとするアプローチを述べています。ホットセグメントは、内容が頻繁に上書きされるセグメントです。したがって、そのようなセグメントでは、より多くのブロックが(新しいセグメントで)上書きされ、使用のために解放されるように、それをクリーニングする前に長い時間待つことが最善の方針です。対照的に、コールドセグメントは、デッドブロックをいくつか持つことがありますが、残りの内容は比較的安定しています。したがって、著者らは、コールドセグメントをより早くクリーニングし、ホットセグメントを後でクリーニングし、正確にそれを行うものを開発すべきであると結論づけています。しかし、ほとんどのポリシーと同様に、このポリシーは完全ではありません。後のアプローチでは、よりうまくいく方法が示されています[MR + 97]。

## 43.12 Crash Recovery And The Log
1つの最終的な問題：LFSがディスクに書き込んでいるときにシステムがクラッシュするとどうなりますか？以前の章でジャーナリングについて思い出してきたように、更新中のクラッシュはファイルシステムにとっては厄介なものであり、LFSも考慮する必要があります。

通常の操作では、LFSはセグメントに書き込みを行い(バッファリング)、その後(セグメントがいっぱいになるか、またはある程度の時間が経過したときに)セグメントをディスクに書き込みます。LFSはこれらの書込みのログを作成します。すなわち、checkpoint regionはヘッドおよびテールセグメントを指し、各セグメントは書き込まれる次のセグメントを指します。LFSはcheckpoint regionも定期的に更新します。これらの操作(セグメントへの書き込み、CRへの書き込み)のいずれかでクラッシュが発生する可能性があります。LFSはこれらの構造への書き込み中のクラッシュをどのように処理しますか？

最初に2番目のケースを対処してみましょう。CR更新が原子的に確実に行われるように、LFSは実際にはディスクの両端に1つずつ2つのCRを保持し、交互に書き込みます。また、LFSは、iノードマップやその他の情報への最新のポインタでCRを更新するときに注意するプロトコルを実装します。具体的には、最初にヘッダー(タイムスタンプ付き)、次にCRの本文、最後に最後のブロック(タイムスタンプ付き)を書き出します。CRの更新中にシステムがクラッシュした場合、LFSは一貫性のないタイムスタンプのペアを見ることでこれを検出できます。LFSは常に一貫性のあるタイムスタンプを持つ最新のCRを使用することを選択し、CRの一貫した更新が達成されます。

最初のケースに対処しましょう。LFSは30秒ごとにCRを書き込むので、ファイルシステムの最後の整合性のあるスナップショットはかなり古いかもしれません。したがって、再起動時にLFSはcheckpoint region、それが指すimap部分、および後続のファイルとディレクトリを読み込むだけで簡単に回復できます。しかし、更新の最後の数秒は失われます。これを改善するために、LFSは、データベースコミュニティでロールフォワードと呼ばれる手法を使用して、これらのセグメントの多くを再構築しようとします。

基本的な考え方は、最後のcheckpoint regionから始まり、ログの終わりを見つけて(CRに含まれています)、それを使って次のセグメントを読み込み、そこに有効な更新があるかどうかを確認します。存在する場合、LFSはファイルシステムをそれに応じて更新し、最後のチェックポイント以降に書き込まれたデータとメタデータの多くを回復します。詳細については、Rosenblum賞を受賞した論文[R92]を参照してください。

## 43.13 Summary
LFSは、ディスクを更新する新しい方法を導入しています。LFSは、場所のファイルを上書きするのではなく、常にディスクの未使用部分に書き込み、後で古い空間をクリーニングによって再利用します。このアプローチは、データベースシステムではシャドウページング[L77]と呼ばれ、file system speakではコピーオンライトとも呼ばれ、それらを高効率で書き出すことが可能で、LFSはインメモリセグメントにすべての更新を集めて書き込むことができ、順次に一緒に出します。

>> TIP: TURN FLAWS INTO VIRTUES  
>> システムに根本的な欠陥がある場合はいつでも、それらの機能を調べ有用なものに変えることができるかどうかを確認してください。NetAppのWAFLは古いファイルの内容でこれを行います。古いバージョンを利用できるようにすることで、WAFLはかなり頻繁にするクリーンを心配する必要はなくなりました(最終的にはバックグラウンドで古いバージョンは削除されます)ので、クールな機能を提供し、すべて1つの素晴らしい考えでLFSクリーニングの問題の多くを対処します。システムにこれの他の例がありますか？間違いなく、あなたは自分で考える必要があります。なぜなら、この章は頭文字"O"で終わっているからです。…OVER

LFSが生成する一般的な大きな書き込みは、さまざまなデバイスでのパフォーマンスに優れています。ハードドライブでは、書き込みが大きくなるため、位置決め時間が最小限に抑えられます。RAID-4やRAID-5などのパリティベースのRAIDでは、書き込みの問題が完全に回避されます。最近の研究では、フラッシュベースのSSD [H + 17]の高性能化には大きなI/Oが必要であることが示されています。したがって、おそらく驚くべきことに、LFS形式のファイルシステムは、これらの新しい媒体であっても優れた選択肢となる可能性があります。

このアプローチの欠点は、ゴミを生成することです。データの古いコピーがディスク全体に散在し、後で使用するためにそのようなスペースを再利用したい場合は、古いセグメントを定期的にクリーニングする必要があります。クリーニングはLFSの多くの論争の焦点となり、クリーニングコスト(SS + 95)に対する懸念はおそらくLFSの当初のフィールドへの影響を制限していました。しかし、NetAppのWAFL [HLM94]、SunのZFS [B07]、Linuxのbtrfs [R + 13]、さらに現代のフラッシュベースのSSD [AD14]を含む最新の商用ファイルシステムでは、同様のコピーオンライトアプローチでディスクに書き込んでいます。LFSの知的遺産は、これらの現代的なファイルシステム上で生き続けています。特に、WAFLはそれらを機能に変えることでクリーニング問題を回避しました。スナップショットを使用して古いバージョンのファイルシステムを提供することにより、ユーザーは誤って現在のファイルを削除するたびに古いファイルにアクセスすることができます。

## 参考文献
[AD14] “Operating Systems: Three Easy Pieces”  
Chapter: Flash-based Solid State Drives  
Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau  
A bit gauche to refer you to another chapter in this very book, but who are we to judge?  

[B07] “ZFS: The Last Word in File Systems”  
Jeff Bonwick and Bill Moore  
Copy Available: http://www.ostep.org/Citations/zfs last.pdf  
Slides on ZFS; unfortunately, there is no great ZFS paper (yet). Maybe you will write one, so we can cite it here?

[H+17] “The Unwritten Contract of of Solid State Drives”  
Jun He, Sudarsun Kannan, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau  
EuroSys ’17, April 2017  
In this paper, we study which unwritten rules one must follow in order to extract high performance from an SSD, both in the short term and over the long haul. Interestingly, both request scale (having large requests) and request locality still matter, even on solid-state storage. The more things change ...

[HLM94] “File System Design for an NFS File Server Appliance”  
Dave Hitz, James Lau, Michael Malcolm  
USENIX Spring ’94  
WAFL takes many ideas from LFS and RAID and puts it into a high-speed NFS appliance for the multi-billion dollar storage company NetApp.

[L77] “Physical Integrity in a Large Segmented Database”  
R. Lorie  
ACM Transactions on Databases, 1977, Volume 2:1, pages 91-104 The original idea of shadow paging is presented here.

[MJLF84] “A Fast File System for UNIX”  
Marshall K. McKusick, William N. Joy, Sam J. Leffler, Robert S. Fabry  
ACM TOCS, August, 1984, Volume 2, Number 3  
The original FFS paper; see the chapter on FFS for more details.

[MR+97] “Improving the Performance of Log-structured File Systems  
with Adaptive Methods”  
Jeanna Neefe Matthews, Drew Roselli, Adam M. Costello, Randolph Y. Wang, Thomas E. Anderson  
SOSP 1997, pages 238-251, October, Saint Malo, France  
A more recent paper detailing better policies for cleaning in LFS.  

[M94] “A Better Update Policy”  
Jeffrey C. Mogul  
USENIX ATC ’94, June 1994  
In this paper, Mogul finds that read workloads can be harmed by buffering writes for too long and then sending them to the disk in a big burst. Thus, he recommends sending writes more frequently and in smaller batches.

[P98] “Hardware Technology Trends and Database Opportunities”  
David A. Patterson  
ACM SIGMOD ’98 Keynote Address, Presented June 3, 1998, Seattle, Washington  
Available: http://www.cs.berkeley.edu/˜pattrsn/talks/keynote.html  
A great set of slides on technology trends in computer systems. Hopefully, Patterson will create another of these sometime soon.

[R+13] “BTRFS: The Linux B-Tree Filesystem”  
Ohad Rodeh, Josef Bacik, Chris Mason  
ACM Transactions on Storage, Volume 9 Issue 3, August 2013  
Finally, a good paper on BTRFS, a modern take on copy-on-write file systems.

[RO91] “Design and Implementation of the Log-structured File System”  
Mendel Rosenblum and John Ousterhout  
SOSP ’91, Pacific Grove, CA, October 1991  
The original SOSP paper about LFS, which has been cited by hundreds of other papers and inspired many real systems.

[R92] “Design and Implementation of the Log-structured File System”  
Mendel Rosenblum  
http://www.eecs.berkeley.edu/Pubs/TechRpts/1992/CSD-92-696.pdf  
The award-winning dissertation about LFS, with many of the details missing from the paper.

[SS+95] “File system logging versus clustering: a performance comparison”  
Margo Seltzer, Keith A. Smith, Hari Balakrishnan, Jacqueline Chang, Sara McMains, Venkata Padmanabhan  
USENIX 1995 Technical Conference, New Orleans, Louisiana, 1995  
A paper that showed the LFS performance sometimes has problems, particularly for workloads with many calls to fsync() (such as database workloads). The paper was controversial at the time.

[SO90] “Write-Only Disk Caches”  
Jon A. Solworth, Cyril U. Orji  
SIGMOD ’90, Atlantic City, New Jersey, May 1990  
An early study of write buffering and its benefits. However, buffering for too long can be harmful: see Mogul [M94] for details.

[Z+12] “De-indirection for Flash-based SSDs with Nameless Writes”  
Yiying Zhang, Leo Prasath Arulraj, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau  
FAST ’13, San Jose, California, February 2013  
Our paper on a new way to build flash-based storage devices. Because FTLs (flash-translation layers) are usually built in a log-structured style, some of the same issues arise in flash-based devices that do in LFS. In this case, it is the recursive update problem, which LFS solves neatly with an imap. A similar structure exists in most SSDs.

\newpage

# 44 Flash-based SSDs
何十年ものハードディスクドライブの支配から、永続的なストレージデバイスの新しい形態が最近、世界で重要な役割を果たしました。一般にソリッドステートストレージと呼ばれるこのようなデバイスには、ハードドライブのような機械的または可動的な部品はありません。むしろ、メモリとプロセッサによく似たトランジスタから簡単に構築されています。しかし、典型的なランダムアクセスメモリ(例えば、DRAM)とは異なり、このようなソリッドステートストレージデバイス(SSD)は、電力損失にもかかわらず情報を保持するので、データの永続的記憶に使用するための理想的な候補です。

私たちが注目する技術は、1980年代に舛岡富士雄さんが作ったフラッシュ(より具体的には、NANDベースのフラッシュ)[M + 14]と呼ばれています。Flashには、いくつかのユニークな特性があります。例えば、与えられているチャンク(フラッシュページ)に書き込むためには、より大きなチャンク(すなわち、フラッシュブロック)を消去しなければならず、これはかなり高価になる可能性があります。さらに、あまりにも頻繁にページに書き込むと、ページが消耗します。これらの2つの特性は、フラッシュベースのSSDの構築を興味深い課題になっています。

>> CRUX: HOW TO BUILD A FLASH-BASED SSD  
>> どのようにフラッシュベースのSSDを構築できますか？どのようにして高価な消去の性質を処理できますか？繰り返し上書きするとデバイスが消耗してしまうので、どのようにして長時間続くデバイスを構築できますか？技術進歩は止まるだろうか？または驚くことがなくなるでしょうか？

## 44.1 Storing a Single Bit
フラッシュ・チップは、1つのトランジスタに1つ以上のビットを格納するように設計されています。トランジスタ内にトラップされた電荷のレベルは2進値にマッピングされます。Single Level Cell(SLC)フラッシュでは、トランジスタ内に1ビットのみが記憶される(すなわち、1または0)。Multi Level Cell(MLC)フラッシュでは、2ビットが異なるレベルの電荷に符号化され、例えば、00,01,10,11は、低、若干低、若干高、高レベルで表される。セルあたり3ビットをエンコードするTriple Level Cell(TLC)フラッシュもあります。全体として、SLCチップはより高い性能を達成し、より高価です。

>> TIP: BE CAREFUL WITH TERMINOLOGY  
>> フラッシュが文脈の中で私たちが何度も(ブロック、ページ)何度も使用してきた言葉ですが、以前とは若干異なる方法で使用されていることにお気づきかもしれません。新しい用語は、あなたの人生をより困難にするために作成されたものではありませんが(しかし、そうしたことがあるかもしれませんが)、用語の決定が行われる中心的な権限がないために生じます。文脈に応じて、他の人によってはページというのがブロックであったり、その逆もあります。あなたの仕事はシンプルです。各文章内の適切な用語を知り、規律に精通した人々があなたが話していることを理解できるようにそれらを使用します。唯一の解決策はシンプルですが、時には痛みを伴うことです。あなたの記憶を使用してください。

もちろん、そのようなビットレベルのストレージがデバイス物理のレベルでどのように動作するかについては、多くの詳細があります。この本の範囲を超えて、あなた自身の[J10]でそれについてもっと読むことができます。

## 44.2 From Bits to Banks/Planes
彼らが古代ギリシャで言うように、単一のビット(またはいくつか)を格納することはストレージシステムを作成することではありません。したがって、フラッシュチップは、多数のセルからなるbanks(バンク達)から作られます。または多数のセルからであるplanes(プレーン達)から構成されます。

バンクは2つの異なるサイズの単位、すなわち、一般に128KBまたは256KBのサイズのブロック(消去ブロックとも呼ばれる)、サイズが数KB(例えば4KB)のページごとにアクセスされます。各バンク内には多数のブロックがあります。各ブロック内に多数のページがあります。フラッシュを考えるときは、この新しい用語を覚えておく必要があります。これは、ディスクやRAIDで参照するブロック、および仮想メモリで参照するページとは異なります。

図44.1に、ブロックとページを持つフラッシュ・プレーンの例を示します。この単純な例では、それぞれが4ページを含む3つのブロックがあります。ブロックとページを区別する理由は次のとおりです。この区別は、読み取りや書き込みなどのフラッシュ操作で重要なものであり、さらにデバイスの全体的なパフォーマンスにとっても重要です。あなたが学ぶ最も重要な(そして奇妙な)ことは、ブロック内のページに書き込むためには、最初にブロック全体を消去しなければならないということです。このトリッキーな詳細は、フラッシュベースのSSDを面白くて価値のある課題にすることと、この章の後半の主題になります。  
![](../44/img/fig44_1.PNG)  

## 44.3 Basic Flash Operations
このフラッシュ構成を考えると、フラッシュチップがサポートする3つの低レベル動作があります。読み取りコマンドは、フラッシュからページを読み取るために使用されます。消去とプログラムは並行して書き込みに使用されます。

- Read (1ページ)：  
フラッシュチップのクライアントは、読み出しコマンドと適切なページ番号をデバイスに指定するだけで、任意のページ(例えば、2KBまたは4KB)を読み取ることができます。この操作は通常、デバイス上の場所に関係なく、以前の要求の場所にかかわらず(ディスクとはまったく異なります)、10マイクロ秒程度以上と非常に高速です。どの場所にも一様に迅速にアクセスできることは、そのデバイスがランダムアクセスデバイスであることを意味します。

- Erase(ブロック)：  
フラッシュ内のページに書き込む前に、デバイスの性質上、ページ内にあるブロック全体を最初に消去する必要があります。重要なことに、ブロックの内容を消去します(各ビットを値1に設定します)。そのため、ブロック内の必要なデータが、消去を実行する前に他の場所(メモリまたは別のフラッシュブロック)にコピーされていることを確認する必要があります。消去コマンドは非常に高価で、数ミリ秒かかります。終了すると、ブロック全体がリセットされ、各ページがプログラム可能な状態になります。

- Program(1ページ)：  
一度ブロックが消去されると、プログラムコマンドを使用して、ページ内の1を0に変更するなど、望んだ内容をフラッシュに書き込むことができます。ページのプログラミングは、ブロックを消去するよりも安価ですが、ページを読むよりもコストがかかります。通常、最新のフラッシュチップでは約100マイクロ秒です。

フラッシュチップについて考える一つの方法は、各ページに関連する状態があることです。ページはINVALID状態で開始します。ページが存在するブロックを消去することによって、ページの状態(およびそのブロック内のすべてのページ)をERASEDに設定します。これはブロック内の各ページの内容をリセットしますが、それらを(重要な)プログラマブルにします。ページをプログラミングすると、その状態がVALIDに変わり、その内容が設定され、読み込めることを意味します。読み込みはこれらの状態に影響しません(ただし、プログラムされたページからのみ読み込めます)。ページがプログラムされると、その内容を変更する唯一の方法は、ページが存在するブロック全体を消去することです。次に、4ページブロック内のさまざまな消去およびプログラム操作の後の状態遷移の例を示します。  
![](../44/img/fig44_1_1.PNG)  

### A Detailed Example
書き込みプロセス(すなわち、消去およびプログラミング)は非常に珍しいので、それが意味を成すことを確認するための詳細な例を見てみましょう。この例では、4ページのブロック内に次の4つの8ビットページがあるとします(非現実的に小さいサイズですが、この例では便利です)。各ページは以前にプログラムされているのでVALIDです。  
![](../44/img/fig44_1_2.PNG)  
今度は、新しい内容をページ0に書きたいとします。ページを書き込むには、最初にブロック全体を消去する必要があります。ブロックをこの状態にしておくと仮定しましょう  
![](../44/img/fig44_1_3.PNG)  
例えば、コンテンツ00000011でページ0をプログラムし、古いページ0(内容00011000)を上書きするとします。そうした後、ブロックは次のようになります。  
![](../44/img/fig44_1_4.PNG)  
ページ1,2,3の以前の内容はすべて消えてしまいました！したがって、ブロック内のページを上書きする前に、まず必要なデータを別の場所(メモリ、フラッシュ上のどこかなど)に移動する必要があります。消去の性質は、私たちがすぐに学ぶように、フラッシュベースのSSDをどのように設計するかで強く影響を与えます。

### Summary
要約すると、ページを読むのは簡単です。単にページを読むだけです。フラッシュチップはこれを非常にうまくやっています。パフォーマンス面では、機械的なシークおよびローテーションコストのために遅い現在のディスクドライブのランダムな読み取りパフォーマンスを大幅に上回る可能性があります。

ページを書くのは手間がかかります。ブロック全体が最初に消去されなければなりません(最初に必要なデータを別の場所に移動してください)そして望んだページをプログラムします。これは高価なだけでなく、このプログラム/消去サイクルを頻繁に繰り返すと、フラッシュチップには大きな信頼性の問題が生じる可能性があります。フラッシュを使用してストレージシステムを設計する場合、書き込みのパフォーマンスと信頼性が中心的な焦点です。近代のSSDがこれらの問題をどのように攻撃し、これらの制限にもかかわらず優れたパフォーマンスと信頼性を提供する方法について、すぐにわかります。

## 44.4 Flash Performance And Reliability
私たちは、生のフラッシュチップからストレージデバイスを構築することに興味があるので、基本的な性能特性を理解することは価値があります。図44.2は、一般的なプレス[V12]に見られるいくつかの数字の概略を示しています。ここでは、セルごとに1,2,3ビットの情報を格納するSLC、MLC、TLCフラッシュ全体での読み取り、プログラム、消去の基本動作レイテンシを示します。  
![](../44/img/fig44_2.PNG)  
表からわかるように、読み取りのレイテンシは非常に良いので、完了するのはわずか10マイクロ秒です。プログラムのレイテンシは、SLCの場合は200マイクロ秒と低く、より多くのビットを各セルにパックするほど高くなります。優れた書き込み性能を得るためには、複数のフラッシュ・チップを並行して使用する必要があります。最後に、消去はかなり高価で、典型的には数ミリ秒かかる。このコストを扱うことは、現代のフラッシュ記憶装置設計の中心です。

フラッシュチップの信頼性を考えてみましょう。(ドライブヘッドが記録面と実際に接触するような、厄介でかなり物理的なヘッドクラッシュを含む)様々な理由で故障する可能性がある機械的ディスクとは異なり、フラッシュチップは純粋なシリコンであり、その点で、少し信頼性の問題は心配です。主な関心事は摩耗です。フラッシュブロックが消去されてプログラムされると、少し余分な電荷がゆっくりと発生します。時間が経つにつれて、その余分な電荷が蓄積するにつれて、0と1とを区別するのがますます困難になります。不可能になる時点で、ブロックは使用不能になります。

ブロックの典型的な寿命は、現在よく知られていません。製造元は、MLCベースのブロックを10,000 P/E(プログラム/消去)サイクルの寿命として評価します。つまり、各ブロックを消去してから10,000回プログラムすると壊れます。SLCベースのチップは、1つのトランジスタにつき1ビットしか記憶しないため、より長い寿命、通常100,000 P/Eサイクルで定格されます。しかし、最近の研究では、生存期間が予想以上に長いことが示されています[BD10]。

フラッシュチップ内の1つの他の信頼性問題は、disturbance(妨害)として知られています。フラッシュ内の特定のページにアクセスすると、隣接するページのビットが反転する可能性があります。このようなビット反転は、ページがそれぞれ読み出されているかプログラムされているかに応じて、read disturbsまたはprogram disturbsとして知られています。

>> TIP: THE IMPORTANCE OF BACKWARDS COMPATIBILITY  
>> 下位互換性は、階層化されたシステムで常に懸念されます。2つのシステム間で安定したインタフェースを定義することにより、相互運用性を確保しながらインタフェースの両側で革新を実現できます。このアプローチは多くの領域で素晴らしい成功を収めてきました。オペレーティングシステムはアプリケーション用に比較的安定したAPIを持ち、ディスクはファイルシステムと同じブロックベースのインターフェースを提供し、IPネットワーキングスタックの各レイヤーは上記のレイヤーに固定された変更のないインターフェースを提供します。驚くことではないが、ある世代で定義されたインタフェースが次の世代では適切でない可能性があるため、そのような剛性には欠点があります。場合によっては、システム全体を完全に再設計することについて考えるのが有益な場合もあります。優れた例は、Sun ZFSファイルシステム[B07]にあります。ZFSの作成者は、ファイルシステムとRAIDの相互作用を再考することで、より効果的な統合全体を構想しました。

## 44.5 From Raw Flash to Flash-Based SSDs
フラッシュ・チップの基本的な理解を踏まえて、次のタスク、すなわち、フラッシュ・チップの基本セットを典型的なストレージ・デバイスのようなものに変える方法があります。標準記憶インターフェースは、単純なブロックベースのものであり、ブロックアドレスが与えられた場合、512バイト(またはそれ以上)のブロック(セクタ)が読み書き可能である。フラッシュベースのSSDのタスクは、標準のブロックインタフェースを内部の生のフラッシュチップが提供することです。内部的には、SSDはいくつかのフラッシュチップ(永続ストレージ用)で構成されています。SSDはある量の揮発性(すなわち、非永続的)メモリ(例えば、SRAM)も含みます。このようなメモリは、データのキャッシュ、バッファリングのためだけでなく、マッピングテーブルも役立ちます。最後に、SSDにはデバイスの操作を調整するための制御ロジックが含まれています。詳細は[A + 08]を参照してください。図44.3に簡略化したブロック図を示します。

![](../44/img/fig44_3.PNG)  

この制御ロジックの本質的な機能の1つは、クライアントの読み取りと書き込みを満足させ、必要に応じて内部のフラッシュ操作に変換することです。Flash Translation Layer(FTL)は、この機能を正確に提供します。FTLは、論理ブロック(デバイス・インタフェースを構成する)上で読取りと書込み要求を取り出し、物理ブロックおよび物理ページ(実際のフラッシュ・デバイスを構成する)上の低レベル読取り、消去、プログラム・コマンドに変換します。FTLは、優れた性能と高い信頼性を提供するという目的でこの作業を達成する必要があります。

優れたパフォーマンスは、技術の組み合わせによって実現できます。1つの鍵は、複数のフラッシュ・チップを並列に使用することです。この技術についてはこれ以上検討するつもりはありませんが、現代のSSDはすべて、内部で複数のチップを使用してより高い性能を得ています。もう1つのパフォーマンス目標は、書き込みの増幅を減らすことです。これは、FTLでフラッシュ・チップに発行された合計書き込みトラフィック(バイト単位)を合計書き込みトラフィック(バイト単位)で割ったものが、クライアントによってSSDに送信されます。以下で説明するように、FTL構築に対する単純なアプローチは、高い書き込み増幅率と低い性能につながります。

いくつかの異なるアプローチを組み合わせることで高い信頼性が達成されます。上記で議論したように、1つの主な問題は摩耗です。1つのブロックがあまりにも頻繁に消去され、プログラムされると、使用できなくなります。結果として、FTLは、フラッシュのブロック間にできるだけ均等に書き込みを広げて、デバイスのすべてのブロックがほぼ同時に消耗するようにする必要があります。そうすることはwear levelingと呼ばれ、現代のFTLに不可欠な部分です。

もう一つの信頼性の懸念は、プログラム妨害である。このような妨害を最小限に抑えるために、FTLは、通常、消去されたブロック内のページを、低いページから高いページの順にプログラムします。このsequential programming(順序プログラミング)手法は妨害を最小限に抑え、広く利用されています。

## 44.6 FTL Organization: A Bad Approach
FTLの最も単純な構成は、direct mapped(ダイレクトマップ)と呼ばれるものです。このアプローチでは、論理ページNへの読み出しは、物理ページNの読み出しに直接マッピングされます。論理ページNへの書き込みは、より複雑です。FTLは、最初に、ページNが内包されているブロック全体を読み取らなければいけません。そのとき、そのブロックを消去する必要があります。最後に、FTLは古いページと新しいページをプログラムします。

おそらく推測できるように、ダイレクトマップFTLには、パフォーマンスと信頼性の両方の点で多くの問題があります。各書き込みではパフォーマンス上の問題が発生します。デバイスはブロック全体の読み取り(コスト高)、消去(非常に高価)、プログラム(コスト高)の順に処理する必要があります。最終的な結果は、(ブロック内のページ数に比例する)厳しいライト増幅と、その結果として、機械的なシークと回転遅延を伴う典型的なハードドライブよりも遅い、ひどい書き込み性能です。

さらに悪いのは、このアプローチの信頼性です。ファイルシステムのメタデータまたはユーザーファイルのデータが繰り返し上書きされると、同じブロックが消去、プログラムされ、何度も繰り返し書き換えられ、すぐに消耗してデータが失われる可能性があります。ダイレクト・マップ・アプローチでは、クライアント・仕事量の消耗を制御することができません。仕事量が論理ブロック全体に均等に書き込み負荷を分散しない場合、一般的なデータを含む物理ブロックがすぐに消耗します。信頼性とパフォーマンスの両方の理由から、ダイレクトマップFTLは悪い考えです。

## 44.7 A Log-Structured FTL
このような理由から、現在のFTLは、ログ構造であり、ストレージデバイス(上に示すように)とその上のファイルシステム(log structured file systemsの章を参照)の両方で有用なアイデアです。論理ブロックNへの書き込みの際に、デバイスは、書き込まれているブロック内の次の空き領域に書き込みを追加します。私たちはこれを書き込みロギングスタイル呼んでいます。ブロックNの後続の読み取りを可能にするために、デバイスはマッピングテーブルを保持しています(メモリ上に、そしてデバイス上の何らかの形で永続的に)。このテーブルには、システム内の各論理ブロックの物理アドレスが格納されます。

基本的なログベースのアプローチがどのように機能するかを理解するための例を見てみましょう。クライアントには、デバイスは一般的なディスクのように見え、512バイトのセクタ(またはセクタのグループ)に読み書きできます。簡単にするために、クライアントが4KBのチャンクを読み書きしているとします。SSDには、4 KBの4つのページにそれぞれ分割された16 KBサイズのブロックがいくつか含まれているとします。これらのパラメータは非現実的です(フラッシュブロックは通常より多くのページで構成されます)が、私たちの教訓の目的を十分に果たします。クライアントが次の一連の操作を発行するとします。  
- Write(100) with contents a1  
- Write(101) with contents a2  
- Write(2000) with contents b1  
- Write(2001) with contents b2  

これらの論理ブロックアドレス(例えば100)は、SSDのクライアント(例えば、ファイルシステム)が情報がどこに位置しているかを記憶するために使用されます。内部的には、デバイスは、これらのブロック書き込みを、生のハードウェアによってサポートされている消去およびプログラム動作に変換し、各論理ブロックアドレスに対して、SSDのどの物理ページがそのデータを記憶するかを何らかの形で記録しなければいけません。SSDのすべてのブロックが現在有効ではないと仮定し、ページをプログラムする前に消去する必要があります。ここでは、すべてのページがINVALID(i)とマークされたSSDの初期状態を示します。  
![](../44/img/fig44_3_1.PNG)  
第1の書き込みが(論理ブロック100への)SSDによって受信されると、FTLは、それを4つの物理ページ：0,1,2,3を含む物理ブロック0に書き込むことを決定します。ブロックは消去されていないので、まだそれに書き込むことはできません。デバイスは最初に消去コマンドをブロック0に発行する必要があります。削除すると、次の状態になります。  
![](../44/img/fig44_3_2.PNG)  
これでブロック0はプログラム可能な状態になります。ほとんどのSSDは、順番にページに書き込みます(すなわち、低から高に)、プログラム妨害に関連する信頼性の問題を低減します。そのときSSDは、論理ブロック100の書き込みを物理ページ0に指示する。  
![](../44/img/fig44_3_3.PNG)  
しかし、クライアントが論理ブロック100を読み込みたい場合はどうなりますか？どのようにそれがどこにあるのか見つけることができますか？SSDは、論理ブロック100に発行された読取りを物理ページ0の読取りに変換しなければないけません。このような機能に対応するために、FTLは論理ブロック100を物理ページ0に書込むとき、この事実をメモリ内マッピングテーブルに記録します。このマッピングテーブルの状態を図でも追跡します  
![](../44/img/fig44_3_4.PNG)  
これで、クライアントがSSDに書き込むときに何が起こるかを確認できます。SSDは書き込みの場所を見つけます。通常、次の空きページを選択するだけです。そのときブロックの中身であるそのページをプログラムし、マッピングテーブルに論理から物理へのマッピングを記録します。後続の読み取りは、テーブルを使用して、クライアントが提示した論理ブロックアドレスを、データを読み取るのに必要な物理ページ番号に変換するだけです。例題である書き込みストリームの残りの書き込み(101、2000、2001)を調べてみましょう。これらのブロックを書き込んだ後、デバイスの状態は次のようになります。  
![](../44/img/fig44_3_5.PNG)  
ログベースのアプローチでは、パフォーマンスが向上し(しばらくの間一回しか必要な消去が行われず、ダイレクト・マップ・アプローチによる高価なread modify writeが回避されます)、信頼性が大幅に向上します。FTLは、すべてのページにわたって書き込みを分散し、wear levelingと呼ばれる処理を実行し、デバイスの寿命を延ばすことができます。さらに下のwear levelingについて説明します。

>> ASIDE: FTL MAPPING INFORMATION PERSISTENCE  
>> あなたは疑問に思うかもしれません。デバイスが電力を失う場合はどうなりますか？メモリ内のマッピングテーブルは消えますか？明らかに、このような情報は本当に失われてはいけません。なぜなら、デバイスは永続的なストレージデバイスとして機能しないためです。SSDには、マッピング情報を回復する手段が必要です。

最も簡単なことは、Out Of Band(OOB)領域と呼ばれる、各ページでマッピング情報を記録することです。デバイスの電源が切れて再起動されると、デバイスはOOB領域をスキャンしてメモリ内のマッピングテーブルを再構築します。この基本的なアプローチには問題があります。必要なすべてのマッピング情報を見つけるために大きなSSDをスキャンするのは遅いです。この制限を克服するために、一部のハイエンド端末では、より複雑なロギングとチェックポイント手法を使用して復旧を高速化しています。ロギングについては後でファイルシステムで詳しく説明します。

残念なことに、このログ構造化の基本的なアプローチにはいくつかの欠点があります。1つ目は、論理ブロックの上書きが、ガベージと呼ばれるもの、つまりドライブの周りの古いバージョンのデータとスペースを占有することにつながるということです。デバイスは、前記ブロックおよび将来の書き込みのための空き領域を見つけるためにガーベッジコレクション(GC)を定期的に実行しなければいけません。しかし、過度のガベージコレクションは書き込み増幅を引き上げ、パフォーマンスを低下させます。2つめは、メモリ内マッピングテーブルのコストが高いことです。デバイスが大きくなればなるほど、そのようなテーブルに必要なメモリが増えます。今度は順番にこれらについて話し合っていきます。

## 44.8 Garbage Collection
このようなログ構造アプローチの第1のコストは、ゴミが作成されることであり、ガベージコレクション(すなわち、dead-block reclamation)が行われなければいけません。私たちの継続的な例を使ってこれを理解しましょう。論理ブロック100,101,2000,2001がデバイスに書き込まれたことを思い出してください。さて、ブロック100とブロック101が内容c1とc2で再び書き込まれたとしましょう。書き込みは次の空きページ(この場合は物理ページ4と5)に書き込まれ、それに応じてマッピングテーブルが更新されます。このようなプログラミングを可能にするには、最初にブロック1を消去する必要があります。  
![](../44/img/fig44_3_6.PNG)  

物理ページ0と1はVALIDとマークされていますが、ブロック100と101の古いバージョンのようなゴミがあります。デバイスのログ構造上の性質のため、上書きするとガベージブロックが作成されます。そのため、新しい書き込みを実行するための空き領域を確保するためにデバイスを再利用する必要があります。

ガベージ・ブロック(デッド・ブロックとも呼ばれます)を見つけ出し、将来使用するためにそれらを再利用するプロセスは、ガベージ・コレクションと呼ばれ、現代のSSDの重要なコンポーネントです。基本的なプロセスは、1つまたは複数のゴミ・ページを含むブロックを見つけ、そのブロックからライブ(非ゴミ)ページを読み込み、それらのライブ・ページをログに書き出し、そして(最後に)書き込みに使用するためにブロック全体を再利用します。

例をあげて説明しましょう。デバイスは、上記のブロック0内の死んだページを再利用したいと決定します。ブロック0には、2つのデッドブロック(ページ0と1)と2つのライフブロック(ページ2と3、それぞれブロック2000と2001が含まれています)があります。そうするために、デバイスは以下を行います。  
- ブロック0からライブデータ(2および3ページ)を読み込む  
- ライブデータをログの最後に書き込む  
- ブロック0を消去する(後で使用するために解放する)   

ガベージコレクタを機能させるには、SSDが各ページがライブかデッドかを判断できるように、各ブロック内に十分な情報がなければなりません。この目的を達成する1つの自然な方法は、各ブロック内のある場所に、各ページ内にどの論理ブロックが格納されているかに関する情報を格納することです。デバイスは、マッピングテーブルを使用して、ブロック内の各ページがライブデータを保持するかどうかを判断できます。

(ガベージコレクションが行われる前の)この例から、ブロック0は論理ブロック100,101,2000,2001を保持していました。マッピングテーブルをチェックして(ガベージコレクション前のマッピングテーブル100 -> 4,101 -> 5、2000 ->2、2001 -> 3を含んでいる)、デバイスは、SSDブロック内の各ページがライブ情報を保持しているかどうかを容易に判定することができます。例えば、2000と2001はまだマップに参照されています。100と101はそうではないため、ガベージコレクションの候補となります。このガベージコレクションプロセスがこの例で完了すると、デバイスの状態は次のようになります。  
![](../44/img/fig44_3_7.PNG)  
ご覧のように、ガベージコレクションは高価な場合があり、ライブデータの読み込みと書き換えが必要です。再利用の理想的な候補は、死んだページだけで構成されるブロックです。この場合、高価なデータ移行なしでブロックを即座に消去して新しいデータに使用することができます。GCのコストを削減するために、一部のSSDはデバイスをoverprovisionをします[A + 08]。余分なフラッシュ容量を追加することで、クリーニングを遅らせてバックグラウンドにプッシュすることができ、デバイスが忙しくないときに実行されます。容量を増やすと、内部帯域幅も増加します。これは、クリーニングに使用できるため、クライアントに認識される帯域幅に害を与えません。現代の多くのドライブは、このように過度にoverprovisionし、優れた全体的なパフォーマンスを達成するための1つの鍵です。

## 44.9 Mapping Table Size
ログ構造化の第2のコストは、非常に大きなマッピングテーブルの可能性であり、デバイスの4 KBページごとに1つのエントリがあります。たとえば、1 TBの大きなSSDを使用すると、4 KBのページごとに1つの4バイトエントリが発生するため、これらのマッピングのためだけに1 GBのメモリが必要になります。従って、このページレベルFTL方式は実用的ではありません。

### Block-Based Mapping
マッピングのコストを削減する1つのアプローチは、ページごとではなく、デバイスのブロックごとにポインタを保持することだけであり、マッピング情報の量をSizeblock/Sizepageの係数だけ減少させます。このブロック・レベルのFTLは、仮想メモリー・システムでより大きなページ・サイズを持つことに似ています。その場合は、VPNのビット数を少し使って、仮想アドレスごとのオフセットを大きくします。

残念ながら、ログベースのFTL内でブロックベースのマッピングを使用することは、パフォーマンスの理由からうまく機能しません。最大の問題は、「小さな書き込み」で発生します(すなわち、物理ブロックのサイズより小さいもの)。この場合、FTLは古いブロックから大量のライブデータを読み込み、それを新しい書き込みにコピーする必要があります(小さな書き込みのデータと一緒に)。このデータコピーは、書き込み増幅を大幅に増加させ、したがって性能を低下させます。

この問題をより明確にするために、例を見てみましょう。クライアントが以前に論理ブロック2000,2001,2002,2003(内容、a、b、c、d)を書き出し、それらが物理ページ4,5,6,7の物理ブロック1内にあると仮定します。ページごとのマッピングでは、変換テーブルは、2000→4,2001→5,2002→6,2003→7の4つの論理ブロックのマッピングを記録する必要があります。

代わりに、ブロックレベルのマッピングを使用する場合、FTLはすべてのデータに対して単一のアドレス変換を記録するだけで済みます。ただし、アドレスマッピングは前の例と少し異なります。具体的には、フラッシュ内の物理ブロックのサイズであるチャンクに細断されるデバイスの論理アドレス空間を考えます。したがって、論理ブロックアドレスは、チャンク番号とオフセットの2つの部分からなります。4つの論理ブロックが各物理ブロック内に収まると仮定しているので、論理アドレスのオフセット部分は2ビット必要になります。残りの(最上位)ビットがチャンク番号を形成します。  
![](../44/img/fig44_3_8.PNG)  
ブロックベースのFTLでは、読み込みが容易です。最初に、FTLは、アドレスから最上位のビットを取り出すことによって、クライアントによって提示された論理ブロックアドレスからチャンク番号を抽出する。そのとき、FTLはテーブル内のチャンク番号から物理ページへのマッピングを検索します。最後に、FTLは、論理アドレスからのオフセットをブロックの物理アドレスに追加することによって、望んだフラッシュページのアドレスを計算します。

例えば、クライアントが論理アドレス2002への読出しを発行する場合、デバイスは論理チャンク番号(500)を抽出し、マッピングテーブル内の変換を検索し(4を見つける)、論理アドレス(2)からのオフセットを加算して変換します(4)。結果の物理ページアドレス(6)は、データが配置されている場所です。FTLはその物理アドレスに読み出しを発行し、望んだデータ(c)を得ることができます。

しかし、クライアントが論理ブロック2002(内容c')に書き込むとどうなるでしょうか？この場合、FTLは2000,2001,2003を読み込み、4つの論理ブロックすべてを新しい場所に書き出し、それに応じてマッピング・テーブルを更新する必要があります。ここに示すように、ブロック1(常駐していたデータ)は消去して再利用できます。  
![](../44/img/fig44_3_9.PNG)  
この例からわかるように、ブロックレベルのマッピングは変換に必要なメモリ量を大幅に削減しますが、書き込みがデバイスの物理ブロックサイズよりも小さくなるとパフォーマンスに大きな問題が発生します。実際の物理ブロックは256KB以上になる可能性があるため、このような書き込みは非常に頻繁に発生する可能性があります。従って、よりよい解決策が必要です。その解決策が何であるかを私たちが教えてきた中で思いつくことはできますか？あなたはこの後を読む前に、あなた自身でそれを理解できますか？

### Hybrid Mapping
柔軟な書き込みを可能にするとともにマッピングコストを削減するために、最新のFTLの多くはハイブリッドマッピング技術を採用しています。このアプローチでは、FTLはいくつかのブロックを消去したままにして、すべての書き込みをそれらに指示します。これらはログブロックと呼ばれます。FTLは、純粋なブロック・ベースのマッピングで必要とされるすべてのコピーを行わずに、ログ・ブロック内の任意の場所の任意のページに書き込めるように、これらのログ・ブロックのページ単位のマッピングを保持します。

したがって、FTLには論理的に、メモリに2種類のマッピングテーブルがあります。すなわち、ログテーブルと呼ばれるページ単位のマッピングの小さなセットと、データテーブルのブロック単位のマッピングの大きなセットです。特定の論理ブロックを探すとき、FTLはまずログテーブルを調べます。論理ブロックの位置がそこに見つからない場合、FTLはデータテーブルを参照してその位置を見つけ、要求されたデータにアクセスします。

ハイブリッドマッピング戦略の鍵は、ログブロックの数を小さく保つことです。ログブロックの数を少なく保つために、FTLはログブロック(ページごとのポインタを持つ)を定期的に調べて、単一のブロックポインタだけが指し示すことができるブロックに切り替える必要があります。このスイッチは、ブロック[KK + 02]の内容に基づいて3つの主な技術のうちの1つによって実行されます。

たとえば、FTLが以前に論理ページ1000,1001,1002,1003を書き出し、物理ブロック2(物理ページ8,9,10,11)に配置したとします。1000,1001,1002,1003への書き込みの内容をそれぞれa、b、c、dとします。  
![](../44/img/fig44_3_10.PNG)  
ここで、クライアントが、現在利用可能なログブロックの1つ、例えば物理ブロック0(物理ページ0,1,2,3)に、これらのページ(データa', b', c', d')を全く同じ順序で上書きすると仮定します。この場合、FTLの状態は次のようになります。  
![](../44/img/fig44_3_11.PNG)  
これらのブロックは以前と全く同じ方法で記述されているため、FTLはswitch mergeと呼ばれる処理を実行できます。この場合、ログブロック(0)はページ0,1,2,3の格納場所になり、1つのブロックポインタで参照されます。古いブロック(2)は消去され、ログブロックとして使用されます。この最良の場合、ページ単位のポインタはすべて単一のブロックポインタに置き換えられます。  
![](../44/img/fig44_3_12.PNG)  
このswitch mergeは、ハイブリッドFTLの最良のケースです。残念ながら、FTLはあまり運がない場合もあります。同じ初期条件(物理ブロック2に格納されている論理ブロック0,1,2,4)があり、クライアントが論理ブロック0と1だけを上書きする場合を考えてみましょう。  
![](../44/img/fig44_3_13.PNG)  
この物理ブロックの他のページを再統一するために、単一のブロックポインタだけでそれらを参照できるようにするため、FTLはpartial merge(部分マージ)と呼ばれる処理を実行します。この操作では、2と3がブロック4から読み込まれ、ログに追加されます。結果として得られるSSDの状態は、上記のswitch mergeと同じです。ただし、この場合、FTLはその目的を達成するために余分なI/Oを実行する必要がありました(物理ページ18と19から論理ブロック2と3を読み取り、物理ページ22と23に書き出す)。したがって、書き込み増幅が増加します。

フル・マージ(full merge)と呼ばれるFTLが直面する最後のケースであり、さらに多くの作業が必要です。この場合、FTLは、クリーニングを実行するために他の多くのブロックからページを集める必要があります。たとえば、ページ0、4、8、12がログブロックAに書き込まれたとします。このログブロックをブロックマップページに切り替えるには、まずFTLが論理ブロック0,1,2,3を含むデータブロックを作成する必要があります。したがって、FTLは1,2,3を別の場所から読み取り、0,1,2,3を一緒に書き出す必要があります。次に、マージは、論理ブロック4について同じことをして、5,6,7を見つけてそれらを単一のデータブロックに調整する必要があります。論理ブロック8および12に対して同じことが行われなければならず、次に(最終的に)、ログブロックAは解放されます。驚くことではないが、頻繁な完全なマージはパフォーマンスに重大な損害を与える可能性があります[GY + 09]。

## 44.10 Wear Leveling
最後に、現代のFTLが実装しなければならない関連するバックグラウンド活動は、上記のようにウェアレベリングです。基本的な考え方は簡単です。複数の消去/プログラム・サイクルがフラッシュ・ブロックを消耗するため、FTLはデバイスのすべてのブロックにその作業を均等に広げるために最善を尽くすべきです。このようにして、すべてのブロックは、「人気の」ブロックがすぐに使用できなくなる代わりに、ほぼ同じ時間に消耗します。

基本的なログ構造化アプローチは、書き込み負荷を分散させるための最初の良い仕事を行い、ガベージコレクションも役立ちます。ただし、ブロックが上書きされない長寿命のデータでいっぱいになることがあります。この場合、ガベージコレクションはブロックを再利用することはないため、書き込み負荷の公平性を受け取ることはありません。

この問題を解決するには、FTLは定期的にそのブロックからライブデータをすべて読み込み、別の場所に書き直す必要があります。したがって、ブロックを再度書き込み可能にする必要があります。このウェアレベリングのプロセスは、SSDの書き込み増幅を増加させ、したがって、すべてのブロックがほぼ同じレートで確実に摩耗するように、余分なI/Oが必要になるため、パフォーマンスが低下します。多くの異なるアルゴリズムが文献[A + 08、M + 14]が存在します。これらに興味を持ったのであれば読んでみてください。

## 44.11 SSD Performance And Cost
クロージングする前に、最新のSSDのパフォーマンスとコストを調べて、永続的なストレージシステムでどのように使用されるのかを理解してみましょう。どちらの場合も、従来のハードディスクドライブ(HDD)と比較し、両者の最大の違いを強調します。

### Performance
ハード・ディスク・ドライブとは異なり、フラッシュ・ベースのSSDは機械的なコンポーネントを備えておらず、実際には「ランダム・アクセス」デバイスであるという点でDRAMに多くの点で似ています。ディスクドライブと比較してパフォーマンスの最大の違いは、ランダムな読み書きを実行するときに実現されます。一般的なディスクドライブでは1秒間に数百回のランダムI/Oしか実行できませんが、SSDの方がはるかに優れています。ここでは、近代的なSSDのデータを使用して、SSDの性能がどれだけ優れているかを確認します。我々は、FTLが生のチップの性能問題をどれくらいうまく隠すかに特に関心があります。

表44.4に、3つの異なるSSDと最先端のハードドライブのパフォーマンスデータを示します。データはいくつかの異なるオンラインソースから取得しました[S13、T15]。左の2つの列はランダムなI/Oパフォーマンスを示し、右の2つの列は順次なI/Oです。最初の3行はSamsung、Seagate、Intelの3種類のSSDのデータを表示し、最後の行はハードディスクドライブ(またはHDD)のパフォーマンスを示します。この場合はSeagateのハイエンドドライブです。

![](../44/img/fig44_4.PNG)  

テーブルから興味深い事実をいくつか学ぶことができます。まず、最も劇的なのは、SSDと唯一のハードドライブ間のランダムI/Oパフォーマンスの違いです。SSDはランダムI/Oで数十MB/sから数百MB/sを達成していますが、この「高性能」ハードドライブでは数MB/sのピークがあります(実際には2 MB/sになるように丸めました)。第2に、順次I/Oパフォーマンスの点で、違いがはるかに少ないことがわかります。順次I/Oパフォーマンスが必要なだけであれば、SSDのパフォーマンスでも向上しますが、ハードドライブはまだまだ良い選択です。第3に、SSDのランダム読み取りパフォーマンスがSSDランダム書き込みパフォーマンスほど良くないことがわかります。

このような予期しなかった良好なランダム書き込みパフォーマンスの理由は、ランダム書き込みを順次書き込みに変換してパフォーマンスを向上させる、多くのSSDのログ構造設計によるものです。最後に、SSDは順次I/OとランダムI/Oのパフォーマンスに差があるため、ハードドライブのファイルシステムを構築する方法については、後続の章で学習する多くのテクニックがSSDに適用されます。順次I/OとランダムI/Oの差の大きさは小さくなりますが、ランダムI/Oを減らすためにファイルシステムを設計する方法を慎重に考慮するには十分なギャップがあります。

### Cost
上で見たように、SSDのパフォーマンスは、順次I/Oを実行している場合でも、現代のハードドライブを大幅に上回ります。それでは、なぜSSDは記憶媒体としてハードドライブを完全に置き換わっていないのですか？その答えは簡単です。コスト、具体的には、容量単位あたりのコストです。現在、[A15]のSSDは、250GBのドライブでは150ドルの費用がかかります。そのようなSSDのコストは60セント/GBです。一般的なハードドライブは1TBのストレージで約50ドルかかります。つまり、1GBあたり5セントです。これら2つの記憶媒体の間にはまだ10倍以上のコスト差があります。

これらのパフォーマンスとコストの違いにより、大規模なストレージシステムの構築方法が決まります。パフォーマンスが主な関心事である場合、特にランダムな読み取りパフォーマンスが重要な場合は、SSDを使用するのが最適です。一方、大規模なデータセンターを組み立てて膨大な量の情報を保管したい場合、大きなコスト差がハードドライブに向かうでしょう。もちろん、ハイブリッドアプローチである、SSDとハードドライブの両方で組み立てられているストレージシステムがあります。より使用頻度が高い「ホット」データ用の少数のSSDで高性能を提供しつつ、コストを節約するためにハードドライブ上の(使用頻度の低い)データを保管するハイブリッドアプローチがあります。価格差が存在する限り、ハードドライブは使われ続けます。

## 44.12 Summary
フラッシュベースのSSDは、世界の経済を動かすデータセンター内のラップトップ、デスクトップ、およびサーバーで共通の存在になっています。したがって、あなたはおそらくそれらについて何かを知るべきでしょうか？

ここで悪いニュースがあります。この章(この本の多くのもののように)は、最新の状態を理解するための最初のステップにすぎません。実際のデバイス性能に関する研究(Chen et alらの[CK + 09]やGrupp et alらの[GC + 09]など)、FTLデザインの問題(作品を含むGuptaらの[GY + 09]、Huangらの[H + 14]、Kimらの[KK + 02]、Leeらの[L + 07]、Agrawalらの[A + 08] 、Zhangらの[Z + 12])、フラッシュで構成される分散システム(Gordon らの[CG + 09]およびCORFU らの[B + 12]を含む)があります。

学術論文を読むだけではありません。一般的なプレス(例えば、[V12])の最近の進歩についても読無必要があります。そこでは、実用的な(まだ、便利ではない)情報を学ぶことができます。サムスンが同じSSD内でTLCとSLCセルの両方を使用してパフォーマンスを最大化する(SLCが書き込みをすばやくバッファに入れることができる)だけでなく、最大容量(TLCはセルあたりより多くのビットを格納できる)を行っています。そして、彼らが言うように、これは氷山の先端です。おそらくMaらの優秀な(そして最近の)調査[M + 14]から始まって、あなた自身がこの研究の"氷山"についてもっと詳しく知ってください。しかし、注意してください。氷山は船の中でも最も強力なものを沈めることができます[W15]。

## 参考文献
[A+08] “Design Tradeoffs for SSD Performance”  
N. Agrawal, V. Prabhakaran, T. Wobber, J. D. Davis, M. Manasse, and R. Panigrahy  
USENIX ’08, San Diego California, June 2008  
An excellent overview of what goes into SSD design.

[A15] “Amazon Pricing Study”  
Remzi Arpaci-Dusseau  
February, 2015  
This is not an actual paper, but rather one of the authors going to Amazon and looking at current prices of hard drives and SSDs. You too can repeat this study, and see what the costs are today. Do it!

[B+12] “CORFU: A Shared Log Design for Flash Clusters”  
M. Balakrishnan, D. Malkhi, V. Prabhakaran, T. Wobber, M. Wei, J. D. Davis  
NSDI ’12, San Jose, California, April 2012  
A new way to think about designing a high-performace replicated log for clusters using Flash.

[BD10] “Write Endurance in Flash Drives: Measurements and Analysis”  
Simona Boboila, Peter Desnoyers  
FAST ’10, San Jose, California, February 2010  
A cool paper that reverse engineers flash-device lifetimes. Endurance sometimes far exceeds manufacturer predictions, by up to 100×.

[B07] “ZFS: The Last Word in File Systems”  
Jeff Bonwick and Bill Moore  
Available: http://www.ostep.org/Citations/zfs last.pdf  
Was this the last word in file systems? No, but maybe it’s close.

[CG+09] “Gordon: Using Flash Memory to Build Fast, Power-efficient Clusters for Data-intensive Applications”  
Adrian M. Caulfield, Laura M. Grupp, Steven Swanson  
ASPLOS ’09, Washington, D.C., March 2009  
Early research on assembling flash into larger-scale clusters; definitely worth a read.

[CK+09] “Understanding Intrinsic Characteristics and System Implications of Flash Memory based Solid State Drives”  
Feng Chen, David A. Koufaty, and Xiaodong Zhang  
SIGMETRICS/Performance ’09, Seattle, Washington, June 2009  
An excellent overview of SSD performance problems circa 2009 (though now a little dated).

[G14] “The SSD Endurance Experiment”  
Geoff Gasior  
The Tech Report, September 19, 2014  
Available: http://techreport.com/review/27062  
A nice set of simple experiments measuring performance of SSDs over time. There are many other similar studies; use google to find more.

[GC+09] “Characterizing Flash Memory: Anomalies, Observations, and Applications”  
L. M. Grupp, A. M. Caulfield, J. Coburn, S. Swanson, E. Yaakobi, P. H. Siegel, J. K. Wolf  
IEEE MICRO ’09, New York, New York, December 2009  
Another excellent characterization of flash performance.

[GY+09] “DFTL: a Flash Translation Layer Employing Demand-Based Selective Caching of Page-Level Address Mappings”  
Aayush Gupta, Youngjae Kim, Bhuvan Urgaonkar  
ASPLOS ’09, Washington, D.C., March 2009  
This paper gives an excellent overview of different strategies for cleaning within hybrid SSDs as well as a new scheme which saves mapping table space and improves performance under many workloads.

[H+14] “An Aggressive Worn-out Flash Block Management Scheme  
To Alleviate SSD Performance Degradation”  
Ping Huang, Guanying Wu, Xubin He, Weijun Xiao  
EuroSys ’14, 2014  
Recent work showing how to really get the most out of worn-out flash blocks; neat!  

[J10] “Failure Mechanisms and Models for Semiconductor Devices”  
Report JEP122F, November 2010  
Available: http://www.jedec.org/sites/default/files/docs/JEP122F.pdf  
A highly detailed discussion of what is going on at the device level and how such devices fail. Only for those not faint of heart. Or physicists. Or both.

[KK+02] “A Space-Efficient Flash Translation Layer For Compact Flash Systems”  
Jesung Kim, Jong Min Kim, Sam H. Noh, Sang Lyul Min, Yookun Cho  
IEEE Transactions on Consumer Electronics, Volume 48, Number 2, May 2002  
One of the earliest proposals to suggest hybrid mappings.

[L+07] “A Log Buffer-Based Flash Translation Layer Using Fully-Associative Sector Translation”  
Sang-won Lee, Tae-Sun Chung, Dong-Ho Lee, Sangwon Park, Ha-Joo Song  
ACM Transactions on Embedded Computing Systems, Volume 6, Number 3, July 2007  
A terrific paper about how to build hybrid log/block mappings.

[M+14] “A Survey of Address Translation Technologies for Flash Memories”  
Dongzhe Ma, Jianhua Feng, Guoliang Li  
ACM Computing Surveys, Volume 46, Number 3, January 2014  
Probably the best recent survey of flash and related technologies.

[S13] “The Seagate 600 and 600 Pro SSD Review”  
Anand Lal Shimpi  
AnandTech, May 7, 2013  
Available: http://www.anandtech.com/show/6935/seagate-600-ssd-review  
One of many SSD performance measurements available on the internet. Haven’t heard of the internet? No problem. Just go to your web browser and type “internet” into the search tool. You’ll be amazed at what you can learn.

[T15] “Performance Charts Hard Drives”  
Tom’s Hardware, January 2015  
Available: http://www.tomshardware.com/charts/enterprise-hdd-charts/  
Yet another site with performance data, this time focusing on hard drives.

[V12] “Understanding TLC Flash”  
Kristian Vatto  
AnandTech, September, 2012  
Available: http://www.anandtech.com/show/5067/understanding-tlc-nand  
A short description about TLC flash and its characteristics.

[W15] “List of Ships Sunk by Icebergs”  
Available: http://en.wikipedia.org/wiki/List of ships sunk by icebergs  
Yes, there is a wikipedia page about ships sunk by icebergs. It is a really boring page and basically everyone knows the only ship the iceberg-sinking-mafia cares about is the Titanic.

[Z+12] “De-indirection for Flash-based SSDs with Nameless Writes”  
Yiying Zhang, Leo Prasath Arulraj, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau  
FAST ’13, San Jose, California, February 2013  
Our research on a new idea to reduce mapping table space; the key is to re-use the pointers in the file system above to store locations of blocks, instead of adding another level of indirection.

\newpage

# 45 Data Integrity and Protection
これまでに研究してきたファイルシステムの基本的な進歩を超えて、多くの機能を検討する価値があります。この章では、信頼性にまた重点を置いて説明します(RAIDの章で以前にストレージシステムの信頼性を検討したことがあります)。具体的には、ファイルシステムやストレージシステムは、最新のストレージデバイスの信頼性の低い性質を考慮して、データが安全であることをどのように保証する必要がありますか？この一般的な領域は、data integrity(データ完全性)またはdata protection(データ保護)と呼ばれます。そこで、ストレージシステムがデータをあなたに返すときに、ストレージシステムに入力したデータと同じであることを保証するために使用される手法を検討します。

>> CRUX: HOW TO ENSURE DATA INTEGRITY  
>> ストレージに書き込まれたデータがシステムによってどのように確実に保護されるべきですか？どんな技術が必要ですか？そのようなテクニックを効率的にするには、省スペースと時間の両方のオーバーヘッドはどうですか？

## 45.1 Disk Failure Modes
RAIDについての章で学んだように、ディスクは完璧ではなく、失敗することもあります。初期のRAIDシステムでは、障害のモデルは非常に単純でした。ディスク全体が動作しているかまたは完全に故障しているか、これであれば、そのような障害の検出は簡単です。このようなディスク障害のフェールストップモデルは、RAIDの構築を比較的簡単にします[S90]。

あなたが学んでいないのは、現代のディスクが示す他のタイプの故障モードのすべてです。具体的には、Bairavasundaram et alです。現代のディスクは時には大部分は動作しているように見えますが、1つまたは複数のブロックに正常にアクセスするのに問題があります[B + 07、B + 08]。具体的には、 Latent Sector Errors(LSEs)とblock corruptionの2つのタイプのシングルブロック障害が共通して考慮する価値があります。ここで、それぞれをより詳しく説明します。

LSEsは、ディスクセクタ(またはセクタグループ)が何らかの形で損傷を受けたときに発生します。たとえば、何らかの理由でディスクヘッドが表面に接触した場合(ヘッドクラッシュ、通常の操作では起こらないもの)、表面が損傷してビットが判読不能になる可能性があります。宇宙線もビットを反転させることができ、不正確な内容につながります。幸いにも、ディスク内の Error Correcting Codes(ECC)は、ブロック内のディスク上のビットが正常であるかどうかを判断し、場合によっては、ブロックを修正するためにドライブで使用されます。ドライブがエラーを解決するのに十分な情報を持っていない場合、ディスクは、要求を発行されたときにエラーを返して読み取ります。

また、ディスク自体が検出できないような、ディスクブロックが破損することもあります。たとえば、バグのあるディスクファームウェアは、間違った場所にブロックを書き込む可能性があります。そのような場合、ディスクECCはブロックの内容が正常であることを示しますが、クライアントの観点からは、後でアクセスしたときに間違ったブロックが返されます。同様に、ブロックが障害のあるバスを経由してホストからディスクに転送されると、クライアントが望んでいない破損したデータがディスクに格納されます。これらのタイプの障害は、サイレントフォールトであるため特に危険です。故障したデータを戻すときに問題の兆候は見られません。

Prabhakaran et alは、ディスク障害のこのより現代的な見解を、部分ディスク故障モデル[P + 05]として説明しています。このビューでは、従来のフェイルストップモデルの場合のように、ディスク全体が引き続き故障する可能性があります。つまり、ディスクは見かけ上動作していますが、1つまたは複数のブロックがアクセス不能(すなわちLSE)になったり、間違った内容(すなわち、破損)を保持する可能性があります。したがって、一見して動作するディスクにアクセスすると、与えられたブロックを読み書きしようとするとき(サイレントではない部分的なフォールト)、エラーを返したり、間違ったデータを返すことがあります(部分的なサイレントフォールト)。これらの両方のタイプの障害は、まれですが、どれだけ稀でしょうか？図45.1は、2つのバイラバンダダラム研究[B + 07、B + 08]の結果の一部をまとめたものです。

![](../45/img/fig45_1.PNG)

図は、調査の過程で少なくとも1つのLSEまたはブロック破損を示したドライブの割合を示しています(約3年間、150万台以上のディスクドライブ)。この図はさらに、結果を「安価な」ドライブ(通常はSATAドライブ)と「高価な」ドライブ(通常はSCSIまたはFibreChannel)に細分しています。お分かりのように、より良いドライブを購入することで、両方のタイプの問題の頻度が(桁違いに)低下しますが、ストレージシステムでの処理方法を慎重に考えなければならないほど頻繁に発生します。

LSEsに関する追加的な調査結果は次のとおりです。  
- 複数のLSEsを使用する高価なドライブは、安価なドライブと同じくらい追加のエラーを発生させる可能性が高い
- ほとんどのドライブでは、年間エラー率は2年目に増加します
- ディスクサイズに合わせてLSEsの数が増えます
- LSEsを持つほとんどのディスクは50未満です
- LSEsを持つディスクは、追加のLSEsを開発する可能性が高い
- かなりの量の空間的および時間的局所性が存在する
- ディスクスクラブが便利です(ほとんどのLSEsはこのように見つかっています)

破損に関するいくつかの発見は次の通りです。  
- 破損の可能性は、同じドライブクラス内のさまざまなドライブモデルによって大きく異なります
- 年季による影響はモデルによって異なります
- 仕事量とディスクサイズが破損にほとんど影響しない
- 破損しているほとんどのディスクにはいくつかの破損しかありません
- 破損は、ディスク内またはRAID内のディスク間で独立していない
- 空間的局所性と時間的局所性が存在する
- LSEsとの相関は低い

これらの失敗の詳細については、元の論文[B + 07、B + 08]を読むべきでしょう。しかし、うまくいけば、要点は明らかです。信頼できるストレージシステムを本当に構築したいのであれば、LSEsと破損のブロックのそれぞれの検出と復旧のための機械を組み込む必要があります。

## 45.2 Handling Latent Sector Errors
部分的なディスク障害のこれらの2つの新しいモードを考えると、我々は今、それらについて何ができるのかを見極めるべきです。最初に2つのうちのlatent sector errorsと名前がついている簡単な方に取り組んでみましょう。

>> CRUX: HOW TO HANDLE LATENT SECTOR ERRORS  
>> ストレージシステムは latent sector errorsをどのように処理すべきですか？このような部分的な失敗を処理するには、どれくらい余分な機械が必要ですか？

結果として、latent sector errorsは(定義によって)容易に検出されるので、扱うのが簡単です。ストレージシステムがブロックにアクセスしようとしたときに、ディスクがエラーを返した場合、ストレージシステムは正しいデータを返すために必要な冗長メカニズムを使用するだけです。たとえば、ミラーリングされたRAIDでは、システムは代替コピーにアクセスする必要があります。パリティベースのRAID-4またはRAID-5システムでは、パリティグループの他のブロックからブロックを再構築する必要があります。したがって、LSEsなどの容易に検出される問題は、標準的な冗長性メカニズムを通じて容易に回復されます。

LSEsの普及率は、長年にわたりRAID設計に影響を与えてきました。フルディスク障害とLSEsの両方が同時に発生すると、特に興味深い問題がRAID-4/5システムで発生します。具体的には、ディスク全体が故障すると、RAIDはパリティグループ内の他のすべてのディスクを読み取り、欠損値を再計算することによってディスクを(たとえば、ホットスペアに)再構築しようと試みます。再構築中に他のディスクのいずれかでLSEが発生した場合、問題が発生します。再構成が正常に完了できません。

この問題を解決するために、いくつかのシステムでは余分な冗長性が追加されています。たとえば、NetAppのRAID-DPは、一つの[C + 04]ではなく2つのパリティディスクに相当します。再構成中にLSEが検出されると、余分なパリティが欠損ブロックを再構築するのに役立ちます。いつものように、各ストライプの2つのパリティブロックを維持する方がコストがかかります。しかし、NetApp WAFLファイルシステムのログ構造は、多くの場合、コストを軽減します[HLM94]。残りのコストは、第2のパリティブロック用の余分なディスクのスペースです。

## 45.3 Detecting Corruption: The Checksum
では、より挑戦的な問題であるデータ破損によるsilent failuresの問題に取り組んでみましょう。破損が発生した場合にユーザーが悪意のあるデータを取得するのを防ぐことができ、悪意あるデータを返すディスクに導けるでしょうか？

>> CRUX: HOW TO PRESERVE DATA INTEGRITY DESPITE CORRUPTION  
>> このようなサイレントな障害の性質を考えると、ストレージシステムは破損が発生したときに何を検出することができますか？どんな技術が必要ですか？どのように効率的に実装できますか？

latent sector errorsとは異なり、破損の検出は重要な問題です。ブロックが不良であるとクライアントがどのように伝えることができますか？特定のブロックが不良であることが分かったら、復旧は前と同じです。ブロックのコピーをいくつか用意する必要があります(うまくいけば、壊れていないこともあります)。したがって、我々はここで検出技術に焦点を当てます。

最新のストレージシステムでデータの整合性を保持するために使用される主なメカニズムはチェックサムと呼ばれます。チェックサムは、単にデータの塊(例えば4KBのブロック)を入力とし、そのデータ上の関数を計算してデータの内容(例えば4または8バイト)の小さな要約を生成する関数の結果です。この概要はチェックサムと呼ばれます。このような計算の目的は、データにチェックサムを格納し、その後にデータの現在のチェックサムが元のストレージ値と一致することを確認することによって、データが何らかの形で壊れているか、変更されているかどうかを検出できるようにすることです。

>> TIP: THERE’S NO FREE LUNCH  
>> There’s No Such Thing As A Free Lunch、つまりTNSTAAFLは古いアメリカのイディオムであり、あなたが何かを無料で手に入れているときには、実際には多少の費用がかかります。ダイナーが顧客に無料のランチを宣伝し、飲み物を引き出そうと考えていた昔からのものです。あなたが入ったときにだけ、あなたは"無料"ランチを取得するために、あなたは1つ以上のアルコール飲料を購入しなければならないことを認識しましたか？もちろん、特にあなたがアルコール依存症(または典型的な学部生)である場合、これは実際問題ではないかもしれません。

### Common Checksum Functions
多数の異なる関数がチェックサムを計算するために使用され、強さ(すなわち、それらがデータ保全性をいかに良好に保っているか)および速度(すなわち、それらをいかに迅速に計算することができるか)で変化します。ここではシステムに共通するトレードオフが発生します。通常、より多くの保護を受けるほど、コストが高くなります。free lunchなどはありません。

あるものは排他的論理和(XOR)に基づく単純なチェックサム関数です。XORベースのチェックサムの場合、チェックサムは、チェックサムされるデータブロックの各チャンクを排他的論理和(XOR)することによって計算され、ブロック全体のXORを表す単一の値を生成します。

これをより具体的にするために、16バイトのブロックに対して4バイトのチェックサムを計算していると想像してください(このブロックは実際にディスクセクタまたはブロックになるには小さすぎますが、例のために役立ちます)。16データバイトの16進数で、次のようになります。  
![](../45/img/fig45_1_1.PNG)  
バイナリ形式で表示すると、次のようになります。  
![](../45/img/fig45_1_2.PNG)  
行ごとに4バイトのグループにデータを並べたので、結果のチェックサムがどのようになるかを簡単に確認できます。各列でXORを実行して最終的なチェックサム値を取得します。  
![](../45/img/fig45_1_3.PNG)  
結果は16進数で0x201b9403です。XORは合理的なチェックサムですが、限界があります。たとえば、チェックサム単位内の同じ位置の2ビットが変化した場合、チェックサムは破損を検出しません。このため、人々は他のチェックサム機能を調査しました。

もう1つの基本的なチェックサム機能は追加です。このアプローチには、高速であるという利点があります。オーバーフローを無視して、データの各チャンクに対して2の補数加算を実行するだけです。それはデータの多くの変化を検出することができるが、例えばデータがシフトされた場合には良くありません。

ちょっと複雑なアルゴリズムがFletcherチェックサムとして知られていますが、これは発明者John F. Fletcher氏[F82]の名前です。計算が非常に簡単で、2つのチェックバイトs1とs2の計算が必要です。具体的には、ブロックDがバイトd_1 ... d_nで構成され、s1は以下のように定義されます：s1 =(s1 + d_i)mod255(全てのd_iに対して計算される)。s2は、s2 =(s2 + s1)mod255(すべてのd_iに対して計算される)[F04]です。Fletcherのチェックサムは、すべてのシングルビット、ダブルビットエラー、および多くのバーストエラー[F04]を検出して、CRCとほぼ同じくらい強いです(下記参照)。

1つの最終的に使用されるチェックサムは、Cyclic Redundancy Check(CRC)として知られています。データブロックDを介してチェックサムを計算したいと仮定します。Dを大規模な2進数であるかのように扱います(結局のところビット列です)、合意された値(k)で割ります。この除算の残りはCRCの値です。明らかになったように、このバイナリモジュロ演算をむしろ効率的に実装することができ、したがってネットワーキングにおけるCRCの人気は高いです。詳細は[M13]を参照してください。

どのような方法を使用しても、完全なチェックサムがないことは明らかです。内容が同一ではない2つのデータブロックに同じチェックサムがある可能性があります。結局のところ、チェックサムを計算することは、何かを大きく(例えば4KB)取って、はるかに小さい(例えば4または8バイト)要約を生成することです。つまり、直感的でなければいけません。良いチェックサム関数を選ぶ際に、我々は衝突の可能性を最小限に抑えながら計算しやすいものを見つけることを試みています。

### Checksum Layout
チェックサムを計算する方法について少し理解したので、次にストレージシステムでチェックサムを使用する方法を分析しましょう。最初に問題となるのは、チェックサムのレイアウト、つまりどのようにチェックサムをディスクに保存すればよいでしょうか？

最も基本的なアプローチでは、各ディスクセクタ(またはブロック)にチェックサムを格納するだけです。データブロックDが与えられたら、そのデータC(D)でチェックサムを呼び出します。したがって、チェックサムがないと、ディスクレイアウトは次のようになります。  
![](../45/img/fig45_1_4.PNG)  
チェックサムを使用すると、レイアウトはすべてのブロックに対して1つのチェックサムを追加します。  
![](../45/img/fig45_1_5.PNG)  
チェックサムは通常小さい(例えば8バイト)ディスクであり、ディスクはセクタサイズのチャンク(512バイト)またはその倍数でしか書き込めないので、上記のレイアウトをどのように達成するかという問題があります。ドライブメーカーが採用している解決策の1つは、ドライブを520バイトのセクタでフォーマットすることです。1セクタあたり余分な8バイトを使用してチェックサムを格納することができます。このような機能を持たないディスクでは、ファイルシステムは512バイトのブロックにパックされたチェックサムを格納する方法を理解しなければなりません。そのような可能性の1つは次のとおりです。  
![](../45/img/fig45_1_6.PNG)  
このスキームでは、n個のチェックサムがセクタ内に一緒に格納され、その後にn個のデータブロックが続き、次のn個のブロック用のチェックサムセクタが…と繰り返します。このスキームは、すべてのディスクで作業する利点がありますが、効率が低下する可能性があります。たとえば、ファイルシステムがブロックD1を上書きしたい場合、C(D1)を含むチェックサムセクタを読み込み、その中のC(D1)を更新してから、チェックサムセクタおよび新しいデータブロックを書き出す必要がありますD1(したがって、1回の読み出しと2回の書き込み)。 以前のアプローチ(1セクタあたり1つのチェックサムの)は単なる書き込みを実行するだけです。

## 45.4 Using Checksums
チェックサムレイアウトが決定したら、チェックサムの使用方法を実際に理解することができます。ブロックDを読むとき、クライアント(すなわち、ファイルシステムまたはストレージコントローラ)は、ディスクC_s(D)からそのチェックサムを読み取ります。これをstored checksum(したがって添え字C_s)と呼びます。次に、クライアントは、検索されたブロックDに対するチェックサムを計算します。これは、computed checksum(C_c(D))と呼ばれます。この時点で、クライアントはstored checksumとcomputed checksumを比較します。もしそれらが等しければ(すなわち、C_s(D)== C_c(D))、データは破損していない可能性があり、したがって安全にユーザに戻すことができます。もしそれらが等しくないとき(すなわち、C_s(D)!= C_c(D))、これは、格納されたチェックサムがその時点でのデータの値を反映しているため、データが格納されてから変更されたことを意味します。この場合、私たちはチェックサムが検出するのに役立つ破損があります。

破損を考えると、自然な疑問は私たちがそれについて何をすべきかということです。ストレージシステムに冗長コピーがある場合、その答えは簡単です。代わりに使用してみてください。ストレージシステムにそのようなコピーがない場合は、エラーを返す可能性があります。どちらの場合でも、破損の検出は魔法の弾丸ではないことを認識してください。他に破損していないデータを取得する方法がない場合、あなたは単に運がないだけです。

## 45.5 A New Problem: Misdirected Writes
上記の基本的なスキームは、破損したブロックの一般的なケースでうまくいきます。しかし、現代のディスクには、さまざまなソリューションを必要とする異常ないくつかのfailureモードがあります。関心のある第1のfailureモードは、misdirected write(誤った方向の書き込み)と呼ばれます。これは、間違った場所以外のデータをディスクに正しく書き込むディスクコントローラとRAIDコントローラで発生します。単一ディスクシステムでは、これは、ディスクがブロックD_xを(おそらくは)xに対応するのではなく、yをアドレス指定する(したがって、"破損する"D_y)ように書いたことを意味します。さらに、マルチ・ディスク・システムでは、コントローラは、ディスクiのxにアドレスするのではなく、むしろ他のディスクjに書き込むかもしれません。

>> CRUX: HOW TO HANDLE MISDIRECTED WRITES  
>> ストレージシステムまたはディスクコントローラは、misdirected writesをどのように検出すべきですか？チェックサムにはどのような追加機能が必要ですか？

驚くことではないですが、答えは簡単です。各チェックサムに少しだけ情報を追加してください。この場合、物理識別子(物理ID)を追加することは非常に役に立ちます。例えば、格納された情報がブロックのディスク及びセクタ番号と共にチェックサムC(D)を含むならば、クライアントが正しい情報がブロック内に存在するかどうかを容易に判断することができます。具体的には、クライアントがディスク10のブロック4(D_10,4)を読み取っている場合、格納された情報には、そのディスク番号とセクタオフセットが含まれている必要があります(下図参照)。情報が一致しない場合、間違った書込みが行われ、破損が検出されます。2ディスクシステムでこのような追加情報がどのように表示されるかの例を次に示します。この図では、チェックサムは通常は小さい(例えば8バイト)一方、ブロックははるかに大きい(例えば、4KB以上)ので、前の他のものと同様に、この数字は縮尺通りではないことに注意してください。  
![](../45/img/fig45_1_7.PNG)  
ディスク上には冗長性がかなりあることがディスク上のフォーマットからわかります。各ブロックごとにディスク番号が各ブロック内で繰り返され、ブロックのオフセットもブロック自体の横に保持されます。冗長な情報の存在は驚くべきことではありません。冗長性は、エラー検出(この場合)および回復(他の場合)の鍵です。わずかな余分な情報は、完璧なディスクで厳密には必要とされませんが、問題が発生した場合に問題のある状況を検出するのに役立ちます。

## 45.6 One Last Problem: Lost Writes
残念ながら、間違った書込みは私たちが直面する最後の問題ではありません。具体的には、現代の記憶装置の中には、書込みが完了したことを装置が上位層に通知したにもかかわらず、失われた書込みとして知られる問題もあります。したがって、残っているのは、更新された新しいコンテンツではなく、ブロックの古いコンテンツです。

明らかな問題は、上記のチェックサム戦略(基本的なチェックサムや物理的な識別情報など)が失われた書き込みを検出するのに役立つかどうかです。残念なことに、答えはノーです。古いブロックは一致するチェックサムを持つ可能性が高く、上記の物理ID(ディスク番号とブロックオフセット)も正しいでしょう。

>> CRUX: HOW TO HANDLE LOST WRITES  
>> ストレージシステムまたはディスクコントローラは、失われた書き込みをどのように検出する必要がありますか？ チェックサムにはどのような追加機能が必要ですか？

[K + 08]に役立つさまざまな解決策があります。1つの古典的なアプローチ[BS04]は、書き込み検証またはread after writeを実行することです。書き込み後にデータを直ちに読み戻すことによって、システムはデータが実際にディスク表面に到達することを保証することができます。しかし、このアプローチは非常に遅く、書き込みを完了するために必要なI/Oの数が倍になります。

一部のシステムでは、システム内の他の場所でチェックサムを追加して、失われた書き込みを検出します。たとえば、SunのZFS(Zettabyte File System)には、各ファイルシステムのiノードとファイルに含まれるすべてのブロックの間接ブロックにチェックサムが含まれています。したがって、データブロック自体への書き込みが失われても、inode内のチェックサムは古いデータと一致しません。inodeとデータの両方への書き込みが同時に失われた場合にのみ、そのようなスキームは失敗しますが、(残念ながら可能です！)そのような状況はありません。

## 45.7 Scrubbing
この議論のすべてを考えると、あなたは疑問に思うかもしれません。いつこれらのチェックサムが実際にチェックされますか？もちろん、データがアプリケーションによってアクセスされるときにいくらかのチェックが行われますが、ほとんどのデータはアクセスすることはほとんどないため、チェックされません。チェックされていないデータは、信頼性の高いストレージシステムでは問題になります。ビット腐敗が特定のデータのすべてのコピーに最終的に影響する可能性があるからです。

この問題を解決するために、多くのシステムではさまざまな形式のディスクスクラビングが使用されています[K + 08]。システムのすべてのブロックを定期的に読み取り、チェックサムが有効かどうかをチェックすることで、ディスクシステムは、特定のデータ項目のすべてのコピーが破損する可能性を減らすことができます。典型的なシステムは、夜間または週単位でスキャンをスケジュールします。

## 45.8 Overheads Of Checksumming
終了する前に、データ保護のためにチェックサムを使用するオーバーヘッドのいくつかについて説明します。コンピュータシステムで一般的であるように、スペースと時間という2つの異なる種類のオーバーヘッドがあります。

スペースのオーバーヘッドには2つの形式があります。最初はディスク(または他の記憶媒体)自体にあります。格納された各チェックサムはディスク上の空き領域を占有します。これはもはやユーザーデータに使用できません。典型的な比率は、ディスク上のスペースのオーバーヘッドが0.19％であるため、4 KBのデータブロックにつき8バイトのチェックサムになります。

第2の種類のスペースオーバヘッドは、システムのメモリ内にある。データにアクセスするときに、チェックサムとデータそのもののためにメモリに余裕がなければなりません。しかし、システムが単にチェックサムをチェックし、それが一旦終了するこのオーバーヘッドは短命であり、懸念するものではありません。チェックサムがメモリに保持されている場合(メモリ破損[Z + 13]に対する保護レベルが追加されている場合)に限り、この小さなオーバーヘッドは観測可能です。

スペースのオーバーヘッドは小さいものの、チェックサムによる時間オーバーヘッドはかなり目立つことがあります。最低限、CPUは、データが格納されているとき(格納されているチェックサムの値を決定するとき)とアクセスされたとき(チェックサムを再度計算してそれを格納されたチェックサムと比較するとき)の各ブロックのチェックサムを計算する必要があります。チェックサム(ネットワークスタックを含む)を使用する多くのシステムで採用されているCPUオーバーヘッドを削減する方法の1つは、データコピーとチェックサムを1つの効率的なアクティビティに組み合わせることです。何らかの形で(例えば、カーネルページキャッシュからユーザバッファにデータをコピーするために)コピーが必要とされるので、コピー/チェックサムを組み合わせることは非常に効果的です。

CPUオーバーヘッド以外にも、いくつかのチェックサム方式により、余分なI/Oオーバーヘッドが発生する可能性があります。具体的には、チェックサムがデータとは区別されて格納されている(特にアクセスするための余計なI/Oが必要な場合)、background scrubbingに必要な余分なI/Oです。前者は設計によって減らすことができます。おそらくこのようなscrubbing活動がいつ行われるかを制御することによって、後者は調整され、その影響は制限されます。真夏の夜中に、生産労働者のほとんどが就寝してしまったので、このようなscrubbing活動を実行してストレージシステムの堅牢性を高める良い機会になるかもしれません。

## 45.9 Summary
現代のストレージシステムでは、チェックサムの実装と使用に焦点を当てて、データ保護について説明しました。異なるチェックサムが異なるタイプのフォルトに対して保護します。記憶装置が進化するにつれて、新しいfailure modesが間違いなく発生するでしょう。おそらくこのような変化は、研究界や産業界にこれらの基本的アプローチのいくつかを再び使ったり、まったく新しいアプローチを発明したりすることになるでしょう。それは時間が教えてくれるかもしれませんし、そうではないかもしれません。そういった意味で時間というのは面白いです。

## 参考文献

[B+07] “An Analysis of Latent Sector Errors in Disk Drives”  
Lakshmi N. Bairavasundaram, Garth R. Goodson, Shankar Pasupathy, Jiri Schindler  
SIGMETRICS ’07, San Diego, California, June 2007  
The first paper to study latent sector errors in detail. As described in the next citation [B+08], a collaboration between Wisconsin and NetApp. The paper also won the Kenneth C. Sevcik Outstanding Student Paper award; Sevcik was a terrific researcher and wonderful guy who passed away too soon. To show the authors it was possible to move from the U.S. to Canada and love it, he once sang us the Canadian national anthem, standing up in the middle of a restaurant to do so.

[B+08] “An Analysis of Data Corruption in the Storage Stack”  
Lakshmi N. Bairavasundaram, Garth R. Goodson, Bianca Schroeder,  
Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau  
FAST ’08, San Jose, CA, February 2008  
The first paper to truly study disk corruption in great detail, focusing on how often such corruption occurs over three years for over 1.5 million drives. Lakshmi did this work while a graduate student at Wisconsin under our supervision, but also in collaboration with his colleagues at NetApp where he was an intern for multiple summers. A great example of how working with industry can make for much more interesting and relevant research.

[BS04] “Commercial Fault Tolerance: A Tale of Two Systems”  
Wendy Bartlett, Lisa Spainhower  
IEEE Transactions on Dependable and Secure Computing, Vol. 1, No. 1, January 2004  
This classic in building fault tolerant systems is an excellent overview of the state of the art from both IBM and Tandem. Another must read for those interested in the area.

[C+04] “Row-Diagonal Parity for Double Disk Failure Correction”  
P. Corbett, B. English, A. Goel, T. Grcanac, S. Kleiman, J. Leong, S. Sankar  
FAST ’04, San Jose, CA, February 2004  
An early paper on how extra redundancy helps to solve the combined full-disk-failure/partial-disk-failure problem. Also a nice example of how to mix more theoretical work with practical.

[F04] “Checksums and Error Control”  
Peter M. Fenwick  
Copy Available: http://www.ostep.org/Citations/checksums-03.pdf  
A great simple tutorial on checksums, available to you for the amazing cost of free.

[F82] “An Arithmetic Checksum for Serial Transmissions”  
John G. Fletcher  
IEEE Transactions on Communication, Vol. 30, No. 1, January 1982  
Fletcher’s original work on his eponymous checksum. Of course, he didn’t call it the Fletcher checksum, rather he just didn’t call it anything, and thus it became natural to name it after the inventor. So don’t blame old Fletch for this seeming act of braggadocio. This anecdote might remind you of Rubik and his cube; Rubik never called it “Rubik’s cube”; rather, he just called it “my cube.”

[HLM94] “File System Design for an NFS File Server Appliance”  
Dave Hitz, James Lau, Michael Malcolm  
USENIX Spring ’94  
The pioneering paper that describes the ideas and product at the heart of NetApp’s core. Based on this system, NetApp has grown into a multi-billion dollar storage company. If you’re interested in learning more about its founding, read Hitz’s autobiography “How to Castrate a Bull: Unexpected Lessons on Risk, Growth, and Success in Business” (which is the actual title, no joking). And you thought you could avoid bull castration by going into Computer Science.

[K+08] “Parity Lost and Parity Regained”  
Andrew Krioukov, Lakshmi N. Bairavasundaram, Garth R. Goodson, Kiran Srinivasan, Randy Thelen, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau  
FAST ’08, San Jose, CA, February 2008  
This work of ours, joint with colleagues at NetApp, explores how different checksum schemes work (or don’t work) in protecting data. We reveal a number of interesting flaws in current protection strategies, some of which have led to fixes in commercial products.

[M13] “Cyclic Redundancy Checks”  
Author Unknown  
Available: http://www.mathpages.com/home/kmath458.htm  
Not sure who wrote this, but a super clear and concise description of CRCs is available here. The internet is full of information, as it turns out.

[P+05] “IRON File Systems”  
Vijayan Prabhakaran, Lakshmi N. Bairavasundaram, Nitin Agrawal, Haryadi S. Gunawi, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau  
SOSP ’05, Brighton, England, October 2005  
Our paper on how disks have partial failure modes, which includes a detailed study of how file systems such as Linux ext3 and Windows NTFS react to such failures. As it turns out, rather poorly! We found numerous bugs, design flaws, and other oddities in this work. Some of this has fed back into the Linux community, thus helping to yield a new more robust group of file systems to store your data.

[RO91] “Design and Implementation of the Log-structured File System”  
Mendel Rosenblum and John Ousterhout  
SOSP ’91, Pacific Grove, CA, October 1991  
Another reference to this ground-breaking paper on how to improve write performance in file systems.

[S90] “Implementing Fault-Tolerant Services Using The State Machine Approach: A Tutorial”  
Fred B. Schneider  
ACM Surveys, Vol. 22, No. 4, December 1990  
This classic paper talks generally about how to build fault tolerant services, and includes many basic definitions of terms. A must read for those building distributed systems.

[Z+13] “Zettabyte Reliability with Flexible End-to-end Data Integrity”  
Yupu Zhang, Daniel S. Myers, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau  
MSST ’13, Long Beach, California, May 2013  
Our own work on adding data protection to the page cache of a system, which protects against memory corruption as well as on-disk corruption.

\newpage

# 48 Distributed Systems
分散システムは世界の顔を変えました。あなたのWebブラウザが地球上の他のどこかのWebサーバに接続するとき、それは単純な形式のクライアント/サーバ分散システムのように見えます。しかし、GoogleやFacebookなどの最新のWebサービスにコンタクトすると、ただ1台のマシンと対話するだけではありません。これらの複雑なサービスの舞台裏では、大規模な収集(すなわち、数千件)のマシンから構築され、それぞれが協力してサイトの特定のサービスを提供します。したがって、分散システムを研究することが興味深いのは明らかです。実際、それはクラス全体にふさわしいものです。ここでは、主要なトピックのいくつかを紹介します。

分散システムを構築する際には、数多くの新たな課題が生じます。我々が重点的に取り組むのは失敗です。「完璧な」コンポーネントやシステムを構築する方法を知らない(そして、決してそうなることはありません)ので、機械、ディスク、ネットワーク、およびソフトウェアはすべて時々失敗します。しかし、現代のWebサービスを構築するときには、失敗しないかのようにクライアントに表示したいと考えています。どうすればこの作業を達成できますか？

>> THE CRUX:HOW TO BUILD SYSTEMS THAT WORK WHEN COMPONENTS FAIL  
>> 常に正しく動作しない部品から作業システムを構築するにはどうすればよいですか？基本的な質問は、RAIDストレージアレイで議論したトピックのいくつかを思い出させるはずです。しかし、ここでの問題は、ソリューションと同様に、より複雑になる傾向があります。

面白いことに、障害は分散システムを構築する上での中心的課題ですが、チャンスでもあります。マシンは失敗します。マシンが故障したという事実だけでは、システム全体が故障しなければならないということを意味するものではありません。一連のマシンをまとめて、コンポーネントが定期的に故障しているにもかかわらず、ほとんど失敗しないシステムを構築することができます。この現実は、分散システムの中心的な美しさと価値です。なぜなら、それらがGoogleやFacebookなど、あなたが使っているすべての最新のWebサービスの下にある理由です。

>> TIP: COMMUNICATION IS INHERENTLY UNRELIABLE  
>>事実上すべての状況において、通信を根本的に信頼できない活動と見なすことは良いことです。ビットの破損、ダウンや動いていないリンクやマシン、着信パケットのバッファスペースの不足は、すべて同じ結果につながります。パケットが宛先に到達しないことがあります。そのような信頼できないネットワークの上に信頼できるサービスを構築するためには、パケット損失に対処できる技術を検討する必要があります。

その他の重要な問題も存在します。システムのパフォーマンスはよく重要です。私たちの分散システムを一緒に接続するネットワークで、システム設計者は、与えられたタスクをどのように達成するか、送信されるメッセージの数を減らし、通信を効率的(低い待ち時間、高帯域幅)にできる限り慎重に考えなければならないことがよくあります。

最後に、セキュリティも必要な考慮事項です。遠隔地に接続するとき、遠隔地の当事者が誰であるかを保証することが中心的な問題になります。さらに、第三者が2つの間の進行中の通信を監視または変更できないようにすることもまた課題です。

ここでは、分散システムで最も新しい通信の基本的な側面について説明します。すなわち分散システム内のマシンは、どのように相互に通信する必要がありますか？利用可能な最も基本的なプリミティブ、メッセージから始めそれらの上にいくつかのより高いレベルのプリミティブを構築します。上記したように、障害は中心的な焦点になります。通信レイヤが障害をどのように処理すべきですか？

## 48.1 Communication Basics
現代のネットワークの中心的な教えは、通信は根本的に信頼性がないということです。広域インターネットでも、Infinibandなどのローカルエリア高速ネットワークでも、パケットは定期的に失われたり、破損したり、宛先に到達しない場合があります。

パケットの損失や破損の原因は多数あります。時には伝送中に、いくつかのビットが電気的または他の同様の問題のために反転されます。場合によっては、ネットワークリンクやパケットルーター、リモートホストなど、システム内の要素が何らかの形で損傷を受けたり正しく動作しないことがあります。ネットワークケーブルが誤って切断されることがあります。

しかし、より根本的なのは、ネットワークスイッチ、ルータ、エンドポイント内でのバッファリングの不足によるパケット損失です。具体的には、すべてのリンクが正しく機能し、システム内のすべてのコンポーネント(スイッチ、ルータ、エンドホスト)が期待どおりに稼動していることを保証できる場合でも、次の理由により、まだ失われる可能性があります。パケットがルータに到着したとします。処理されるパケットについては、ルータ内のどこかのメモリに配置する必要があります。このようなパケットが多数到着すると、ルータ内のメモリがすべてのパケットに対応できない可能性があります。その時点でルータが持つ唯一の選択肢は、1つまたは複数のパケットをドロップすることです。この同様の現象はエンドホストでも発生します。単一のマシンに大量のメッセージを送信すると、マシンのリソースが容易に圧倒され、パケット損失が再び発生します。

従って、パケット損失はネットワーキングにおいて基本的なものです。したがって、問題は次のようになります。これをどう処理しますか？

![](../48/img/fig48_1.PNG)

## 48.2 Unreliable Communication Layers
1つの簡単な方法はこれです。それに対処しないことです。一部のアプリケーションでは、パケットロスを処理する方法が分かっているため、よく聞かれるエンドツーエンド(章の最後を見てください)の議論の一例である、基本的な信頼性の低いメッセージングレイヤーと通信できるようにするのが便利な場合があります。このような信頼性の低いレイヤの優れた例は、今日のほぼすべての現代システムで利用可能なUDP/IPネットワーキングスタックにあります。UDPを使用するには、通信エンドポイントを作成するためにソケットAPIを使用します。他のマシン上(または同じマシン上)のプロセスは元のプロセスにUDPデータグラムを送ります(データグラムは最大サイズまでの固定サイズのメッセージです)。

![](../48/img/fig48_2.PNG)

図48.1と図48.2に、UDP/IPの上に構築された単純なクライアントとサーバーを示します。クライアントは、サーバーにメッセージを送信し応答を返します。この少量のコードで分散システムの構築を始めてみてください。

UDPは、信頼性の低い通信レイヤの大きな例です。それを使用すると、パケットが紛失(ドロップ)し、宛先に到達しないという状況に遭遇します。送信者は決して損失の通知されません。しかし、それはUDPが何の失敗に対しても守らないということを意味するものではありません。たとえば、UDPにはいくつかの形式のパケット破損を検出するためのチェックサムが含まれています。

しかし、多くのアプリケーションは単に宛先にデータを送信し、パケットの消失を心配する必要がないため、もっと多くの仕組みが必要です。具体的には、信頼できないネットワーク上に信頼できる通信が必要です。

>> TIP: USE CHECKSUMS FOR INTEGRITY  
>> チェックサムは、最新のシステムで迅速かつ効果的に破損を検出するためによく使用される方法です。単純なチェックサムは加算です。データのまとまりのバイトを合計します。もちろん基本的な巡回冗長コード(CRC)、フレッチャーチェックサム、その他多数の他の多くの洗練されたチェックサムが作成されています[MK09]。
ネットワークでは、チェックサムは次のように使用されます。あるマシンから別のマシンにメッセージを送信する前に、そのメッセージのバイトのチェックサムを計算します。次に、メッセージとチェックサムの両方を宛先に送信します。宛先では、受信側は受信メッセージ上のチェックサムを計算します。この計算されたチェックサムが送信されたチェックサムと一致する場合、受信者は、データが送信中に破損しない可能性があるという確信を感じることができます。
チェックサムは、いくつかの異なる軸に沿って評価することができます。効率性の主な考慮事項の1つは、データの変更がチェックサムの変更につながるかどうかです。チェックサムが強ければ強いほど、データの変化が気付かれなくなります。パフォーマンスはもう一つの重要な基準です。チェックサムの計算にはどのくらいのコストがかかりますか？残念なことに、有効性とパフォーマンスはよく不安定です。つまり、高品質のチェックサムは計算コストがかかることがあります。人生はまた完璧ではありません。

## 48.3 Reliable Communication Layers
信頼性の高い通信レイヤを構築するには、パケットロスを処理するための新しいメカニズムと技術が必要です。クライアントが信頼性の低い接続を介してサーバーにメッセージを送信する簡単な例を考えてみましょう。最初に質問しなければならないのは、送信者は受信者が実際にメッセージを受信したことをどのように知っていますか？ということです。私たちが使用するテクニックは、肯定応答またはackと呼ばれています。アイデアは単純です。送信者は受信者にメッセージを送信します。受信者はその受付の肯定応答のために短いメッセージを送り返します。図48.3にプロセスを示します。  
![](../48/img/fig48_3.PNG)

送信者がメッセージの肯定応答を受信すると、受信者が実際に元のメッセージを受信したことを安心することができます。しかし、肯定応答を受信しなかった場合、送信者は何をすべきですか？

![](../48/img/fig48_4.PNG)

このケースを処理するには、タイムアウトと呼ばれる追加のメカニズムが必要です。送信者がメッセージを送信すると、送信者は一定時間後にタイマーをオフにするように設定するようになります。その時点で確認応答が受信されなかった場合、送信者はメッセージが失われたと判断しまう。次に、送信者は単に送信の再試行を行い今度は同じメッセージを再度送信し、この時間が経過することを期待します。このアプローチが機能するためには、送信者はメッセージをもう一度送信する必要がある場合に備えて、メッセージのコピーを保持する必要があります。タイムアウトと再試行の組み合わせにより、アプローチのタイムアウト/再試行と呼ぶ人もいました。かなり賢いネットワーキングタイプでしょう？違いますか？図48.4に例を示します。

![](../48/img/fig48_5.PNG)

残念ながら、この形式のタイムアウト/再試行では十分ではありません。図48.5にトラブルの原因となるパケットロスの例を示します。この例では、失われた元のメッセージではなく確認応答です。送信者の観点からは状況は同じように見えます。応答が受信されず、タイムアウトと再試行が順番に行われます。しかし受信機の観点からは、それは全く異なっています。今では同じメッセージが2回受信されました！これがOKである場合もありますが、一般的にはそうではありません。ファイルをダウンロードしているときに何が起こり、余分なパケットがダウンロード内で繰り返されるか想像してください。したがって、信頼できるメッセージ層を目指しているときには、通常、各メッセージが受信者によって1回だけ受信されることを保証したいと考えています。

受信者が重複メッセージの送信を検出できるようにするには、送信者は各メッセージを独自の方法で識別しなければならず、受信者は以前に各メッセージを見たかどうかを追跡する必要があります。受信者が重複した送信を見た場合、単にメッセージを確認するだけですが、(重要なことに)データを受信するアプリケーションにメッセージを渡しません。したがって、送信者はackを受信しますが、メッセージは2回受信されず、上記の1回のセマンティクスは維持されます。

重複したメッセージを検出する方法はたくさんあります。たとえば、送信者はメッセージごとに一意のIDを生成できます。受信者はそれまでに見たすべてのIDを追跡することができます。このアプローチはうまくいくかもしれませんが、非常にコストがかかり、無制限のメモリですべてのIDを追跡する必要があります。

メモリをほとんど必要としない簡単なアプローチではこの問題が解決され、メカニズムはシーケンスカウンタとして知られています。シーケンスカウンタでは、送信側と受信側は、各側が維持するカウンタの開始値(例えば、1)で合意します。メッセージが送信されるたびに、カウンタの現在の値がメッセージとともに送信されます。このカウンタ値(N)はメッセージのIDになります。メッセージが送信された後、送信側は値を(N + 1に)インクリメントします。

受信者は、その送信者からの着信メッセージのIDの期待値としてそのカウンタ値を使用します。受信したメッセージ(N)のIDが受信者のカウンタ(N)と一致する場合、メッセージを確認してアプリケーションに渡します。この場合、受信者はこのメッセージが最初に受信されたと判断します。次に受信者はそのカウンタを(N + 1に)インクリメントし、次のメッセージを待ちます。

ackが失われた場合、送信者はメッセージNをタイムアウトして再送します。この時点では、受信者のカウンタはより高い(N + 1)ため、受信者は既にこのメッセージを受信したことを知ります。したがって、メッセージを確認しますが、アプリケーションに渡すことはありません。この簡単な方法で、シーケンスカウンタを使用して重複を避けることができます。

最も一般的に使用される信頼性の高い通信レイヤーは、TCP/IPと呼ばれます。TCPは、ネットワーク[VJ88]の輻輳を処理するための機械、複数の未解決の要求、その他数百の小さな微調整と最適化を含めて、上で説明したよりもはるかに洗練されています。あなたが好奇心が強いならそれについてぜひ読んでください。より良い方法として、ネットワーキングコースを受講することで、その教材をうまく学ぶことができます。

## 48.4 Communication Abstractions
基本的なメッセージング層が与えられたら、この章の次の質問でアプローチします。分散システムを構築するときに、通信の概念を使用する必要がありますか？

>> TIP: BE CAREFUL SETTING THE TIMEOUT VALUE  
>> 議論から推測できるように、タイムアウト値を正しく設定することは、タイムアウトを使用してメッセージ送信を再試行する重要な側面です。タイムアウトが小さすぎると、送信者は不必要にメッセージを再送信し、送信者とネットワークのリソースにCPU時間を浪費します。タイムアウトが大きすぎる場合、送信者は再送信に時間がかかり過ぎるため、送信者のパフォーマンスが低下します。したがって、単一のクライアントとサーバーの観点から見た「正しい」値は、パケットの損失を検出するのに十分な時間だけ待ちますが、それ以上は待機しません。
しかし、分散システムには単一のクライアントとサーバ以上のものがあります。これについては後の章で説明します。多くのクライアントが単一のサーバーに送信するシナリオでは、サーバーでのパケット損失が、サーバーが過負荷になっていることを示す指標になる場合があります。もしそうであった場合、クライアントは異なる適応方法で再試行することがあります。たとえば、最初のタイムアウトの後、クライアントはタイムアウト値をより高い量、おそらく元の値の2倍に増やす可能性があります。初期のアロハネットワークの先駆けで、初期イーサネットで採用されている[A70]、このような指数関数的なバックオフスキームは、過剰な再送信によってリソースが過負荷になる状況を回避します。ロバストシステムは、この性質の過負荷を避けるために努力しています。

システムコミュニティは、長年にわたり多くのアプローチを開発してきました。1つの作業でOSの抽象化が行われ、分散環境で動作するように拡張されました。例えば、Distributed Shared Memory(DSM)システムは、異なるマシン上のプロセスが大きな仮想アドレス空間[LH89]を共有することを可能にします。この抽象化によって、分散計算はマルチスレッドアプリケーションのようなものに変わります。唯一の違いは、これらのスレッドが同じマシン内の異なるプロセッサではなく、異なるマシン上で実行されることです。

ほとんどのDSMシステムの動作する方法は、OSの仮想メモリシステムを使用する方法です。あるマシンでページにアクセスすると、2つのことが起こります。最初の(最良の)ケースでは、ページは既にマシン上にローカルに存在しているため、データが迅速にフェッチされます。2番目のケースでは、ページは現在他のマシンにあります。ページフォルトが発生し、ページフォールトハンドラが他のマシンにメッセージを送信してページをフェッチし、要求プロセスのページテーブルにインストールして実行を続行します。

このアプローチは今日多くの理由で広く使用されていません。DSMの最大の問題は、障害の処理方法です。たとえば、マシンが故障した場合などを想像してみてください。そのマシンのページはどうなりますか？分散計算のデータ構造がアドレス空間全体に広がっている場合はどうでしょうか？この場合、これらのデータ構造の一部は突然使用できなくなります。あなたのアドレス空間の一部が失われたときの失敗を扱うことは困難です。「次の」ポインタがそのアドレス空間の一部分を指し示すlinked listを想像してください。とんでもない！

さらなる問題はパフォーマンスです。通常、コードを書くとき、メモリへのアクセスは安いと仮定します。DSMシステムでは、一部のアクセスは安価ですが、他のものはページフォルトやリモートマシンからの高価なフェッチを引き起こします。したがって、このようなDSMシステムのプログラマは、ほとんど通信が全く起こらず、そのようなアプローチのポイントの多くを解決するような計算を構成することにかなり注意する必要がありました。この分野では多くの研究が行われましたが、実用的な影響はほとんどありませんでした。今日では、誰もDSMを使用して信頼性の高い分散システムを構築していません。

## 48.5 Remote Procedure Call (RPC)
OSの抽象化は分散システムを構築するうえで貧弱な選択であることが判明しましたが、プログラミング言語(PL)抽象化ははるかに理にかなっています。最も支配的な抽象化は、Remote Procedure Callまたは略してRPCのアイデア[BN84]に基づいています。

リモートプロシージャコールパッケージはすべて、単純な目的を持っています。つまり、リモートマシン上でコードを実行するプロセスをローカル関数を呼び出すのと同じくらい簡単にすることです。したがって、クライアントに対してプロシージャー呼び出しが行われ、しばらくしてから結果が戻されます。サーバーは、エクスポートしたいルーチンを単に定義します。残りの魔法はRPCシステムによって処理され、一般的に二つあります。1つ目はRPCシステムには一般にスタブジェネレータ(プロトコルコンパイラと呼ばれることもあります)、二つ目は、実行時ライブラリがあります。ここでこれらの各部分をより詳しく見ていきます。

### Stub Generator
スタブジェネレータの仕事は簡単です。関数の引数と結果をメッセージの中に入れ、パッキングする際の苦痛の一部を自動化して取り除くことです。数多くの利点が生まれます。設計上、手作業でそのようなコードを書く際に起こる単純な間違いを避けることができます。さらに、スタブコンパイラは、おそらくそのようなコードを最適化して、パフォーマンスを向上させることができます。このようなコンパイラへの入力は、単にサーバーがクライアントにエクスポートする呼び出しのセットです。概念的には、これは次のような単純なものです。
```c
interface {
int func1(int arg1);
int func2(int arg1, int arg2);
};
```
スタブジェネレータはこのようなインタフェースをとり、いくつかの異なるコードを生成します。クライアントの場合、クライアントスタブが生成されます。クライアントスタブは、インタフェースで指定された各関数を含みます。このRPCサービスを使用したいクライアントプログラムはこのクライアントスタブにリンクし、RPCを作成するためにそのクライアントスタブを呼び出します。

内部的には、クライアントスタブのこれらの各機能は、リモートプロシージャコールを実行するために必要なすべての作業を行います。クライアントに対して、コードは単に関数呼び出しとして現れます(例えば、クライアントはfunc1(x)を呼び出す)。内部的には、`func1()`のクライアントスタブ内のコードはこれを行います：  
- メッセージバッファを作成します。メッセージバッファは通常、あるサイズのバイトの連続した配列です。

- 必要な情報をメッセージバッファにパックする。この情報には、関数が必要とするすべての引数(例えば、上の例ではfunc1の整数)など、呼び出される関数の識別子が含まれています。この情報をすべて1つの連続したバッファに入れるプロセスは、the marshaling of argumentsまたはthe serialization of the messageと呼ばれることがあります。

- 宛先RPCサーバーにメッセージを送信します。RPCサーバーとの通信、およびRPCサーバーが正しく動作するために必要なすべての詳細は、RPCランタイムライブラリによって処理されます(後述)。

- 返事を待ちます。関数呼び出しは通常同期的であるため、呼び出しはその完了を待ちます。

- リターンコードと他の引数をアンパックします。関数が単一の戻りコードを返すだけの場合、このプロセスは簡単です。しかし、より複雑な関数はより複雑な結果(例えばリスト)を返す可能性があり、したがってスタブはそれらをアンパックする必要があります。この手順は、unmarshalingまたはdeserializationとも呼ばれます。

- 発信者に戻ります。最後に、クライアント・スタブからクライアント・コードに戻ります。

サーバーの場合は、コードも生成されます。サーバーで実行される手順は次のとおりです。
- メッセージを解凍します。unmarshalingまたはdeserializationと呼ばれるこの手順は、着信メッセージから情報を取り出します。 関数の識別子と引数が抽出されます。

- 実際の関数を呼び出します。最後に！リモート関数が実際に実行されるところに達しました。RPCランタイムはIDで指定された関数を呼び出し、目的の引数を渡します。

- 結果をパッケージ化する。戻り引数は、単一の応答バッファにマーシャリングされます。

- 返信を送信します。最後に返信が発信者に送信されます。

スタブコンパイラで考慮すべき他の重要な問題がいくつかあります。最初のものは複雑な引数です。つまり、複雑なデータ構造をどのようにパッケージ化して送信しますか？たとえば、`write()`システムコールを呼び出すときには、整数ファイル記ディスクリプタ、バッファへのポインタ、書き込まれるバイト数(ポインタで始まる)を示すサイズの3つの引数が渡されます。RPCパッケージにポインタが渡された場合、そのポインタをどのように解釈するのかを把握し、正しい動作を実行できる必要があります。通常これはよく知られているタイプ(RPCコンパイラが理解できるサイズのデータをチャンクを渡すために使用されるバッファtなど)や、データ構造に詳細情報を付けることによって、コンパイラがどのバイトをシリアル化する必要があるか知ることが可能です。

別の重要な問題は、並行性に関するサーバーの構成です。シンプルなサーバーは単純なループで要求を待機し、各要求を1つずつ処理します。しかし、あなたが推測したように、これは非常に非効率的である可能性があります。1つのRPCコールを(例えば、I/O上で)ブロックすると、サーバリソースが浪費されます。したがって、ほとんどのサーバーは、何らかの並行方式で構築されます。一般的な構成はスレッドプールです。この構成では、サーバーの起動時に有限のスレッドセットが作成されます。メッセージが到着すると、これらのワーカースレッドの1つにディスパッチされ、RPC呼び出しの処理が行われ、最終的に応答します。この間、メインスレッドは他の要求を受信し続け、おそらくそれらを他のワーカーにディスパッチします。このような構成により、サーバー内での同時実行が可能になり、その使用率が向上します。RPC呼び出しが正しい動作を保証するためにロックおよび他の同期プリミティブを使用する必要があるため、プログラミングがかなり複雑であり、標準コストも同様に発生します。

### Run-Time Library
ランタイムライブラリは、RPCシステムでの重い作業の多くを処理します。ほとんどのパフォーマンスと信頼性の問題がここで処理されます。このようなランタイムレイヤを構築する際の主要な課題のいくつかについて説明します。

我々が克服しなければならない最初の課題の1つは、リモートサービスを見つける方法です。命名のこの問題は分散システムにおける共通の問題であり、ある意味では現在の議論の範囲を超えています。最も簡単なアプローチは、既存のネーミングシステム(例えば、現在のインターネットプロトコルによって提供されるホスト名およびポート番号)上に構築されます。このようなシステムでは、クライアントは、使用しているポート番号だけでなく、目的のRPCサービスを実行しているマシンのホスト名またはIPアドレスを知っていなければなりません(ポート番号は、複数の通信チャネルを一度に許可する)。プロトコルスイートは、システム内の他のマシンからパケットを特定のアドレスにルーティングするメカニズムを提供する必要があります。命名についての良い議論のためには、インターネット上のDNSと名前解決について読むか、SaltzerとKaashoekの本[SK09]の優秀な章を読んだほうが良いでしょう。

クライアントが特定のリモートサービスのためにどのサーバーと通信するべきかを知ったら、次の質問はどのトランスポートレベルのプロトコルがRPCを構築すべきかということです。具体的には、RPCシステムがTCP/IPなどの信頼性の高いプロトコルを使用するか、UDP/IPなどの信頼性の低い通信レイヤー上に構築する必要がありますか？

純粋にその選択は容易です。明らかに、リモートサーバーに確実にリクエストを送付したいと願っており、確実に回答を受け取ることを希望しています。したがって、TCPなどの信頼性の高いトランスポートプロトコルを選択する必要があります。

残念ながら、信頼できる通信層の上にRPCを構築すると、パフォーマンスが大幅に低下する可能性があります。上記の議論から、信頼性の高い通信レイヤーがどのように機能するかを思い出してください。肯定応答とタイムアウト/リトライです。したがって、クライアントがRPC要求をサーバーに送信すると、サーバーは応答を返して、呼び出し元が要求を受信したことを認識します。同様に、サーバーがクライアントに応答を送信すると、クライアントは受信したことをサーバーが認識するように応答します。信頼できる通信レイヤの上に要求/応答プロトコル(RPCなど)を構築することによって、2つの「余分な」メッセージが送信されます。

このため、多くのRPCパッケージは、UDPなどの信頼性の低い通信レイヤーの上に構築されています。これにより、より効率的なRPCレイヤーが可能になりますが、RPCシステムに信頼性を提供する責任が追加されます。RPC層は、前述のように、タイムアウト/リトライと確認応答を使用して、望ましいレベルの責任を達成します。何らかの形式のシーケンス番号付けを使用することによって、通信層は、各RPCが正確に1回(障害がない場合)、または最大で1回(障害が発生した場合)発生することを保証できます。

### Other Issues
RPCランタイムにも同様に処理する必要のある問題がいくつかあります。たとえば、リモートコールの完了に時間がかかる場合はどうなりますか？私たちのタイムアウト機構が与えられると、長時間実行される遠隔呼び出しは、クライアントに失敗として現れる可能性があり、したがって再試行を引き起こし、したがってここでは注意が必要です。1つの解決方法は、応答がすぐに生成されない場合に明示的な確認応答(受信者から送信者へ)を使用することです。これにより、クライアントは要求を受信したサーバーを知ることができます。その後、しばらくしてから、クライアントは定期的にサーバーがリクエストの作業しているかどうかを尋ねることができます。サーバが"yes"と言っている場合、クライアントは待っていなければなりません(結局のところ、手続き呼び出しが実行を完了するのに時間がかかることがあります)。

ランタイムには、単一のパケットに収まるものよりも大きな引数を持つプロシージャ・コールも処理する必要があります。いくつかの下位レベルのネットワークプロトコルは、送信側の断片化(より大きなパケットをより小さいパケットのセットにする)と受信側の再アセンブリ(小さな部分をより大きな論理全体に分解する)を提供します。そうでなければ、RPCランタイムはそのような機能自体を実装しなければならないかもしれません。詳細は、BirrellとNelsonの優れたRPCペーパーを参照してください[BN84]。

多くのシステムが扱う1つの問題は、バイトオーダーの問題です。ご存じのように、値を格納するのにビッグエンディアンのオーダー、またはリトルエンディアンのオーダーを使用されるマシンもあります。ビッグエンディアンは、アラビア数字のように、最上位ビットから最下位ビットまでのバイト(整数)を格納します。リトルエンディアンはその逆です。どちらも数値情報を保存するのにも同様に有効です。ここでの問題は、異なるエンディアンのマシン間でのやりとり方法です。

>> Aside: The End-to-End Argument  
>> エンド・ツー・エンドの引数は、システム内の最高レベル、すなわち通常「終わり」のアプリケーションが最終的に、特定の機能を真に実装できる階層化されたシステム内の唯一の場所である場合をもたらします。彼らの画期的な論文[SRC84]では、優れた例として、2つのマシン間での信頼性の高いファイル転送が挙げられます。もし、マシンAからマシンBにファイルを転送したいとき、B上で終了するバイトがAで開始したバイトとまったく同じであることを確認するには、この「エンドツーエンド」チェックが必要です。ネットワークまたはディスクなどの低レベルの信頼できる機械は、そのような保証を提供しません。  
コントラストは、信頼性の高いファイル転送の問題を、システムの下位レイヤに信頼性を追加することによって解決しようとするアプローチです。たとえば、信頼性の高い通信プロトコルを構築し、それを使用して信頼性の高いファイル転送を構築するとします。通信プロトコルは、タイムアウト/リトライ、肯定応答、およびシーケンス番号を使用して、送信者によって送信された各バイトが受信者によって順番に受信されることを保証します。残念なことに、このようなプロトコルを使用しても信頼性の高いファイル転送はできません。通信が行われる前に送信者のメモリでバイトが壊れているとか、受信者がデータをディスクに書き込んだときに何か悪いことが起きたとします。そのような場合、たとえバイトがネットワーク上で確実に配信されたとしても、私たちのファイル転送は最終的には信頼できません。信頼性の高いファイル転送を構築するには、エンドツーエンドチェックの信頼性が必要です。具体的には、転送の完了後、受信者ディスクのファイルを読み戻し、チェックサムを計算し、そのチェックサムと送信者のファイルと比べる必要があります。  
この必然的なことは、下位層が余分な機能を提供することによって、実際にシステムの性能を向上させることができ、あるいはシステムを最適化できることである。したがって、システム内の低レベルでこのような機械を使用することを排除すべきではありません。むしろ、全体的なシステムやアプリケーションで最終的に使用される場合、そのような機械の有用性を慎重に検討する必要があります。

RPCパッケージは、メッセージフォーマット内で明確なエンディアンを提供することによって、これを処理します。SunのRPCパッケージでは、XDR(eXternal Data Representation)レイヤーがこの機能を提供します。メッセージを送信または受信するマシンがXDRのエンディアンと一致する場合、メッセージは単に期待どおりに送受信されます。しかし、マシンの通信がエンディアンが異なる場合、メッセージ内の各情報を変換する必要があります。したがって、エンディアンの差はパフォーマンスコストを小さくすることができます。

最後の問題は、通信の非同期性をクライアントに公開するかどうかであるため、パフォーマンスの最適化が可能になります。具体的には、典型的なRPCは同期的に行われます。すなわち、クライアントがプロシージャコールを発行するとき、プロシージャコールが戻るのを待ってから続行する必要があります。この待ち時間は長くなる可能性があり、クライアントが他の作業を行う可能性があるため、RPCパッケージによってはRPCを非同期に呼び出すことができます。非同期RPCが発行されると、RPCパッケージは要求を送信してすぐに戻ります。クライアントは他のRPCやその他の有用な計算を呼び出すなど、自由に他の作業を行うことができます。クライアントはある時点で、非同期RPCの結果を見たいと思うでしょう。RPC層にコールバックして、未処理のRPCが完了するのを待つように指示します。この時点で戻り引数にアクセスできます。

## 48.6 Summary
私たちは、新しいトピック、分散システム、そしてその主要な問題の導入を見てきました。これは、現在は一般的なイベントである障害をどのように処理するかです。彼らがGoogleの中で言うように、デスクトップマシンだけでは障害はまれです。何千ものマシンを持つデータセンターにいると、常に障害が発生しています。どの分散システムの鍵も、その失敗をどのように処理するかです。

通信は、分散システムの核心を形成することもわかりました。その通信の一般的な抽象化は、クライアントがサーバー上でリモート呼び出しを行うことを可能にするリモートプロシージャコール(RPC)にあります。RPCパッケージは、ローカルプロシージャコールを厳密に反映したサービスを提供するために、タイムアウト/リトライと確認応答を含む詳細情報のすべてを処理します。

RPCパッケージを実際に理解する最良の方法は、もちろん自分で使用することです。SunのRPCシステムは、スタブコンパイラrpcgenを使用して古いものです。GoogleのgRPCとApache Thriftは、現代的なものと同じです。1つを試し、すべての気に病むことが何であるかを見てください。

## 参考文献
[A70] “The ALOHA System — Another Alternative for Computer Communications”  
Norman Abramson  
The 1970 Fall Joint Computer Conference  
The ALOHA network pioneered some basic concepts in networking, including exponential back-off and retransmit, which formed the basis for communication in shared-bus Ethernet networks for years.

[BN84] “Implementing Remote Procedure Calls”  
Andrew D. Birrell, Bruce Jay Nelson  
ACM TOCS, Volume 2:1, February 1984  
The foundational RPC system upon which all others build. Yes, another pioneering effort from our friends at Xerox PARC.

[MK09] “The Effectiveness of Checksums for Embedded Control Networks”  
Theresa C. Maxino and Philip J. Koopman  
IEEE Transactions on Dependable and Secure Computing, 6:1, January ’09  
A nice overview of basic checksum machinery and some performance and robustness comparisons between them.

[LH89] “Memory Coherence in Shared Virtual Memory Systems”  
Kai Li and Paul Hudak  
ACM TOCS, 7:4, November 1989  
The introduction of software-based shared memory via virtual memory. An intriguing idea for sure, but not a lasting or good one in the end.

[SK09] “Principles of Computer System Design”  
Jerome H. Saltzer and M. Frans Kaashoek  
Morgan-Kaufmann, 2009  
An excellent book on systems, and a must for every bookshelf. One of the few terrific discussions on naming we’ve seen.

[SRC84] “End-To-End Arguments in System Design”  
Jerome H. Saltzer, David P. Reed, David D. Clark  
ACM TOCS, 2:4, November 1984  
A beautiful discussion of layering, abstraction, and where functionality must ultimately reside in computer systems.

[VJ88] “Congestion Avoidance and Control”  
Van Jacobson  
SIGCOMM ’88  
A pioneering paper on how clients should adjust to perceived network congestion; definitely one of the key pieces of technology underlying the Internet, and a must read for anyone serious about systems, and for Van Jacobson’s relatives because well relatives should read all of your papers.

\newpage

# 49 Sun’s Network File System (NFS)
分散クライアント/サーバーコンピューティングの最初の用途の1つは、分散ファイルシステムの分野でした。このような環境では、多数のクライアントマシンと1つのサーバー(またはいくつか)が存在します。サーバはそのデータをディスクに保存し、クライアントは整形式プロトコルメッセージを通じてデータを要求します。図49.1に基本設定を示します。

![](../49/img/fig49_1.PNG)

画像からわかるように、サーバーにはディスクがあり、クライアントはネットワーク上のメッセージを送信して、それらのディスク上のディレクトリとファイルにアクセスします。なぜこの取り決めに迷惑をかけるのですか？(つまり、クライアントにローカルディスクを使用させるのはどうですか？)主に、この設定により、クライアント間でデータを簡単に共有できます。したがって、あるマシン(クライアント0)のファイルにアクセスしてから別のマシン(クライアント2)を使用すると、ファイルシステムのビューは同じになります。あなたのデータは、これらの異なるマシン間で自然に共有されます。二次的な利点は、集中管理です。たとえば、多数のクライアントからではなく、少数のサーバーマシンからファイルをバックアップすることができます。もう1つの利点はセキュリティです。ロックされたマシンルーム内のすべてのサーバーを使用すると、特定の種類の問題が発生するのを防ぐことができます。

>> CRUX: HOW TO BUILD A DISTRIBUTED FILE SYSTEM  
>> どのように分散ファイルシステムを構築しますか？考えるべき重要な側面は何ですか？何が悪くなるのは簡単ですか？既存のシステムから何を学ぶことができますか？

## 49.1 A Basic Distributed File System
ここでは、単純化された分散ファイルシステムのアーキテクチャについて検討します。シンプルなクライアント/サーバ分散ファイルシステムには、これまで検討してきたファイルシステムよりも多くのコンポーネントがあります。クライアント側には、クライアント側のファイルシステムを通じてファイルとディレクトリにアクセスするクライアントアプリケーションがあります。クライアントアプリケーションは、サーバーに格納されているファイルにアクセスするために、クライアント側ファイルシステムにたいしてシステムコール(`open()`、`read()`、`write()`、`close()`、`mkdir()`など)を発行します。したがって、クライアントアプリケーションにとって、ファイルシステムは、おそらくパフォーマンスを除いて、ローカル(ディスクベース)ファイルシステムと異なるものではないようです。このように、分散ファイルシステムはファイルへの透過的なアクセスを提供します。結局のところ、誰が異なるAPIセットを必要とするファイルシステムを使用したいのか、また、そうでなければ使用する苦痛があったのでしょうか？

クライアントサイドファイルシステムの役割は、これらのシステムコールを処理するために必要なアクションを実行することです。たとえば、クライアントが`read()`要求を発行した場合、クライアント側のファイルシステムは、特定のブロックを読み取るために、サーバー側のファイルシステム(またはファイルサーバーと呼ばれる)にメッセージを送信することがあります。ファイルサーバはディスク(またはそれ自身のインメモリキャッシュ)からブロックを読み込み、要求されたデータのメッセージをクライアントに返します。クライアント側のファイルシステムは、`read()`システムコールに提供されたユーザバッファにデータをコピーします。これで要求は完了します。クライアント上の同じブロックの後続の`read()`は、クライアントメモリまたはクライアントのディスクにもキャッシュされることに注意してください。そのような場合には、ネットワークトラフィックを生成する必要はありません。

![](../49/img/fig49_2.PNG)

この簡単な概要から、クライアント/サーバー分散ファイルシステムには、クライアント側のファイルシステムとファイルサーバーという2つの重要なソフトウェアが存在することがわかります。一緒に動作することによって、分散ファイルシステムの動作が決まります。今では、特定のシステムのSunのNetwork File System(NFS)を検討する必要があります。

>> ASIDE: WHY SERVERS CRASH  
>> NFSv2プロトコルの詳細に入る前に、なぜサーバがクラッシュするのでしょうか？まあ、あなたが推測するかもしれないが、多くの理由があります。サーバは単に停電に悩まされることがあります(一時的に)。電源が復旧した場合にのみ、マシンを再起動することができます。サーバーは、よく数十万行または数百万行のコードで構成されています。したがって、彼らにはバグがあります(良いソフトウェアでも100〜1000行のコードあたりいくつかのバグがあります)ので、最終的にバグが発生し、クラッシュする可能性があります。また、メモリリークもあります。メモリリークがわずかであっても、システムのメモリ不足やクラッシュが発生します。最後に、分散システムでは、クライアントとサーバーの間にネットワークがあります。ネットワークが変な動作している場合(たとえば、パーティション化され、クライアントとサーバーは動作しているが通信できない場合など)、リモートマシンがクラッシュしたように見えるかもしれませんが、実際にはネットワーク経由で到達できないのです。

## 49.2 On To NFS
最も初期で成功した分散システムの1つがSun Microsystemsによって開発され、Sun Network File System(またはNFS)として知られています[S86]。Sunは独自のクローズドシステムを構築する代わりに、クライアントとサーバーが通信するために使用する正確なメッセージフォーマットを指定するオープンプロトコルを開発しました。異なるグループが独自のNFSサーバーを開発し、相互運用性を維持しながらNFS市場で競合する可能性があります。現在、NFSサーバー(Oracle/Sun、NetApp [HLM94]、EMC、IBMなど)を販売する企業が数多くあり、NFSの普及はこの「公開市場」アプローチに起因する可能性が高いです。

## 49.3 Focus: Simple and Fast Server Crash Recovery
この章では、古典的なNFSプロトコル(バージョン2、a.k.a. NFSv2)について説明します。これは長年の標準です。NFSv3への移行では小さな変更が加えられ、NFSv4への移行ではより大きなプロトコル変更が行われました。しかし、NFSv2はすばらしくかつ不満足である、この両方の焦点は役立ちます。

NFSv2では、プロトコルの設計における主な目標は、シンプルで高速なサーバークラッシュリカバリでした。マルチクライアント、単一サーバー環境ではこの目標は大きな意味を持ちます。サーバーがダウンしている(または利用できない)場合、すべてのクライアントマシン(およびそのユーザー)は不幸で非生産的になります。したがって、サーバーが進むにつれて、システム全体も同様になります。

## 49.4 Key To Fast Crash Recovery: Statelessness
この単純な目標は、NFSv2ではstateless protocolと呼ばれるものを設計することによって実現されています。サーバーは、設計上、各クライアントで何が起こっているかについては把握していません。たとえば、どのクライアントがどのブロックをキャッシュしているか、または各クライアントで現在開いているファイル、ファイルの現在のファイルポインタの位置などは、サーバーが認識しません。単純に言えば、サーバーはクライアントが何をやっているかを知らなくてよいのです。むしろ、プロトコルは、要求を完了するために必要なすべての情報を各プロトコル要求で配信するように設計されています。それが今ではない場合、このステートレスなアプローチは、我々がプロトコルを以下でより詳細に議論するにつれてより意味を持っていくでしょう。

ステートフル(ステートレスではない)プロトコルの例については、`open()`システムコールを考慮してください。パス名を指定すると、`open()`はファイルディスクリプタ(整数)を返します。このディスクリプタは、このアプリケーションコードのように、後続の`read()`または`write()`要求でさまざまなファイルブロックにアクセスするために使用されます(システムコールの適切なエラーチェックはスペース上の理由から省略されています)。

![](../49/img/fig49_3.PNG)

クライアント側のファイルシステムが、「ファイル'foo'を開いてディスクリプタを返す」というプロトコルメッセージをサーバーに送信することによってファイルを開くとします。ファイルサーバーはファイルをローカルに開き、ディスクリプタをクライアントに返します。その後の読み込みでは、クライアントアプリケーションはそのディスクリプタを使用して`read()`システムコールを呼び出します。クライアント側のファイルシステムは、ファイル内のディスクリプタをファイルサーバーに渡し、「ディスクリプタによって参照されるファイルからいくつかのバイトを読み取って、ここに渡します」と言っています。

この例では、ファイルディスクリプタはクライアントとサーバーの間の共有状態の一部です(Ousterhoutは分散状態[O91]と呼んでいます)。私たちが上記のように共有状態はクラッシュリカバリを複雑にします。最初の読み取りが完了した後で、クライアントが2番目の読み取りを発行する前に、サーバーがクラッシュしたとします。サーバーが起動して再び実行されると、クライアントは次に2回目の読み取りを発行します。残念ながら、サーバはfdがどのファイルを参照しているのか全く分かりません。その情報は一時的(すなわち、メモリ内)であり、したがってサーバがクラッシュしたときに失われました。この状況に対処するために、クライアントとサーバーは何らかのリカバリープロトコルに従わなければなりません。つまり、クライアントは、サーバーが知る必要のある情報をサーバーに伝えるために十分な情報をメモリに保持しないといけません(この場合そのファイルディスクリプタfdはファイルfooを参照します)。

ステートフルなサーバーがクライアントのクラッシュに対処しなければならないという事実を考えると、さらに悪化します。たとえば、ファイルを開いてクラッシュするクライアントを想像してみてください。`open()`はサーバー上のファイルディスクリプタを使います。サーバーはどのようにしてファイルを閉じることができますか？通常の操作では、クライアントは最終的に`close()`を呼び出して、ファイルを閉じるべきであることをサーバーに通知します。しかし、クライアントがクラッシュすると、サーバは`close()`を受け取ることはありません。そのため、ファイルを閉じるためにクライアントがクラッシュしたことに気づく必要があります。

これらの理由から、NFSの設計者はステートレスなアプローチを追求することに決めました。各クライアント操作には、要求を完了するために必要なすべての情報が含まれています。派手なクラッシュリカバリは不要です。サーバーはただちに再起動し、クライアントは最悪の場合、要求を再試行する必要があります。

## 49.5 The NFSv2 Protocol
したがって、NFSv2プロトコル定義に到達します。私たちの問題は簡単です。

>> THE CRUX: HOW TO DEFINE A STATELESS FILE PROTOCOL  
>> ステートレスな操作を可能にするためにネットワークプロトコルをどのように定義できますか？明らかに、`open()`のようなステートフルな呼び出しは(サーバーが開いているファイルを追跡するために)議論の一部になることはできません。ただし、クライアントアプリケーションは、`open()`、`read()`、`write()`、`close()`などの標準API callを呼び出して、ファイルやディレクトリにアクセスする必要があります。したがって、洗練された質問として、どのようにプロトコルがPOSIXファイルシステムAPIをステートレスにサポートするかを定義することができますか？

NFSプロトコルの設計を理解するための鍵は、ファイルハンドルを理解することです。ファイルハンドルは、特定の操作で操作されるファイルまたはディレクトリを一意に記述するために使用されます。したがって、多くのプロトコル要求にはファイルハンドルが含まれています。

ファイルハンドルには、ボリューム識別子、iノード番号、世代番号という3つの重要な要素があると考えることができます。これらの3つの項目は、クライアントがアクセスしたいファイルまたはディレクトリを一意の識別子で構成します。ボリューム識別子は、要求が参照するファイルシステムをサーバーに通知します(NFSサーバーは複数のファイルシステムをエクスポートできます)。inode番号は、要求がそのパーティション内のどのファイルにアクセスしているかをサーバーに伝えます。最後に、iノード番号を再利用する場合は世代番号が必要です。inode番号が再利用されるたびにインクリメントすることで、古いファイルハンドルを持つクライアントが、新しく割り当てられたファイルに誤ってアクセスすることがないようにします。

ここでは、プロトコルの重要な部分の概要を示します。完全なプロトコルは他の場所でも利用可能です(NFS [C00]の詳細な概要については、Callaghanの本を参照してください)。

![](../49/img/fig49_4.PNG)

ここでは、プロトコルの重要な要素について簡単に説明します。最初に、LOOKUPプロトコルメッセージを使用してファイルハンドルを取得し、その後ファイルデータにアクセスするために使用します。クライアントは、ディレクトリファイルのハンドルと検索するファイルの名前を渡し、そのファイル(またはディレクトリ)のハンドルに加えて、その属性もサーバーからクライアントに返されます。

たとえば、クライアントがファイルシステム(/)のルートディレクトリのディレクトリファイルハンドルをすでに持っているとします(実際には、NFSマウントプロトコルで取得されますが、これはクライアントとサーバーが最初に接続された方法です。簡潔さのためにマウントプロトコルをここで議論しません)。クライアント側で実行中のアプリケーションが/foo.txtファイルを開くと、クライアント側のファイルシステムはルックアップ要求をサーバーに送信し、ルートファイルハンドルと名前foo.txtを渡します。成功すると、foo.txtのファイルハンドル(および属性)が返されます。

不思議に思われる場合は、属性は、ファイル作成時間、最終変更時刻、サイズ、所有権とアクセス許可情報などのフィールドといったファイルシステムが各ファイルについて追跡するメタデータだけです。つまり同じタイプの情報を含む、ファイルに対して`stat()`を呼び出すと元に戻ります。

ファイルハンドルが利用可能になると、クライアントはファイルに対してREADおよびWRITEプロトコルメッセージを発行してファイルを読み書きすることができます。READプロトコルメッセージは、プロトコルが、ファイル内のオフセットと、読み込むバイト数をファイルのファイルハンドルに沿って渡すことを要求します。次に、サーバーは読み取りを発行することができます(結局のところ、ハンドルはサーバーにどのボリュームとどのiノードから読み取るべきかを通知し、オフセットとカウントは読み取るファイルのバイトを指示します)。クライアントにデータを返します(または障害が発生した場合はエラー)。WRITEは、データがクライアントからサーバーに渡され、成功コードのみが返される点を除いて、同様に処理されます。

最後の1つの興味深いプロトコルメッセージは、GETATTR要求です。ファイルハンドルがあれば、ファイルの最後の変更時刻を含めて、そのファイルの属性を取り出せます。私たちがキャッシュを議論するときに、なぜこのプロトコル要求がNFSv2で重要であるのかを見ていきます(なぜそうなっているか推測できますか？)。

## 49.6 From Protocol to Distributed File System
うまくいけば、このプロトコルがクライアント側のファイルシステムとファイルサーバを介してファイルシステムにどのように変換されているのかが分かるはずです。クライアント側のファイルシステムは開いているファイルを追跡し、一般にアプリケーション要求を関連する一連のプロトコルメッセージに変換します。サーバーは単にプロトコルメッセージに応答します。プロトコルメッセージには、要求を完了するために必要なすべての情報が含まれています。

たとえば、ファイルを読み取る単純なアプリケーションを考えてみましょう。図(図49.5)では、アプリケーションがどのようなシステムコールを行うか、そしてクライアント側のファイルシステムとファイルサーバがそのような呼び出しに応答する際の動作を示します。

図においてのいくつかのコメントがあります。まず、NFSファイルハンドルへの整数ファイルディスクリプタである、現在のファイルポインタのマッピングなど、クライアントがファイルアクセスのすべての関連状態をどのように追跡しているかに注目してください。これにより、クライアントは、各読み取り要求(もしかしたら、あなたは明示的に読み取るオフセットを指定していないことに気づいたかもしれません)を、正しくフォーマットされた読み取りプロトコルメッセージに変換して、ファイルから読み取るバイトを正確にサーバーに通知します。読み込みが成功すると、クライアントは現在のファイルの位置を更新します。後続の読み取りは同じファイルハンドルで発行されますが、異なるオフセットが発行されます。

次に、サーバーとのやりとりがどこに発生しているかがわかります。最初にファイルを開くと、クライアント側のファイルシステムはLOOKUP要求メッセージを送信します。実際には長いパス名を渡す必要がある場合(例：/home/remzi/foo.txt)、クライアントは3つのルックアップを送信します。1つはディレクトリ/のホームをルックアップし、もう1つはremziのホームをルックアップし、最後にremziのfoo.txtをルックアップします。

第3に、各サーバー要求が、要求全体を完了するために必要なすべての情報をどのように持っているかに気付くことがあります。この設計ポイントは、サーバーの障害から正常に回復できるようにするために重要です。ここで詳しく議論していきます。サーバーは要求に応答できる状態を必要としないことを保証します。

![](../49/img/fig49_5.PNG)

>> TIP: IDEMPOTENCY IS POWERFUL  
>> Idempotency(冪等:ある操作を1回行っても複数回行っても結果が同じであること)は、信頼性の高いシステムを構築する際に役立ちます。操作を複数回発行できる場合、操作の失敗を処理する方がはるかに簡単です。あなたはそれを再試行することができます。操作が冪等でない場合、人生はより困難になります。

## 49.7 Handling Server Failure with Idempotent Operations
クライアントがサーバーにメッセージを送信すると、応答を受信しないことがあります。この失敗に対するさまざまな理由が考えられます。場合によっては、ネットワークによってメッセージが破棄されることがあります。ネットワークはメッセージを失うので、要求または応答のいずれかが失われる可能性があり、したがってクライアントは決して応答を受信しません。

また、サーバーがクラッシュしたため、現在メッセージに応答していない可能性もあります。少し後に、サーバーは再起動され、再度実行を開始しますが、その間にすべての要求が失われます。これらのすべてのケースでは、クライアントに質問が残されます。サーバーがタイムリーに返信しないときにはどうすればよいでしょうか？

NFSv2では、クライアントはこれらのすべての障害を、一様かつ均一な方法で処理します。単純に要求を再試行します。具体的には、要求を送信した後、クライアントは、指定された時間が経過した後にタイマーをオフにするように設定します。タイマーがオフになる前に応答が受信されると、タイマーはキャンセルされ、すべて正常となります。ただし、応答が受信される前にタイマーが切れた場合、クライアントは要求が処理されていないとみなして再送信します。サーバーが応答すると、すべて正常であり、クライアントは問題をきちんと処理しています。

クライアントの要求を単純に再試行する能力(障害の原因にかかわらず)は、ほとんどのNFS要求の重要な特性によるものです。それらは冪等です。操作を複数回実行した結果が操作を1回実行した結果と同等である場合、操作は冪等と呼ばれます。たとえば、値をメモリ位置に3回格納する場合は、1回と同じです。したがって、「値をメモリに格納する」は、等価な演算です。ただし、カウンターを3回インクリメントした場合は、カウンターを1回だけ増やすのとは異なる量になります。従って、「インクリメントカウンタ」は冪等ではありません。より一般的には、データを読み取るだけの操作は明らかに冪等です。データを更新する操作は、このプロパティがあるかどうかを判断するために、より慎重に検討する必要があります。

NFSにおけるクラッシュリカバリの設計の中心は、最も一般的な操作のidempotencyです。LOOKUPとREADの要求は、ファイルサーバからの情報の読み取りのみを行い、更新はしないため、冪等があります。さらに興味深いことに、WRITEリクエストも冪等でもあります。たとえば、WRITEが失敗した場合、クライアントは単純にWRITEを再試行できます。WRITEメッセージには、データ、カウント、およびデータを書き込む正確なオフセットが含まれています。したがって、複数の書き込みの結果が単一の結果の結果と同じであるという知識をもって繰り返すことができます。

![](../49/img/fig49_6.PNG)

このようにして、クライアントは統一された方法ですべてのタイムアウトを処理できます。WRITE要求が単純に失われた場合(上記のケース1)、クライアントは再試行し、サーバは書き込みを実行し、すべて正常です。リクエストが送信されている間にサーバーがダウンした場合でも、2番目のリクエストが送信されたときにバックアップと実行が行われ、すべてが正常に動作します(ケース2)。最後に、サーバは実際にWRITE要求を受信し、そのディスクへの書き込みを発行し、応答を送信します。この返信が失われる可能性があり(ケース3)、再びクライアントがリクエストを再送信します。サーバーが要求を再度受け取ると、まったく同じことを単に実行します。データをディスクに書き込んで、それが完了したことを応答します。今度はクライアントが応答を受け取ると、すべて正常にクライアントはメッセージ損失とサーバー障害の両方を一様に処理します。

いくつかの操作は冪等にするのが難しいです。たとえば、すでに存在するディレクトリを作成しようとすると、mkdir要求が失敗したことが通知されます。したがって、NFSでは、ファイルサーバがMKDIRプロトコルメッセージを受信してそれを正常に実行しますが、応答が失われた場合、クライアントはそれを繰り返して、実際に操作が最初に成功した後に再試行で失敗したときに、その障害に遭遇します。したがって、人生は完璧ではありません。

>> TIP: PERFECT IS THE ENEMY OF THE GOOD (VOLTAIRE’S LAW)  
>> あなたが美しいシステムを設計するときでさえ、すべてのコーナーケースがあなたが思うように正確に動作しないことがあります。上記のmkdirの例を参照してください。異なるセマンティクスを持つようにmkdirを再設計することができます。それによって、それは冪等になります(あなたがそうする方法を考えてください)。しかし、なぜ煩わしいのでしょうか？NFS設計の哲学は重要なケースの大部分をカバーしており、全体的にシステム設計は障害に関してきれいで簡単です。言い換えると、システムは完璧ではなく、まだシステムを構築していくことは良いエンジニアリングです。明らかに、この知恵はヴォルテールによって言及されています。「賢明なイタリア人は、最高は良の敵であると言っている」そのため、これをヴォルテールの法則と呼んでいます。

## 49.8 Improving Performance: Client-side Caching
分散ファイルシステムはさまざまな理由から有効ですが、ネットワーク全体ですべての読み取りおよび書き込み要求を送信すると、パフォーマンスに大きな問題が発生する可能性があります。ネットワークは通常、ローカルメモリやディスクと比べて高速ではありません。したがって、別の問題があります。分散ファイルシステムのパフォーマンスをどのように改善できますか？

上のサブタイトルの大胆な言葉を読んで、あなたが推測するかもしれない答えは、クライアント側のキャッシュです。NFSクライアント側のファイルシステムは、サーバーから読み取ったファイルデータ(およびメタデータ)をクライアントメモリにキャッシュします。したがって、第1のアクセスが高価である(すなわち、ネットワーク通信を必要とする)間に、その後のアクセスはクライアントメモリから非常に迅速にサービスされます。

キャッシュは、書き込み用の一時バッファとしても機能します。クライアントアプリケーションが最初にファイルに書き込むとき、クライアントはデータをサーバーに書き出す前に、データをクライアントメモリ(ファイルサーバーから読み取ったデータと同じキャッシュ内)にバッファリングします。このような書き込みバッファリングは、実際の書き込みパフォーマンスとアプリケーション`write()`の待ち時間を切り離すために便利です。つまり、`write()`へのアプリケーションの呼び出しはただちに成功します(クライアント側ファイルシステムのキャッシュにデータを入れます)。あとでデータはファイルサーバーに書き出されます。

したがって、NFSクライアントはデータをキャッシュし、パフォーマンスは通常優れており、完了しました。本当にそうでしょうか？残念ながら、それほどではありません。複数のクライアントキャッシュを持つあらゆる種類のシステムにキャッシュを追加することは、キャッシュの一貫性の問題と呼ばれる大きく興味深い課題を引き起こします。

## 49.9 The Cache Consistency Problem
キャッシュ一貫性の問題は、2つのクライアントと1つのサーバーで最もよく説明されています。クライアントC1がファイルFを読み取り、そのファイルのコピーをローカルキャッシュに保持すると仮定します。次に、別のクライアントC2がファイルFを上書きし、その内容を変更するとします。新しいバージョンのファイルF(バージョン2)、またはF[v2]と古いバージョンF[v1]を呼び出すことで、2つの異なるファイル名を保持することができます。(ただし、ファイル名は同じですが、内容が異なるだけです)最後に、ファイルFにまだアクセスしていない第3のクライアントC3が存在します。

![](../49/img/fig49_7.PNG)

あなたはおそらく、今後の問題を見ることができます(図49.7)。実際、2つの副問題があります。第1の副問題は、クライアントC2がそれらの書き込みをそのキャッシュに一時的にバッファリングしてから、それらをサーバに送ることができることです。この場合、F[v2]はC2のメモリに格納されていますが、別のクライアント(たとえばC3)からFにアクセスすると、古いバージョンのファイル(F[v1])が取り出されます。したがって、クライアントでの書き込みをバッファリングすることによって、他のクライアントがファイルの古いバージョンを取得する可能性があります。マシンC2にログインし、Fを更新してからC3にログインしてファイルを読み込もうとすると、古いコピーを取得する場合を想像してください。確かに、これはイライラする可能性があります。したがって、キャッシュ一貫性問題のupdate visibility(更新の可視性)と呼ばれている側面を見てみましょう。あるクライアントからの更新が他のクライアントでいつ見えるようになるのですか？

キャッシュ一貫性の第2の副問題は失効したキャッシュです。この場合、C2は最終的にファイルサーバーへの書き込みをフラッシュし、したがってサーバーは最新バージョン(F[v2])を持ちます。しかし、C1はキャッシュ内にF[v1]を保持しています。C1上で実行されているプログラムがファイルFを読み込んだ場合、それは古いバージョン(F[v1])であり、最新のコピー(F[v2])ではなく(よく)望ましくありません。

NFSv2実装は、これらのキャッシュ一貫性の問題を2つの方法で解決します。第1に、更新の可視性に対処するために、クライアントはflush on close(close to open)consistency semantics(整合性セマンティクス)と呼ばれることを実装します。具体的には、ファイルがクライアントアプリケーションに書き込まれ、続いてクライアントアプリケーションによって閉じられると、クライアントはすべての更新(つまり、キャッシュ内のダーティページ)をサーバーにフラッシュします。flush on closeの一貫性により、NFSは、別のノードからの次のオープンに最新のファイル・バージョンが確実に表示されるようにします。

次に、古いキャッシュの問題に対処するために、NFSv2クライアントはまず、キャッシュされたコンテンツを使用する前にファイルが変更されているかどうかを確認します。具体的には、ファイルを開くときに、クライアント側のファイルシステムは、ファイルの属性を取得するためにGETATTR要求をサーバーに発行します。重要なことに、属性には、ファイルがサーバー上で最後に変更された時期に関する情報が含まれています。変更時刻がクライアントキャッシュにフェッチされた時刻よりも新しい場合、クライアントはファイルを無効にしてクライアントキャッシュから削除し、その後の読み込みがサーバーに送られ、最新バージョンのファイルを取得するようにします。一方、クライアントがファイルの最新バージョンを持っていると判断すると、キャッシュされたコンテンツが使用され、パフォーマンスが向上します。

Sunのオリジナルチームがこの解決策を古いキャッシュの問題に実装したとき、彼らは新しい問題を認識しました。突然、NFSサーバーにGETATTR要求があふれていました。従うべき良いエンジニアリングの原則は、一般的なケースを設計し、それをうまく機能させることです。ここでは、ファイルが単一のクライアント(おそらくは繰り返し)からのみアクセスされたのが一般的でしたが、クライアントは常に他の誰もファイルを変更していないことを確認するためにGETATTR要求をサーバーに送信しなければなりませんでした。このように、ほとんどの場合、誰もが「誰かこのファイルを変更した？」と尋ね、常時確認し、クライアントはサーバを攻撃してしまいます。

このような状況(多少)を改善するために、属性キャッシュが各クライアントに追加されました。クライアントはそれにアクセスする前にファイルを検証しますが、たいていの場合、属性キャッシュを調べて属性を取得するだけです。特定のファイルの属性は、ファイルが最初にアクセスされたときにキャッシュに格納され、一定の時間(たとえば3秒)後にタイムアウトになります。したがって、これらの3秒間に、すべてのファイルアクセスは、キャッシュされたファイルを使用することがOKであると判断し、サーバーとのネットワーク通信を行わずに実行します。

## 49.10 Assessing NFS Cache Consistency
NFSキャッシュの一貫性についての最後の言葉。flush on closeの振る舞いが「意味をなす」ために追加されましたが、特定のパフォーマンス問題が発生しました。具体的には、一時ファイルまたは短命ファイルがクライアントで作成され、すぐに削除された場合でも、サーバーに強制されます。より理想的な実装では、このような短命ファイルは削除されるまでメモリ内に保持されるため、サーバーのやりとりが完全になくなり、パフォーマンスが向上する可能性があります。

さらに重要なのは、NFSに属性キャッシュを追加すると、ファイルのどのバージョンが取得されているかを正確に理解することが難しくなりました。時々あなたは最新バージョンを手に入れます。属性キャッシュがまだタイムアウトしていない場合、クライアントがクライアントメモリにあったものをあなたに提供するため、古いバージョンを取得することがあります。ほとんどの場合、これは大丈夫でしたが、時には変な行動につながることもあります。

そこで、NFSクライアントのキャッシュというおかしな点について説明しました。これは、実装の詳細が、ユーザの観測可能なセマンティクスを定義する役目をする興味深い例です。

## 49.11 Implications on Server-Side Write Buffering
これまでのところ、クライアントのキャッシュに焦点が当てられていました。それは興味深い問題のほとんどが発生する場所です。しかし、NFSサーバーはメモリが大量に消費されるマシンであることが多いため、キャッシュに関する懸念もあります。データ(およびメタデータ)がディスクから読み込まれると、NFSサーバーはそのデータをメモリに保持し、その後のデータ(およびメタデータ)の読み取りはディスクには行きません。これにパフォーマンスが(少し)向上します。

より興味深いのは、書き込みバッファリングの場合です。NFSサーバは、書込みが安定した記憶装置(例えば、ディスクまたは他の永続的デバイス)に強制されるまで、WRITEプロトコル要求で成功を返すことは絶対にありえません。サーバーのメモリにデータのコピーを置くことができますが、WRITEプロトコル要求でクライアントに成功を返すと、正しく動作しなくなる可能性があります。なぜかあなたは理解できますか？

答えは、クライアントがサーバーの障害をどのように処理するかについての前提にあります。クライアントが発行した次の書き込み順序を想像してみてください。
```c
write(fd, a_buffer, size); // fill first block with a’s
write(fd, b_buffer, size); // fill second block with b’s
write(fd, c_buffer, size); // fill third block with c’s
```
これらの書き込みは、ファイルの3つのブロックをa、b、cのブロックで上書きします。したがって、ファイルは最初に次のように見えます。
```
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy
zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz
```
これらの書き込み後の最終的な結果は、x、y、zがそれぞれa、b、cで上書きされることを期待するかもしれません。
```
aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb
cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc
```
ここでは、この3つのクライアントの書き込みが、3つの異なるWRITEプロトコル・メッセージとしてサーバーに発行されたと想定します。第1のWRITEメッセージがサーバによって受信され、ディスクに発行され、クライアントがその成功を通知したと仮定します。ここで、2番目の書き込みがメモリにバッファリングされていると仮定し、サーバーはクライアントに成功を報告してから、強制的にディスクに書き込みます。残念ながら、サーバーはディスクに書き込む前にクラッシュします。サーバーはすぐに再起動し、3回目の書き込み要求を受信します。これも成功します。したがって、クライアントに対してはすべての要求は成功しましたが、ファイルの内容が次のようになっていることに驚いています。
```
aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy <--- oops
cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc
```
Yikes！サーバーは、2番目の書き込みがディスクにコミットする前に成功したことをクライアントに通知したため、古いチャンクがファイルに残されます。これは、アプリケーションによっては致命的なものです。この問題を回避するには、クライアントに成功を通知する前に、NFSサーバーが各書き込みを安定した(永続的な)ストレージにコミットする必要があります。これにより、クライアントは書き込み中にサーバー障害を検出し、最終的に成功するまで再試行できます。そうすることで、上の例のようにファイルの内容が混ざってしまうことはありません。

この要件がNFSサーバーの実装で発生する問題は、書き込みパフォーマンスに対して大きな注意を払っていないため、パフォーマンスの大きなボトルネックになる可能性があることです。事実、書き込みを迅速に実行できるNFSサーバを構築するという単純な目的で、一部の企業(Network Applianceなど)が生まれました。彼らが使用するトリックの1つは、最初にbattery backed memoryに書き込みを入れることで、データを失うことがなく、すぐにディスクに書き込む必要もなく、書き込み要求にすばやく応答することができます。2番目のトリックは、最終的にそうする必要があるときにディスクにすばやく書き込むように設計されたファイルシステム設計を使用することです[HLM94、RO91]。

## 49.12 Summary
私たちは、NFS分散ファイルシステムの導入を見てきました。NFSは、サーバの障害に直面している単純かつ高速なリカバリのアイデアを中心にしており、慎重なプロトコル設計によってこの目的を達成しています。操作の強制力は不可欠です。クライアントが失敗した操作を安全に再生できるので、サーバーが要求を実行したかどうかにかかわらず、そうすることは大丈夫です。

また、マルチクライアント、シングルサーバシステムへのキャッシングの導入が複雑になる可能性もあります。特に、システムは、合理的に動作するためにキャッシュ一貫性の問題を解決する必要があります。しかし、NFSはやや特殊なやり方でこれを実行しますが、これは時折、観察可能な変な動作を引き起こすことがあります。最後に、サーバーのキャッシュをどのように扱うのが難しいのかを見てみましょう。成功を返す前に、サーバーへの書き込みを安定したストレージに強制する必要があります(それ以外の場合はデータが失われる可能性があります)。

私たちは確かに関連性の高い他の問題、特にセキュリティについては話していません。初期のNFS実装におけるセキュリティは非常に緩慢でした。クライアント上の任意のユーザーが他のユーザーとして偽装して、実質的にすべてのファイルにアクセスすることはかなり容易でした。より重大な認証サービス(例えば、Kerberos [NT94])とのその後の統合は、これらの明白な欠点に対処してきました。

## 参考文献
[C00] “NFS Illustrated”  
Brent Callaghan  
Addison-Wesley Professional Computing Series, 2000  
A great NFS reference; incredibly thorough and detailed per the protocol itself.

[HLM94] “File System Design for an NFS File Server Appliance”  
Dave Hitz, James Lau, Michael Malcolm  
USENIX Winter 1994. San Francisco, California, 1994  
Hitz et al. were greatly influenced by previous work on log-structured file systems.

[NT94] “Kerberos: An Authentication Service for Computer Networks”  
B. Clifford Neuman, Theodore Ts’o  
IEEE Communications, 32(9):33-38, September 1994  
Kerberos is an early and hugely influential authentication service. We probably should write a book chapter about it sometime...

[O91] “The Role of Distributed State”  
John K. Ousterhout  
Available: ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/state.ps  
A rarely referenced discussion of distributed state; a broader perspective on the problems and challenges.

[P+94] “NFS Version 3: Design and Implementation”  
Brian Pawlowski, Chet Juszczak, Peter Staubach, Carl Smith, Diane Lebel, Dave Hitz  
USENIX Summer 1994, pages 137-152  
The small modifications that underlie NFS version 3.

[P+00] “The NFS version 4 protocol”  
Brian Pawlowski, David Noveck, David Robinson, Robert Thurlow  
2nd International System Administration and Networking Conference (SANE 2000)  
Undoubtedly the most literary paper on NFS ever written.

[RO91] “The Design and Implementation of the Log-structured File System”  
Mendel Rosenblum, John Ousterhout  
Symposium on Operating Systems Principles (SOSP), 1991  
LFS again. No, you can never get enough LFS.

[S86] “The Sun Network File System: Design, Implementation and Experience”  
Russel Sandberg  
USENIX Summer 1986  
The original NFS paper; though a bit of a challenging read, it is worthwhile to see the source of these wonderful ideas.

[Sun89] “NFS: Network File System Protocol Specification”  
Sun Microsystems, Inc. Request for Comments: 1094, March 1989  
Available: http://www.ietf.org/rfc/rfc1094.txt  
The dreaded specification; read it if you must, i.e., you are getting paid to read it. Hopefully, paid a lot. Cash money!

[V72] “La Begueule”  
Francois-Marie Arouet a.k.a. Voltaire  
Published in 1772  
Voltaire said a number of clever things, this being but one example. For example, Voltaire also said “If you have two religions in your land, the two will cut each others throats; but if you have thirty religions, they will dwell in peace.” What do you say to that, Democrats and Republicans?

\newpage

# 50 The Andrew File System (AFS)
Andrew File Systemは、カーネギーメロン大学(CMU)で1980年代に導入されました[H + 88]。カーネギーメロン大学のSatyanarayanan教授(略して「Satya」)が率いるこのプロジェクトの主な目標は簡単でした。それはスケールです。具体的には、サーバーができるだけ多くのクライアントをサポートできるように、分散ファイルシステムをどのように設計できますか？

興味深いことに、スケーラビリティに影響を与える設計と実装には、さまざまな側面があります。最も重要なのは、クライアントとサーバーの間のプロトコルの設計です。たとえば、NFSでは、プロトコルによって、クライアントは定期的にサーバーをチェックして、キャッシュされた内容が変更されたかどうかを判断します。各チェックではサーバーリソース(CPUおよびネットワーク帯域幅を含む)が使用されるため、このようなチェックを頻繁に行うと、サーバーが応答できるクライアントの数が制限され、スケーラビリティが制限されます。

AFSはまた、当初から合理的な目に見える動作がファーストクラスの関心事であった点でNFSとは異なります。NFSでは、クライアント側のキャッシュタイムアウト間隔を含む低レベルの実装の詳細に直接依存するため、キャッシュの一貫性は記述しにくいです。AFSでは、キャッシュの一貫性は簡単で分かりやすく、ファイルがオープンされると、クライアントは通常、サーバーから最新の一貫性のあるコピーを受け取ります。

## 50.1 AFS Version 1
AFS [H + 88、S + 85]の2つのバージョンについて説明します。最初のバージョン(AFSv1と呼ばれますが、実際は元のシステムはITC分散ファイルシステム[S + 85]と呼ばれていました)では、基本的な設計の一部が行われていましたが、スケールの設計はされていません。それを含めて再設計された最終的なプロトコルがあります(AFSv2、または単にAFSと呼ぶ)[H + 88]。ここで最初のバージョンについて説明します。

![](../50/img/fig50_1.PNG)

AFSのすべてのバージョンの基本的な考え方の1つは、ファイルにアクセスしているクライアント・マシンのローカル・ディスク上のファイル全体のキャッシュです。ファイルを`open()`するとき、ファイル全体(存在する場合)がサーバーからフェッチされ、ローカルディスク上のファイルに保存されます。後続のアプリケーションの`read()`および`write()`操作は、ファイルが格納されているローカルファイルシステムにリダイレクトされます。したがって、これらの操作はネットワーク通信を必要とせず、高速です。最後に、`close()`のときに、ファイル(変更されている場合)がサーバーにフラッシュされます。NFSはブロックをキャッシュします(ファイル全体ではなく、NFSはファイル全体のすべてのブロックをキャッシュすることはもちろんですが)が、クライアントメモリ(ローカルディスクではありません)にキャッシュするキャッシュとは明らかに対照的です。

もう少し詳しく説明しましょう。クライアント・アプリケーションが最初に`open()`を呼び出すと、(AFS設計者がVenusと呼ぶ)AFSクライアント側コードは、フェッチプロトコル・メッセージをサーバーに送信します。フェッチプロトコルメッセージは、ファイルサーバ(Viceと呼ばれるグループ)に目的のファイル(たとえば、/home/remzi/notes.txt)のパス名全体を渡します。その後、パス名をたどって目的のファイルを探し、ファイル全体をクライアントに戻します。クライアントサイドのコードは、クライアントのローカルディスクにファイルを(ローカルディスクに書き込むことによって)キャッシュします。上記のように、後続の`read()`および`write()`システム・コールはAFSでは厳密にローカルです(サーバとの通信は発生しません)。それらは単にファイルのローカルコピーにリダイレクトされます。`read()`と`write()`の呼び出しはローカルファイルシステムへの呼び出しと同じように動作するため、ブロックにアクセスするとクライアントのメモリにもキャッシュされます。したがって、AFSはクライアント・メモリーも使用して、ローカル・ディスクにあるブロックのコピーをキャッシュします。最後に、終了すると、AFSクライアントは、ファイルが変更された(すなわち、書き込みのために開かれた)か否かをチェックします。もしそうであったら、新しいバージョンをStoreプロトコルメッセージでサーバーにフラッシュし、ファイルとパス名の全体を永続的な保存のためにサーバーに送信します。

次回にファイルにアクセスするとき、AFSv1はずっと効率的です。具体的には、クライアント側コードは、ファイルが変更されたかどうかを判断するために、最初に(TestAuthプロトコルメッセージを使用して)サーバーに接続します。そうでない場合、クライアントはローカルにキャッシュされたコピーを使用し、ネットワーク転送を回避してパフォーマンスを向上させます。上記の図は、AFSv1のプロトコル・メッセージの一部を示しています。この初期のバージョンのプロトコルでは、ファイルの内容のみがキャッシュされていました。たとえば、ディレクトリはサーバーに保存されていました。

>> TIP: MEASURE THEN BUILD (PATTERSON’S LAW)  
>> 私たちのアドバイザーであるDavid Patterson(RISCとRAIDの名声)は、この問題を解決する新しいシステムを構築する前に、常にシステムの測定と問題の実証を促していました。本能ではなく実験的証拠を用いることで、システム構築のプロセスをより科学的な取り組みに変えることができます。そうすることで、改善されたバージョンが開発される前に、システムをどの程度正確に測定するかを考えさせるという利点があります。最終的に新しいシステムを構築するときには、結果的に2つの点が優れています。まず、実際の問題を解決しているという証拠があります。次に、新しいシステムを適切な場所で測定し、実際に最新の状態を改善する方法を示しました。それで、私たちはこれをパターソンの法則と呼んでいます。

## 50.2 Problems with Version 1
この最初のバージョンのAFSのいくつかの重要な問題は、設計者がファイルシステムを再考する動機となりました。問題を詳細に調べるために、AFSの設計者は、何が間違っていたのかを見つけるために、既存のプロトタイプを測定するのに多大な時間を費やしました。このような実験は良いことです。なぜなら、測定はシステムの仕組みとその改善方法を理解する鍵です。具体的なデータを得ることは、システム構築の必要な部分です。彼らの研究では、AFSv1の主な2つの問題点を発見しました。

- パストラバーサルコストが高すぎます：  
フェッチまたはストアプロトコル要求を実行するとき、クライアントはパス名全体(たとえば、/home/remzi/notes.txt)をサーバーに渡します。サーバは、ファイルにアクセスするためには、完全なパス名トラバーサルを実行しなければなりません。最初にルートディレクトリでhomeを探し、次にhomeでremziを探します。そして最終的に目的のファイルが位置しているところまで下がっていきます。一度に多くのクライアントがサーバーにアクセスすると、AFSの設計者は、サーバーがディレクトリー・パスを辿るだけのCPU時間の大部分を費やしていることに気付きました。

- クライアントが多すぎるTestAuthプロトコルメッセージを発行します。  
NFSやGETATTRプロトコル・メッセージの過多のように、AFSv1はローカル・ファイル(またはその統計情報)がTestAuthプロトコル・メッセージで有効かどうかを確認するために大量のトラフィックを生成しました。したがって、サーバーは、キャッシュされたファイルのコピーを使用することがOKであったかどうかをクライアントに伝えるのに多くの時間を費やしました。ほとんどの場合、答えはファイルが変更されていないということでした。

実際には、AFSv1には2つの問題がありました。サーバー間で負荷が分散されていないため、サーバーはクライアントごとに異なるプロセスを使用し、コンテキスト切り替えやその他のオーバーヘッドを引き起こしました。負荷の不均衡の問題は、負荷を分散するために管理者がサーバー間を移動できるボリュームを導入することで解決されました。プロセスの代わりにスレッドを使用してサーバーを構築することにより、コンテキスト・スイッチの問題がAFSv2で解決されました。しかし、文書のスペースのために、ここでは、システムの規模を制限する上記の2つの主要なプロトコル問題に焦点を当てています。

## 50.3 Improving the Protocol
上記の2つの問題は、AFSのスケーラビリティを制限しました。サーバCPUがシステムのボトルネックになり、各サーバは過負荷にならないために20クライアントにしかサービスできませんでした。サーバーがTestAuthメッセージを受信しすぎていて、フェッチまたはストアメッセージを受信したときに、ディレクトリ階層を走査するのに時間がかかり過ぎていました。したがって、AFS設計者は問題に直面していました。

>> THE CRUX: HOW TO DESIGN A SCALABLE FILE PROTOCOL  
>> どのようにしてサーバ相互作用の数を最小限にするためにプロトコルを再設計するべきですか？つまり、TestAuthメッセージの数をどのように減らすことができますか？さらに、これらのサーバーのやりとりを効率的にするためにプロトコルをどのように設計することができますか？これらの問題の両方を攻撃することにより、新しいプロトコルはAFSというよりスケーラブルなバージョンになります。

## 50.4 AFS Version 2
AFSv2では、クライアント/サーバーの対話数を減らすためのコールバックという概念が導入されました。コールバックとは、サーバーからクライアントへの約束であり、クライアントがキャッシュしているファイルが変更されたときにサーバーがクライアントに通知することを意味します。この状態をシステムに追加することで、クライアントはサーバーに接続しなくても、キャッシュされたファイルがまだ有効かどうかを調べる必要がなくなります。むしろ、サーバーはそれがそうでないと指示するまでファイルが有効であるとみなします。ポーリングと割り込みの類推に注意してください。

AFSv2では、クライアントが関心を持っているファイルを指定するために、パス名ではなくfile identifier(FID)(NFSファイルハンドルに似ています)という概念も導入されました。AFSのFIDは、ボリューム識別子、ファイル識別子、uniquifier(ファイルが削除されたときにボリュームとファイルIDの再利用を可能にするため)があります。したがって、サーバーにパス名全体を送信し、サーバーがパス名を調べて目的のファイルを見つけるのではなく、クライアントはパス名を一度に1つずつ歩き、結果をキャッシングしてサーバーの負荷を軽減します。

たとえば、クライアントが/home/remzi/notes.txtファイルにアクセスし、homeが/にマウントされたAFSディレクトリー(つまり、/がローカル・ルート・ディレクトリーであったが、ホームとその子がAFSにある)であった場合、クライアントは最初にhomeのディレクトリ内容を取得し、それらをローカルディスクにキャッシュし、ホームにコールバックを設定します。次に、クライアントはディレクトリremziをフェッチし、ローカルディスクにキャッシュし、remziでコールバックを設定します。最後に、クライアントはnotes.txtを取得し、この通常のファイルをローカルディスクにキャッシュし、コールバックを設定し、最後ファイルディスクリプタを呼び出しアプリケーションに返します。要約については、図50.2を参照してください。

![](../50/img/fig50_2.PNG)

しかし、NFSとの主な違いは、ディレクトリまたはファイルの各フェッチでAFSクライアントがサーバーとコールバックを確立し、サーバーがキャッシュ状態の変更をクライアントに通知することを保証することです。利点は明白です。/home/remzi/notes.txtへの最初のアクセスは(上記のように)多数のクライアント/サーバ・メッセージを生成しますが、ファイルnotes.txtと同様にすべてのディレクトリのコールバックを確立します。アクセスは全てローカルであり、サーバーとのやり取りはまったく必要ありません。したがって、クライアントでファイルがキャッシュされる一般的なケースでは、AFSはローカルディスクベースのファイルシステムとほぼ同じように動作します。1つのファイルに複数回アクセスすると、2回目のアクセスはファイルにローカルでアクセスするのと同じくらい速くなければなりません。

>> ASIDE: CACHE CONSISTENCY IS NOT A PANACEA  
>> 分散ファイルシステムについて論じるときには、ファイルシステムが提供するキャッシュの一貫性が非常に重要です。しかし、このベースラインの一貫性は、複数のクライアントからのファイルアクセスに関するすべての問題を解決するものではありません。たとえば、複数のクライアントがチェックインとコードのチェックアウトを実行するコードリポジトリを構築している場合、基礎となるファイルシステムに依存してすべての作業を行うことはできません。そのような同時アクセスが発生した場合に「正しい」ことが起こることを確実にするために、明示的なファイルレベルのロックを使用する必要があります。実際、同時更新を本当に気にするアプリケーションは、競合を処理するための余分な機構を追加します。この章で説明したベースラインの一貫性と以前のものは、主にカジュアルな使用に役立ちます。つまり、ユーザーが別のクライアントにログインすると、合理的なバージョンのファイルが表示されます。これらのプロトコルからもっと多くのことを期待することは、失敗、失望、そして涙が詰まった不満のために自分自身で設定することです。

## 50.5 Cache Consistency
私たちがNFSについて議論したとき、私たちが検討したキャッシュ一貫性の2つの側面、update visibility(更新の可視性)とcache staleness(キャッシュが古くなる)がありました。更新の可視性では、問題は次のような場合です。サーバーはいつ新しいバージョンのファイルで更新されますか？cache stalenessの問題は次のとおりです。サーバーに新しいバージョンがインストールされると、古いバージョンのキャッシュされたコピーではなく新しいバージョンがクライアントに表示されるまでの時間はどのくらいですか？

コールバックとファイル全体のキャッシュのために、AFSによって提供されるキャッシュ一貫性は、記述し理解しやすいです。考慮すべき重要な2つのケースがあります。異なるマシン上のプロセス間の一貫性と、同じマシン上のプロセス間の一貫性です。

異なるマシン間では、AFSにより、サーバーで更新が表示され、更新されたファイルがクローズされたときとまったく同じ時刻にキャッシュ・コピーが無効になります。クライアントはファイルを開き、それに(おそらくは繰り返して)書き込みを行います。最終的に閉じられると、新しいファイルはサーバーにフラッシュされます(したがって可視化されます)。この時点で、サーバーはキャッシュされたコピーを持つクライアントのコールバックを「break(中断)」します。ブレークは、各クライアントに連絡して、ファイル上のコールバックがもはや有効ではないことを通知することで実現されます。この手順では、クライアントがファイルの失効したコピーを読み取らないようにします。これらのクライアントで後で開くと、サーバーから新しいバージョンのファイルを再度取得する必要があります(また、新しいバージョンのファイルでコールバックを再確立する役割も果たします)。

AFSは、同じマシン上のプロセス間でこの単純なモデルを例外にします。この場合、ファイルへの書き込みは、他のローカルプロセスに即座に見えます(すなわち、最新の更新を見るためにファイルが閉じられるまで待つ必要はない)。この動作は、一般的なUNIXのセマンティクスに基づいているため、単一のマシンを使用すると、期待どおりに動作します。別のマシンに切り替える場合にのみ、より一般的なAFS整合性メカニズムを検出できます。

さらなる議論に値する興味深いものがクロスマシンの場合に1つあります。特に、異なるマシン上のプロセスが同時にファイルを変更している場合、AFSは当然のことながら最後の書き込んだもの(おそらくlast closer winsと呼ばれる)を採用しています。具体的には、`close()`を最後に呼び出したクライアントは、最後にサーバー上のファイル全体を更新し、したがって「勝った」ファイル、つまり他の人が見ることができるようにサーバー上に残っているファイルになります。結果は、1つのクライアントまたは別のクライアントによって全体的に生成されたファイルです。NFSのようなブロックベースのプロトコルとの違いに注意してください。NFSでは、個々のブロックの書き込みが、各クライアントがファイルを更新するときにサーバにフラッシュされる可能性があり、最終的にサーバ上のファイルが両方のクライアントからの複数の更新が混ざった状態になります。多くの場合、そのような混合ファイル出力はあまり意味がありません。すなわち、2つのクライアントによってJPEG画像が改変されると想像してみてください。結果として生じる書き込みの混合は、有効なJPEGを構成しない可能性があります。

これらの異なるシナリオのいくつかを示すタイムラインを図50.3に示します。列は、Client1とそのキャッシュ状態の2つのプロセス(P1とP2)、Client2の1つのプロセス(P3)とそのキャッシュ状態、およびサーバー(Server)の動作を示しています。全てが想像上、Fと呼ばれる単一のファイル上で動作します。サーバーの場合、図は左の操作が完了した後のファイルの内容を単純に示しています。それを読んで、なぜそれぞれの読み取りが結果を返すのか理解できるかどうかを確認してください。あなたが迷った場合、右のコメント欄が役立ちます。

![](../50/img/fig50_3.PNG)

## 50.6 Crash Recovery
上記の説明から、クラッシュリカバリがNFSよりも関与していることがわかります。あなたは正しいでしょう。例えば、クライアントC1が再起動しているときなど、サーバ(S)がクライアント(C1)に接続できない短い時間があるとします。C1は利用できませんが、Sは1つ以上のコールバックリコールメッセージを送信しようとしている可能性があります。例えば、C1がそのローカルディスク上にファイルFをキャッシュし、次にC2(別のクライアント)がFを更新したとすると、Sはそのファイルをキャッシュしてローカルキャッシュから削除するメッセージをすべてのクライアントに送信します。C1は再起動時にクリティカルなメッセージを見逃す可能性があるため、システムに再参加する際にC1はすべてのキャッシュ内容を疑わしいものとして扱う必要があります。したがって、ファイルFへの次のアクセス時に、C1は、ファイルFのキャッシュされたコピーがまだ有効であるかどうかをサーバに(TestAuthプロトコルメッセージを用いて)尋ねるべきです。もしそうなら、C1はそれを使うことができます。もしそうでなければ、C1はサーバから新しいバージョンを取り出すべきです。

クラッシュ後のサーバー回復もより複雑です。発生する問題は、コールバックがメモリ内に保持されていることです。したがって、サーバが再起動すると、どのクライアントマシンがどのファイルを持っているかは分かりません。したがって、サーバーを再起動すると、サーバーの各クライアントは、サーバーがクラッシュして、すべてのキャッシュの内容を疑わしいものとして扱い、それを使用する前にファイルの有効性を再確立する必要があります。したがって、サーバクラッシュは、各クライアントがタイムリーにクラッシュを認識していることを保証しなければならない、またはクライアントが古いファイルにアクセスする危険性がある大きなイベントです。このような回復を実装するには多くの方法があります。たとえば、サーバーを起動して再度実行するときに、それぞれのクライアントに対して(「キャッシュ内容を信頼しないで」)とメッセージを送ります。またはクライアントが定期的にサーバーが稼動していることを確認します(heartbeatメッセージと呼ばれています)。ご覧のように、よりスケーラブルで合理的なキャッシングモデルを構築するコストがあります。NFSでは、クライアントはほとんどサーバクラッシュに気付けませんでした。

## 50.7 Scale And Performance Of AFSv2
新しいプロトコルを導入したことで、AFSv2が測定され、元のバージョンよりもはるかにスケーラブルであることがわかりました。実際、各サーバーは約20のクライアントではなく約50のクライアントをサポートできます。さらに利点は、クライアント側のパフォーマンスがローカルのパフォーマンスに非常に近くなることでした。一般的なケースでは、すべてのファイルアクセスがローカルであったためです。ファイルの読み込みは通常、ローカルのディスクキャッシュ(および場合によってはローカルメモリ)に送られます。クライアントが新しいファイルを作成したり、既存のファイルに書き込んだ場合にのみ、Storeメッセージをサーバーに送信し、新しい内容でファイルを更新する必要がありました。

一般的なファイル・システム・アクセス・シナリオをNFSと比較することにより、AFSパフォーマンスに関するいくつかの見通しを得ることもできます。図50.4は、我々の定性的比較の結果を示しています。

![](../50/img/fig50_4.PNG)

図では、さまざまなサイズのファイルに対して、一般的な読み書きパターンを分析的に調べます。小さなファイルにはN_s個のブロックがあります。中規模のファイルにはN_m個のブロックがあります。大きなファイルにはN_Lブロックがあります。中小のファイルはクライアントのメモリに収まると仮定します。大容量のファイルはローカルディスクに収められますが、クライアントメモリには収まりません。

また、分析のために、ファイルブロックのリモートサーバーへのネットワーク経由のアクセスには、L_netの時間単位がかかります。ローカルメモリへのアクセスにはL_memが必要で、ローカルディスクへのアクセスにはL_diskが必要です。一般的な前提は、L_net > L_disk > L_memです。最後に、ファイルへの最初のアクセスがどのキャッシュでもヒットしないと仮定します。関連するキャッシュがファイルを保持するのに十分な容量を有する場合、引き続くファイルアクセス(すなわち、「再読み込み」)はキャッシュ内でヒットします。

図の列は、特定の操作(例えば、小さなファイル順次読み取り)がNFSまたはAFSのいずれかにおおよそかかる時間を示しています。右端の列には、AFSとNFSの比率が表示されています。

我々は以下のような観察を行います。まず、多くの場合、各システムのパフォーマンスはほぼ同等です。例えば、最初にファイル(例えば、仕事量1、3、5)を読み取るとき、リモートサーバからファイルをフェッチする時間がほとんどであり、両方のシステムで同じです。この場合、ファイルをローカル・ディスクに書き込む必要があるため、AFSが遅いと考えるかもしれません。ただし、これらの書き込みはローカル(クライアント側)のファイルシステムキャッシュによってバッファされるため、コストは隠されている(上手く消している)可能性があります。同様に、AFSがキャッシュされたコピーをディスクに保管するため、ローカル・キャッシュ・コピーからのAFS読み取りが遅くなると考えるかもしれません。ただし、AFSはここでもローカル・ファイル・システムのキャッシングから利益を得ています。AFS上の読み取りはクライアント側のメモリー・キャッシュでヒットし、パフォーマンスはNFSと似ています。

第2に、大きなファイルシーケンシャル再読み込み(Workload 6)の間に興味深い違いが生じます。AFSは大きなローカル・ディスク・キャッシュを持っているため、ファイルに再度アクセスするとそこからファイルにアクセスします。対照的に、NFSはクライアントメモリ内のブロックのみをキャッシュすることができます。その結果、大きなファイル(すなわち、ローカルメモリよりも大きいファイル)が再読された場合、NFSクライアントはリモートサーバからファイル全体を再フェッチする必要があります。したがって、AFSは、リモートアクセスが実際にローカルディスクよりも遅いと仮定すると、この場合はL_net / L_diskの要因によってNFSより速くなります。この場合のNFSはサーバーの負荷を増加させ、規模にも影響します。

第3に、(新しいファイルの)順次書き込みは、両方のシステム(Workloads 8,9)で同様に実行する必要があることに注意してください。この場合、AFSはファイルをローカル・キャッシュ・コピーに書き込みます。ファイルがクローズされると、AFSクライアントはプロトコルに従ってサーバーへの書き込みを強制します。NFSはクライアント側のメモリ圧迫のために、おそらくいくつかのブロックをサーバーに強制しますが、ファイルが閉じられたときにサーバーに書き込むことで、NFSフflush on closeの一貫性を維持します。すべてのデータをローカル・ディスクに書き込むため、AFSの方が遅いと考えるかもしれません。しかし、それがローカルのファイルシステムに書き込んでいることに気づいてください。これらの書き込みは最初にページ・キャッシュにコミットされ、後で(バックグラウンドで)ディスクにのみ行われるため、AFSはクライアント側のメモリキャッシング基盤のパフォーマンスが向上するため、メリットを受けます。

第4に、順次ファイルの上書き(Workload 10)でAFSが悪化することに注意してください。ここまでは、書き込んだ仕事量も新しいファイルを作成していると想定していました。今回の場合、ファイルが存在し上書きされます。上書きはAFSにとって特に悪いケースです。これは、クライアントが古いファイルを最初にフェッチし、後でそのファイルを上書きするためです。対照的に、NFSは単にブロックを上書きして、最初の(役に立たない)読み込みを回避します。

最後に、大容量ファイル内の小さなデータ・サブセットにアクセスする仕事量は、AFS(Workloads 7,11)よりもNFSが優れたパフォーマンスを発揮します。このような場合、AFSプロトコルは、ファイルがオープンされたときにファイル全体をフェッチします。残念ながら、わずかな読み取りまたは書き込みしか実行されません。さらに悪いことに、ファイルが変更されると、ファイル全体がサーバーに書き戻され、パフォーマンスの影響が倍増します。NFSは、ブロックベースのプロトコルとして、読み取りまたは書き込みのサイズに比例する入出力を実行します。

全体的に見て、NFSとAFSは異なる仮定をしており、結果としてパフォーマンスの結果が異なることは驚くことではありません。これらの違いが重要かどうかは、いつものように仕事量の問題です。

>> ASIDE: THE IMPORTANCE OF WORKLOAD  
>> どのシステムを評価するかの課題は、仕事量の選択です。コンピュータシステムは非常に多くの異なる方法で使用されるため、選択する仕事量は多種多様です。適切な設計上の決定を下すために、ストレージシステムの設計者はどの仕事量を重要なものにするべきですか？  
AFSの設計者は、ファイルシステムの使用状況を測定した経験から、特定の仕事量の前提を設定しました。特に、ほとんどのファイルは頻繁に共有されず、すべてが順番にアクセスされると想定していました。これらの仮定を前提とすると、AFS設計は完璧な意味を持ちます。  
しかし、これらの仮定は必ずしも正しいとは限りません。たとえば、情報を定期的にログに追加するアプリケーションを考えてみましょう。既存の大きなファイルに少量のデータを追加するこれらの小さなログ書き込みは、AFSにとってかなり問題です。他の多くの困難な仕事量、例えばトランザクションデータベースにおけるランダムな更新も同様に存在します。  
どのタイプの仕事量が共通しているかについての情報を得るための1つの場所は、実行されたさまざまな調査研究によるものです。AFSの遡及[H + 88]を含む仕事量分析[B + 91、H + 11、R + 00、V99]の良い例については、これらの調査のいずれかを参照してください。  

## 50.8 AFS: Other Improvements
バークレーのFFS(シンボリックリンクやその他の機能を追加した)の紹介で見たように、AFSの設計者は、システムを構築してシステムを使いやすく管理するための多くの機能を追加する機会を得ました。たとえば、AFSは真のグローバル名前空間をクライアントに提供し、すべてのファイルの名前がすべてのクライアント・マシンで同じになるようにします。対照的にNFSでは、各クライアントがNFSサーバーを好みの方法でマウントすることができます。したがって、慣例(および管理上の努力)によってのみ、クライアント間で同様のファイル名が付けられます。

AFSはまた、セキュリティを真剣に受け入れ、ユーザーを認証するメカニズムを組み込み、ユーザーが望む場合に一連のファイルを秘密に保つことができるようにしました。NFSはこれとは対照的に、長年にわたりセキュリティはきわめて原始的なサポートを持っていました。

AFSには、柔軟なユーザー管理アクセス制御のための機能も含まれています。したがって、AFSを使用する場合、ユーザーは誰がどのファイルに正確にアクセスできるかを非常に制御できます。NFSは、ほとんどのUNIXファイルシステムと同様に、この種の共有のサポートをまったくしません。

最後に、前述したようにAFSはツールを追加して、システム管理者のサーバー管理を簡素化します。システム管理について考えてみると、AFSは遥か先をいっていました。

## 50.9 Summary
AFSは、NFSで見たものとはまったく異なる分散ファイルシステムの構築方法を示しています。AFSのプロトコル設計は特に重要です。(ファイル全体のキャッシュとコールバックを通じて)サーバーのやりとりを最小限に抑えることで、各サーバーは多くのクライアントをサポートし、特定のサイトを管理するために必要なサーバーの数を減らすことができます。単一の名前空間、セキュリティー、アクセス制御リストを含む他の多くの機能は、AFSを使用するうえできわめてうれしいものです。AFSによって提供される一貫性モデルは、理解しやすく、理由を説明するのが簡単であり、時々NFSで観測されるような時折変わった動作につながることはありません。

おそらく残念ながら、AFSは減少傾向にあるでしょう。NFSはオープンスタンダードとなったため、多くの異なるベンダーがそれをサポートし、CIFS(Windowsベースの分散ファイルシステムプロトコル)とともにNFSが市場を支配していました。ウィスコンシンを含む様々な教育機関のように、AFSのインストールは随時見られますが、唯一の影響は実際のシステムそのものではなく、AFSのアイデアによるものです。実際、NFSv4は、サーバ状態(例えば、「オープン」プロトコルメッセージ)を追加するので、基本的なAFSプロトコルと似ているようなものが増えてきています。

## 参考文献
[B+91] “Measurements of a Distributed File System”  
Mary Baker, John Hartman, Martin Kupfer, Ken Shirriff, John Ousterhout  
SOSP ’91, Pacific Grove, California, October 1991  
An early paper measuring how people use distributed file systems. Matches much of the intuition found in AFS.

[H+11] “A File is Not a File: Understanding the I/O Behavior of Apple Desktop Applications”  
Tyler Harter, Chris Dragga, Michael Vaughn, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau  
SOSP ’11, New York, New York, October 2011  
Our own paper studying the behavior of Apple Desktop workloads; turns out they are a bit different than many of the server-based workloads the systems research community usually focuses upon. Also a good recent reference which points to a lot of related work.

[H+88] “Scale and Performance in a Distributed File System”  
John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham, Michael J. West  
ACM Transactions on Computing Systems (ACM TOCS), page 51-81, Volume 6, Number 1, February 1988  
The long journal version of the famous AFS system, still in use in a number of places throughout the world, and also probably the earliest clear thinking on how to build distributed file systems. A wonderful combination of the science of measurement and principled engineering.

[R+00] “A Comparison of File System Workloads”  
Drew Roselli, Jacob R. Lorch, Thomas E. Anderson  
USENIX ’00, San Diego, California, June 2000  
A more recent set of traces as compared to the Baker paper [B+91], with some interesting twists.

[S+85] “The ITC Distributed File System: Principles and Design”  
M. Satyanarayanan, J.H. Howard, D.A. Nichols, R.N. Sidebotham, A. Spector, M.J. West  
SOSP ’85, Orcas Island, Washington, December 1985  
The older paper about a distributed file system. Much of the basic design of AFS is in place in this older system, but not the improvements for scale. The name change to “Andrew” is an homage to two people both named Andrew, Andrew Carnegie and Andrew Mellon. Thesee two rich dudes started the Carnegie Institute of Technology and the Mellon Institute of Industrial Research, respectively, which eventually merged to become what is now known as Carnegie Mellon University.

[V99] “File system usage in Windows NT 4.0”  
Werner Vogels  
SOSP ’99, Kiawah Island Resort, South Carolina, December 1999  
A cool study of Windows workloads, which are inherently different than many of the UNIX-based studies that had previously been done.

\newpage

